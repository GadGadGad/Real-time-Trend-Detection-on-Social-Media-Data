import sys
import os
import json
import pandas as pd
import numpy as np
from datetime import datetime
from sqlalchemy import create_engine
from pyspark.sql.functions import from_json, col, pandas_udf
from pyspark.sql.types import *

# --- TH√äM ƒê∆Ø·ªúNG D·∫™N PROJECT V√ÄO SYS.PATH ---
# ƒê·ªÉ Spark t√¨m th·∫•y c√°c module trong src/
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))

from src.streaming.spark_utils import get_spark_session
from src.core.scoring.trend_scoring import calculate_unified_score
from src.utils.text_processing.vectorizers import get_embeddings

# Import Logic "X·ªãn" t·ª´ file b·∫°n m·ªõi upload
from src.pipeline.pipeline_stages import run_sahc_clustering, calculate_match_scores
from src.pipeline.main_pipeline import clean_text

# --- C·∫§U H√åNH ---
KAFKA_BOOTSTRAP = "localhost:9092"
POSTGRES_URL = "postgresql://user:password@localhost:5432/trend_db"
MODEL_NAME = "paraphrase-multilingual-mpnet-base-v2"

# K·∫øt n·ªëi DB
db_engine = create_engine(POSTGRES_URL)

# Bi·∫øn Global
embedder = None
trends_cache = {} # L∆∞u tr·ªØ trends ƒë·ªÉ matching

def get_embedder_model():
    """Load model 1 l·∫ßn duy nh·∫•t tr√™n Driver"""
    global embedder
    if embedder is None:
        from sentence_transformers import SentenceTransformer
        print("‚è≥ Loading Embedding Model...")
        embedder = SentenceTransformer(MODEL_NAME)
    return embedder

def load_active_trends():
    """
    Load danh s√°ch Google Trends ƒë·ªÉ kh·ªõp n·ªëi. 
    Trong th·ª±c t·∫ø, n√™n ƒë·ªçc t·ª´ DB ho·∫∑c file JSON shared.
    ·ªû ƒë√¢y demo t√¥i t·∫°o gi·∫£ l·∫≠p ho·∫∑c ƒë·ªçc t·ª´ file n·∫øu c√≥.
    """
    global trends_cache
    if not trends_cache:
        # TODO: B·∫°n c√≥ th·ªÉ code th√™m ƒëo·∫°n ƒë·ªçc file refined_trends.json t·∫°i ƒë√¢y
        # ƒê·ªÉ demo ch·∫°y ƒë∆∞·ª£c ngay, t√¥i ƒë·ªÉ danh s√°ch r·ªóng ho·∫∑c sample
        trends_cache = {
            "B√£o Yagi": {"keywords": ["b√£o yagi", "si√™u b√£o", "b√£o s·ªë 3"], "volume": 500000},
            "iPhone 16": {"keywords": ["iphone 16", "apple", "ios 18"], "volume": 200000},
            "Gi√° v√†ng": {"keywords": ["gi√° v√†ng", "sjc", "v√†ng nh·∫´n"], "volume": 100000}
        }
    return trends_cache

# --- SPARK UDF: EMBEDDING (Ch·∫°y song song tr√™n Executor) ---
@pandas_udf(ArrayType(FloatType()))
def compute_embeddings_udf(contents: pd.Series) -> pd.Series:
    # M·ªói executor t·ª± load model ri√™ng
    from sentence_transformers import SentenceTransformer
    # Cache model trong bi·∫øn local c·ªßa worker
    if not hasattr(compute_embeddings_udf, "model"):
        compute_embeddings_udf.model = SentenceTransformer(MODEL_NAME)
    
    embeddings = compute_embeddings_udf.model.encode(contents.tolist(), show_progress_bar=False)
    return pd.Series(embeddings.tolist())

# --- X·ª¨ L√ù MICRO-BATCH (Logic Ch√≠nh) ---
def process_micro_batch(df_batch, batch_id):
    pdf = df_batch.toPandas()
    if pdf.empty: return

    print(f"üöÄ Batch {batch_id}: Processing {len(pdf)} posts...")
    
    # 1. Chu·∫©n b·ªã d·ªØ li·ªáu (Chuy·ªÉn ƒë·ªïi t·ª´ Spark Row sang List Dict)
    posts = []
    valid_indices = []
    
    for idx, row in pdf.iterrows():
        try:
            item = json.loads(row['value'])
            # L√†m s·∫°ch text ngay t·∫°i ƒë√¢y d√πng h√†m t·ª´ main_pipeline
            item['content'] = clean_text(item.get('content', ''))
            
            # N·∫øu ƒë√£ c√≥ embedding t·ª´ UDF (c·ªôt 'embedding'), d√πng lu√¥n
            if 'embedding' in row and row['embedding']:
                item['embedding'] = np.array(row['embedding'])
                posts.append(item)
                valid_indices.append(idx)
        except: continue

    if not posts: return

    # 2. Chu·∫©n b·ªã Embedding Matrix cho Clustering
    # (L·∫•y t·ª´ k·∫øt qu·∫£ UDF ƒë·ªÉ kh√¥ng ph·∫£i t√≠nh l·∫°i)
    post_embeddings = np.array([p['embedding'] for p in posts])
    
    # X√≥a embedding kh·ªèi dict posts ƒë·ªÉ ƒë·ª° t·ªën RAM khi x·ª≠ l√Ω ti·∫øp
    for p in posts: 
        if 'embedding' in p: del p['embedding']

    # 3. CH·∫†Y SAHC CLUSTERING (Logic m·ªõi t·ª´ pipeline_stages.py)
    # L∆∞u √Ω: Streaming data √≠t h∆°n Batch, n√™n gi·∫£m min_cluster_size
    labels = run_sahc_clustering(
        posts, 
        post_embeddings,
        min_cluster_size=2,  # Gi·∫£m xu·ªëng 2 cho demo streaming nhanh nh·∫°y
        epsilon=0.15,        # TƒÉng nh·∫π epsilon ƒë·ªÉ d·ªÖ gom nh√≥m h∆°n
        method='hdbscan'
    )
    
    # 4. KH·ªöP N·ªêI TRENDS (Matching Logic m·ªõi)
    trends = load_active_trends()
    trend_keys = list(trends.keys())
    
    # T·∫°o embedding cho Trends (ƒë·ªÉ so s√°nh Vector)
    model = get_embedder_model()
    trend_queries = [" ".join(trends[t]['keywords']) for t in trend_keys]
    trend_embeddings = model.encode(trend_queries) if trend_queries else np.array([])

    unique_labels = set(labels)
    rows_to_insert = []

    for label in unique_labels:
        if label == -1: continue # B·ªè qua nhi·ªÖu
        
        # L·∫•y b√†i vi·∫øt thu·ªôc c·ª•m
        indices = [i for i, x in enumerate(labels) if x == label]
        cluster_posts = [posts[i] for i in indices]
        
        # T√≠nh Centroid c·ªßa c·ª•m
        cluster_centroid = np.mean(post_embeddings[indices], axis=0)
        
        # ƒê·∫∑t t√™n t·∫°m cho c·ª•m (L·∫•y ƒëo·∫°n text d√†i nh·∫•t ho·∫∑c title)
        best_content = max(cluster_posts, key=lambda x: len(x.get('content','')))
        cluster_query = best_content.get('title') or best_content.get('content')[:100]

        # G·ªçi h√†m MATCHING X·ªäN t·ª´ pipeline_stages.py
        assigned_trend, topic_type, match_score = calculate_match_scores(
            cluster_query=cluster_query,
            cluster_label=label,
            trend_embeddings=trend_embeddings,
            trend_keys=trend_keys,
            trend_queries=trend_queries,
            embedder=model,
            reranker=None, # T·∫°m t·∫Øt Reranker cho nhanh (Streaming c·∫ßn t·ªëc ƒë·ªô)
            rerank=False,
            threshold=0.35, # Ng∆∞·ª°ng nh·∫°y
            cluster_centroid=cluster_centroid
        )
        
        # T√≠nh ƒëi·ªÉm Trend
        trend_data = trends.get(assigned_trend, {'volume': 0})
        unified_score, components = calculate_unified_score(trend_data, cluster_posts)
        
        # Chu·∫©n b·ªã ghi DB
        # T·∫°o JSON m·∫´u tin ƒë·∫°i di·ªán an to√†n
        rep_posts_data = []
        for p in cluster_posts[:3]:
            rep_posts_data.append({
                "source": str(p.get('source', 'Unknown')),
                "content": str(p.get('content', ''))[:200]
            })

        trend_record = {
            "batch_id": str(batch_id),
            "cluster_label": int(label),
            
            # N·∫øu kh·ªõp trend -> D√πng t√™n trend. N·∫øu kh√¥ng -> D√πng t√™n c·ª•m (Discovery)
            "trend_name": assigned_trend if topic_type == "Trending" else f"New: {cluster_query[:50]}...",
            "topic_type": topic_type,
            "category": "T7", # T·∫°m th·ªùi default, mu·ªën x·ªãn th√¨ g·ªçi TaxonomyClassifier
            
            "trend_score": float(unified_score),
            "score_g": components.get('G', 0),
            "score_f": components.get('F', 0),
            "score_n": components.get('N', 0),
            
            "post_count": len(cluster_posts),
            "representative_posts": json.dumps(rep_posts_data, ensure_ascii=False),
            "created_at": datetime.now()
        }
        rows_to_insert.append(trend_record)

    # 5. Ghi v√†o PostgreSQL
    if rows_to_insert:
        df_result = pd.DataFrame(rows_to_insert)
        try:
            df_result.to_sql('detected_trends', con=db_engine, if_exists='append', index=False)
            print(f"‚úÖ Batch {batch_id}: Saved {len(rows_to_insert)} trends. (Top: {df_result.iloc[0]['trend_name']})")
        except Exception as e:
            print(f"‚ùå DB Error: {e}")

# --- MAIN FLOW ---
if __name__ == "__main__":
    spark = get_spark_session()
    
    # 1. ƒê·ªçc Kafka
    df_stream = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP) \
        .option("subscribe", "raw_data") \
        .option("startingOffsets", "latest") \
        .load()
    
    # 2. Parse Value (Gi·∫£ s·ª≠ JSON thu·∫ßn trong c·ªôt value)
    # L·∫•y value as String
    df_text = df_stream.selectExpr("CAST(value AS STRING) as value")
    
    # 3. T√≠nh Embeddings song song (UDF)
    # C·∫ßn tr√≠ch xu·∫•t content t·ª´ chu·ªói JSON tr∆∞·ªõc khi embed
    # (ƒê·ªÉ ƒë∆°n gi·∫£n cho Spark SQL, ta d√πng Pandas UDF ·ªü b∆∞·ªõc sau ho·∫∑c parse lu√¥n ·ªü ƒë√¢y)
    # Tuy nhi√™n, ƒë·ªÉ t·ªëi ∆∞u, ta ƒë·∫©y vi·ªác parse v√†o micro-batch Pandas cho linh ho·∫°t.
    
    # ·ªû ƒë√¢y ta ch·ªâ gom data, vi·ªác t√≠nh embedding n√™n l√†m trong process_batch 
    # HO·∫∂C d√πng UDF n·∫øu mu·ªën t·∫≠n d·ª•ng cluster.
    # C√°ch t·ªët nh·∫•t cho demo single-node: L√†m h·∫øt trong process_micro_batch (nh∆∞ code tr√™n).
    # Nh∆∞ng n·∫øu mu·ªën ƒë√∫ng chu·∫©n Spark:
    
    # Parse JSON ƒë·ªÉ l·∫•y content
    json_schema = StructType([
        StructField("content", StringType(), True),
        StructField("source", StringType(), True)
    ])
    df_parsed = df_text.withColumn("data", from_json(col("value"), json_schema)).select("value", "data.content")
    
    # Ch·∫°y Embedding UDF
    df_embedded = df_parsed.withColumn("embedding", compute_embeddings_udf(col("content")))
    
    # 4. Trigger
    query = df_embedded.writeStream \
        .foreachBatch(process_micro_batch) \
        .trigger(processingTime='10 seconds') \
        .start()
        
    print(f"üì° Trend Detection Streaming Job Started... (SAHC + Hybrid Match Enabled)")
    query.awaitTermination()