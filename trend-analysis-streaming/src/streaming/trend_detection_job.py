import sys
import os
import json
import pandas as pd
import numpy as np
from datetime import datetime
from sqlalchemy import create_engine
from pyspark.sql.functions import from_json, col, to_timestamp
from pyspark.sql.types import StructType, StructField, StringType, MapType

# Import logic c≈©
# (Th√™m ƒë∆∞·ªùng d·∫´n g·ªëc ƒë·ªÉ Python t√¨m th·∫•y module src)
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))

from src.streaming.spark_utils import get_spark_session
from src.utils.text_processing.utils import normalize_url
from src.utils.text_processing.vectorizers import get_embeddings
from src.core.analysis.clustering import cluster_data, extract_cluster_labels
from src.pipeline.trend_scoring import calculate_unified_score

# --- C·∫§U H√åNH ---
KAFKA_BOOTSTRAP = "localhost:9092"
POSTGRES_URL = "postgresql://user:password@localhost:5432/trend_db"
MODEL_NAME = "paraphrase-multilingual-mpnet-base-v2"

# K·∫øt n·ªëi DB ƒë·ªÉ ghi k·∫øt qu·∫£
db_engine = create_engine(POSTGRES_URL)

# Load tr∆∞·ªõc Model Embedding (Global variable ƒë·ªÉ tr√°nh load l·∫°i m·ªói batch)
embedder = None

def get_embedder_model():
    global embedder
    if embedder is None:
        from sentence_transformers import SentenceTransformer
        print("‚è≥ Loading Embedding Model (First time)...")
        embedder = SentenceTransformer(MODEL_NAME)
    return embedder

# --- X·ª¨ L√ù MICRO-BATCH ---
def process_micro_batch(df_batch, batch_id):
    """
    H√†m n√†y ƒë∆∞·ª£c g·ªçi m·ªói khi Spark gom ƒë·ªß d·ªØ li·ªáu (ho·∫∑c h·∫øt timeout).
    df_batch: Spark DataFrame ch·ª©a d·ªØ li·ªáu c·ªßa batch hi·ªán t·∫°i.
    """
    # 1. Chuy·ªÉn sang Pandas ƒë·ªÉ x·ª≠ l√Ω logic ph·ª©c t·∫°p
    pdf = df_batch.toPandas()
    
    if pdf.empty:
        print(f"üí§ Batch {batch_id} empty.")
        return

    print(f"üöÄ Processing Batch {batch_id} with {len(pdf)} records...")
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu cho Core Logic
    # (Map columns t·ª´ Kafka JSON sang format c·ªßa pipeline c≈©)
    posts = []
    for _, row in pdf.iterrows():
        try:
            # Parse JSON string t·ª´ Kafka value
            item = json.loads(row['value'])
            posts.append(item)
        except: continue

    if not posts: return

    # --- T√ÅI S·ª¨ D·ª§NG LOGIC C≈® (MAIN PIPELINE) ---
    
    # A. Text Processing & Embedding
    contents = [p.get('content', '') for p in posts]
    
    # D√πng model ƒë√£ load global
    model = get_embedder_model()
    embeddings = model.encode(contents, show_progress_bar=False)
    
    # B. Clustering (SAHC / HDBSCAN)
    # G·ªçi h√†m t·ª´ src/core/analysis/clustering.py
    # L∆∞u √Ω: Streaming data th∆∞·ªùng √≠t, n√™n gi·∫£m min_cluster_size
    labels = cluster_data(
        embeddings, 
        min_cluster_size=2,  # Streaming batch nh·ªè n√™n gi·∫£m threshold
        epsilon=0.05,
        method='hdbscan'
    )
    
    # C. Scoring & Saving
    # Gom b√†i vi·∫øt theo cluster ƒë·ªÉ t√≠nh ƒëi·ªÉm
    clusters = {}
    unique_labels = set(labels)
    
    rows_to_insert = []
    
    for label in unique_labels:
        if label == -1: continue # B·ªè qua nhi·ªÖu
        
        # L·∫•y c√°c b√†i trong c·ª•m n√†y
        indices = [i for i, x in enumerate(labels) if x == label]
        cluster_posts = [posts[i] for i in indices]
        
        # ƒê·∫∑t t√™n cluster (ƒë∆°n gi·∫£n h√≥a cho streaming)
        # N·∫øu mu·ªën x·ªãn h∆°n th√¨ g·ªçi LLM ·ªü ƒë√¢y (nh∆∞ng s·∫Ω ch·∫≠m)
        # T·∫°m th·ªùi l·∫•y title b√†i ƒë·∫ßu ti√™n ho·∫∑c extract keyword
        cluster_name = cluster_posts[0].get('content', '')[:50].replace('\n', ' ') + "..."
        
        # T√≠nh ƒëi·ªÉm Trend (src/pipeline/trend_scoring.py)
        # Gi·∫£ l·∫≠p trend_data r·ªóng v√¨ ta ƒëang streaming kh√°m ph√° (Discovery)
        trend_dummy = {'volume': 0} 
        score, components = calculate_unified_score(trend_dummy, cluster_posts)
        
        # Chu·∫©n b·ªã row ƒë·ªÉ insert DB
        trend_record = {
            "batch_id": str(batch_id),
            "cluster_label": int(label),
            "trend_name": cluster_name,
            "topic_type": "Discovery" if score < 50 else "Trending", # Ng∆∞·ª°ng t·∫°m
            "category": "T7", # T·∫°m th·ªùi, c·∫ßn classify sau
            "trend_score": float(score),
            "score_g": components.get('G', 0),
            "score_f": components.get('F', 0),
            "score_n": components.get('N', 0),
            "post_count": len(cluster_posts),
            # L∆∞u 1 b√†i m·∫´u ƒë·ªÉ hi·ªÉn th·ªã
            "representative_posts": json.dumps([{
                "source": p.get('source'),
                "content": p.get('content')[:100]
            } for p in cluster_posts[:2]]),
            "created_at": datetime.now()
        }
        rows_to_insert.append(trend_record)
        
    # D. Write to PostgreSQL
    if rows_to_insert:
        df_result = pd.DataFrame(rows_to_insert)
        try:
            df_result.to_sql('detected_trends', con=db_engine, if_exists='append', index=False)
            print(f"‚úÖ Batch {batch_id}: Saved {len(rows_to_insert)} trends to DB.")
        except Exception as e:
            print(f"‚ùå Error writing to DB: {e}")
            
    # E. (Optional) Write raw logs for debugging
    # C√≥ th·ªÉ ghi raw posts v√†o b·∫£ng raw_logs n·∫øu c·∫ßn

# --- MAIN STREAMING FLOW ---
if __name__ == "__main__":
    spark = get_spark_session()
    
    # 1. ƒê·ªçc t·ª´ Kafka
    df_stream = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP) \
        .option("subscribe", "raw_data") \
        .option("startingOffsets", "latest") \
        .load()
    
    # 2. Cast Value sang String (JSON)
    df_string = df_stream.selectExpr("CAST(value AS STRING)")
    
    # 3. Trigger Processing
    # Trigger 10 seconds: Gom d·ªØ li·ªáu m·ªói 10s x·ª≠ l√Ω 1 l·∫ßn
    query = df_string.writeStream \
        .foreachBatch(process_micro_batch) \
        .trigger(processingTime='10 seconds') \
        .start()
        
    print(f"üì° Trend Detection Streaming Job Started... Listening on {KAFKA_BOOTSTRAP}")
    query.awaitTermination()