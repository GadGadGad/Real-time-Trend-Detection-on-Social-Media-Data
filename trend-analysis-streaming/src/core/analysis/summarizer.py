import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from rich.console import Console
import gc

console = Console()

# Available Vietnamese summarization models
SUMMARIZATION_MODELS = {
    'vit5-large': 'VietAI/vit5-large-vietnews-summarization',  # Best quality, ~1.2GB
    'vit5-base': 'VietAI/vit5-base-vietnews-summarization',    # Faster, ~900MB
    'bartpho': 'vinai/bartpho-syllable',                       # Alternative, good for short text
    'gemini': 'gemma-3-27b-it',                              # Cloud LLM, handles massive batches
}

class Summarizer:
    def __init__(self, model_name="VietAI/vit5-base-vietnews-summarization", device=None):
        # Allow shorthand model names
        if model_name in SUMMARIZATION_MODELS:
            model_name = SUMMARIZATION_MODELS[model_name]
        self.model_name = model_name
        
        # [SMART DEVICE SELECTION]
        # N·∫øu kh√¥ng ch·ªâ ƒë·ªãnh device, ki·ªÉm tra VRAM. N·∫øu < 6GB th√¨ m·∫∑c ƒë·ªãnh d√πng CPU ƒë·ªÉ an to√†n.
        if device:
            self.device = device
        else:
            if torch.cuda.is_available():
                total_mem = torch.cuda.get_device_properties(0).total_memory
                if total_mem > 6 * 1024**3: # > 6GB
                    self.device = 'cuda'
                else:
                    self.device = 'cpu' # Fallback to CPU for safety
            else:
                self.device = 'cpu'
                
        self.tokenizer = None
        self.model = None
        self.enabled = False

    def load_model(self):
        if self.model_name.startswith('gemma'):
            from src.core.llm.llm_refiner import LLMRefiner
            self.model = LLMRefiner(provider="gemini", model_path=self.model_name)
            self.enabled = self.model.enabled
            if self.enabled:
                console.print(f"[green]‚úÖ Summarizer using Gemini ({self.model_name})[/green]")
            return

        console.print(f"[cyan]üì• Loading Summarizer: {self.model_name} on {self.device}...[/cyan]")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name)
            
            # [OOM PROTECTION]
            try:
                self.model.to(self.device)
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    console.print("[yellow]‚ö†Ô∏è GPU OOM during Summarizer load. Falling back to CPU...[/yellow]")
                    self.device = 'cpu'
                    self.model.to('cpu')
                    torch.cuda.empty_cache()
                else:
                    raise e
            
            self.enabled = True
            console.print(f"[green]‚úÖ Summarizer loaded on {self.device}[/green]")
        except Exception as e:
            console.print(f"[red]‚ùå Failed to load Summarizer: {e}[/red]")
            self.enabled = False

    def unload_model(self):
        if self.model:
            del self.model
            del self.tokenizer
            self.model = None
            self.tokenizer = None
            self.enabled = False
            gc.collect()
            torch.cuda.empty_cache()
            console.print("[dim]üóëÔ∏è Summarizer unloaded to free resources.[/dim]")

    def summarize_batch(self, texts, max_length=256):
        if not self.enabled: 
            self.load_model()
            if not self.enabled: return texts # Fallback
            
        if self.model_name.startswith('gemini'):
            # True batching for Gemini: group articles into one long prompt
            summaries = []
            gemini_batch_size = 10 # Can handle more, but 20 is safe for response parsing
            for i in range(0, len(texts), gemini_batch_size):
                chunk = texts[i : i + gemini_batch_size]
                chunk_str = "\n".join([f"--- ARTICLE {idx+1} ---\n{t[:2000]}" for idx, t in enumerate(chunk)])
                prompt = (
                    "Role: Senior News Editor. Summarize each article into ONE concise Vietnamese paragraph (max 100 words).\n"
                    "Style: Neutral, journalistic, factual. No \"About this article...\" or \"The article discusses...\". Direct summary only.\n"
                    "Must Include: Key entities, numbers, dates, locations.\n"
                    "Output: A JSON list of strings. Each summary corresponds to the input article order.\n"
                    "Example: [\"H√† N·ªôi ƒë√≥n kh√¥ng kh√≠ l·∫°nh tƒÉng c∆∞·ªùng t·ª´ ƒë√™m 15/10...\", \"Vingroup c√¥ng b·ªë b√°o c√°o t√†i ch√≠nh...\"]\n\n"
                    f"{chunk_str}\n\n"
                    "JSON Output:"
                )
                try:
                    res_text = self.model._generate(prompt)
                    chunk_summaries = self.model._extract_json(res_text, is_list=True)
                    if chunk_summaries and len(chunk_summaries) == len(chunk):
                        summaries.extend(chunk_summaries)
                    else:
                        # Fallback/Error recovery: individual generation if batch failed
                        console.print(f"[yellow]‚ö†Ô∏è Batch summary failed/mismatched for chunk {i//gemini_batch_size}. Retrying individually...[/yellow]")
                        for t in chunk:
                            summaries.append(self.model.summarize_text(t))
                except Exception as e:
                    console.print(f"[red]Gemini Batch Summarization Error: {e}[/red]")
                    summaries.extend([t[:256] for t in chunk]) # Truncation fallback
            return summaries

        console.print(f"[cyan]üìù Summarizing {len(texts)} articles with {self.model_name.split('/')[-1]}...[/cyan]")
        summaries = []
        batch_size = 4 # T4 GPU safe limit
        
        # N·∫øu ch·∫°y tr√™n CPU, t·∫Øt thanh ti·∫øn tr√¨nh rich ƒë·ªÉ log ƒë·ª° r·ªëi v√† nhanh h∆°n
        if self.device == 'cpu':
            iterator = range(0, len(texts), batch_size)
        else:
            from rich.progress import track
            iterator = track(range(0, len(texts), batch_size), description="[cyan]Summarizing batches...[/cyan]")
        
        for i in iterator:
            batch = texts[i : i + batch_size]
            
            # ViT5 requires special format: "vietnews: {text} </s>"
            formatted_batch = [f"vietnews: {text} </s>" for text in batch]
            
            inputs = self.tokenizer(
                formatted_batch, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=1024
            ).to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids=inputs["input_ids"],
                    attention_mask=inputs["attention_mask"],
                    max_length=max_length,
                    early_stopping=True
                )
            
            decoded = self.tokenizer.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=True)
            summaries.extend(decoded)
            
        return summaries

    def sanity_check(self, texts, n_samples=3):
        """
        Run a quick sanity check on summarization quality.
        Shows original vs summary for n random samples.
        """
        import random
        
        if not self.enabled:
            self.load_model()
            if not self.enabled:
                console.print("[red]Cannot run sanity check - model failed to load[/red]")
                return
        
        # Pick random samples
        samples = random.sample(texts, min(n_samples, len(texts)))
        summaries = self.summarize_batch(samples, max_length=150)
        
        console.print("\n[bold cyan]üìã Summarization Sanity Check[/bold cyan]")
        console.print(f"Model: {self.model_name}\n")
        
        for i, (orig, summ) in enumerate(zip(samples, summaries)):
            compression = len(summ) / len(orig) * 100 if orig else 0
            console.print(f"[bold]--- Sample {i+1} ---[/bold]")
            console.print(f"[dim]Original ({len(orig)} chars):[/dim]")
            console.print(f"  {orig[:300]}...")
            console.print(f"[green]Summary ({len(summ)} chars, {compression:.0f}% of original):[/green]")
            console.print(f"  {summ}")
            console.print()
        
        # Quality metrics
        avg_compression = sum(len(s)/len(o) for o, s in zip(samples, summaries)) / len(samples) * 100
        avg_summary_len = sum(len(s) for s in summaries) / len(summaries)
        
        console.print("[bold]Quality Metrics:[/bold]")
        console.print(f"  Avg compression ratio: {avg_compression:.0f}%")
        console.print(f"  Avg summary length: {avg_summary_len:.0f} chars")
        console.print(f"  ‚úÖ Good if compression < 30% and summaries are coherent")