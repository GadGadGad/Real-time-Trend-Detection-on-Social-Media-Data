import json
import csv
import time
import os
import sys
import glob
import random
from datetime import datetime
from kafka import KafkaProducer

# C·∫•u h√¨nh Kafka
KAFKA_TOPIC = 'raw_data'
BOOTSTRAP_SERVERS = ['localhost:9092']

def create_producer():
    try:
        producer = KafkaProducer(
            bootstrap_servers=BOOTSTRAP_SERVERS,
            value_serializer=lambda x: json.dumps(x).encode('utf-8')
        )
        print(f"‚úÖ ƒê√£ k·∫øt n·ªëi t·ªõi Kafka t·∫°i {BOOTSTRAP_SERVERS}")
        return producer
    except Exception as e:
        print(f"‚ùå L·ªói k·∫øt n·ªëi Kafka: {e}")
        return None

# --- Logic Load d·ªØ li·ªáu (M∆∞·ª£n t·ª´ utils/data_loader.py) ---
def clean_text(text):
    import re
    if not text: return ""
    return re.sub(r'\s+', ' ', str(text)).strip()

def load_json_file(filepath):
    """ƒê·ªçc file Social JSON (Facebook)"""
    posts = []
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            for item in data:
                text = item.get('text') or item.get('content') or ''
                # L·∫•y th·ªùi gian t·ªët nh·∫•t c√≥ th·ªÉ
                time_str = item.get('time') or item.get('time_label') or datetime.now().isoformat()
                
                clean = clean_text(text)
                if len(clean) < 20: continue 
                
                posts.append({
                    "source": f"Face: {item.get('pageName', 'Unknown')}",
                    "content": clean,
                    "url": item.get('postUrl', ''),
                    "published_at": time_str,
                    "type": "social",
                    "stats": item.get('stats', {})
                })
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói ƒë·ªçc JSON {filepath}: {e}")
    return posts

def load_csv_file(filepath):
    """ƒê·ªçc file News CSV (VnExpress, v.v.)"""
    posts = []
    source_name = os.path.basename(os.path.dirname(filepath)).upper()
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                content = f"{row.get('title', '')}\n{row.get('content', '')}"
                if len(content) < 20: continue
                
                posts.append({
                    "source": source_name,
                    "content": clean_text(content),
                    "url": row.get('url', ''),
                    "published_at": row.get('published_at', datetime.now().isoformat()),
                    "type": "news",
                    "stats": {} # News th∆∞·ªùng kh√¥ng c√≥ like/share trong file csv crawler
                })
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói ƒë·ªçc CSV {filepath}: {e}")
    return posts

# --- Logic Replay ---
def run_replay(data_dir, speed=1.0):
    """
    data_dir: Th∆∞ m·ª•c ch·ª©a file d·ªØ li·ªáu (data/raw_demo)
    speed: T·ªëc ƒë·ªô replay (v√≠ d·ª• 0.1 l√† nhanh g·∫•p 10 l·∫ßn, 1.0 l√† 1 gi√¢y = 1 gi√¢y)
           Nh∆∞ng ƒë·ªÉ demo nhanh, ta th∆∞·ªùng ch·ªâ sleep 1 kho·∫£ng random nh·ªè.
    """
    producer = create_producer()
    if not producer: return

    all_posts = []
    
    # 1. Qu√©t t·∫•t c·∫£ file trong th∆∞ m·ª•c data_dir
    print(f"üìÇ ƒêang qu√©t d·ªØ li·ªáu t·ª´: {data_dir}...")
    
    # T√¨m JSON (Social)
    json_files = glob.glob(os.path.join(data_dir, "**/*.json"), recursive=True)
    for f in json_files:
        all_posts.extend(load_json_file(f))
        
    # T√¨m CSV (News)
    csv_files = glob.glob(os.path.join(data_dir, "**/*.csv"), recursive=True)
    for f in csv_files:
        all_posts.extend(load_csv_file(f))
        
    print(f"üìä T·ªïng c·ªông: {len(all_posts)} b√†i vi·∫øt.")
    
    # 2. X√°o tr·ªôn ng·∫´u nhi√™n ƒë·ªÉ m√¥ ph·ªèng d·ªØ li·ªáu ƒë·∫øn t·ª´ nhi·ªÅu ngu·ªìn c√πng l√∫c
    # (Trong th·ª±c t·∫ø n√™n sort theo time, nh∆∞ng format time m·ªói ngu·ªìn kh√°c nhau kh√° ph·ª©c t·∫°p ƒë·ªÉ parse chu·∫©n)
    random.shuffle(all_posts)
    
    print("üöÄ B·∫Øt ƒë·∫ßu Replay v√†o Kafka topic 'raw_data'...")
    
    try:
        for i, post in enumerate(all_posts):
            # G·ª≠i v√†o Kafka
            producer.send(KAFKA_TOPIC, value=post)
            
            # Log ti·∫øn ƒë·ªô
            if (i+1) % 10 == 0:
                sys.stdout.write(f"\rüì§ ƒê√£ g·ª≠i: {i+1}/{len(all_posts)} messages...")
                sys.stdout.flush()
            
            # Gi·∫£ l·∫≠p ƒë·ªô tr·ªÖ (Streaming delay)
            # Sleep random t·ª´ 0.05s ƒë·∫øn 0.2s ƒë·ªÉ t·∫°o c·∫£m gi√°c data ƒëang tr√¥i v·ªÅ
            time.sleep(random.uniform(0.05, 0.2) * speed)
            
        producer.flush()
        print(f"\n‚úÖ Ho√†n t·∫•t replay {len(all_posts)} messages.")
        
    except KeyboardInterrupt:
        print("\nüõë D·ª´ng replay.")
    finally:
        producer.close()

if __name__ == "__main__":
    # ƒê∆∞·ªùng d·∫´n m·∫∑c ƒë·ªãnh ƒë·∫øn th∆∞ m·ª•c data demo
    # Gi·∫£ s·ª≠ ch·∫°y t·ª´ root project
    DEFAULT_DATA_DIR = "data/raw_demo"
    
    if len(sys.argv) > 1:
        DEFAULT_DATA_DIR = sys.argv[1]
        
    if not os.path.exists(DEFAULT_DATA_DIR):
        print(f"‚ùå Th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i: {DEFAULT_DATA_DIR}")
        print("üëâ H√£y t·∫°o th∆∞ m·ª•c 'data/raw_demo' v√† copy file d·ªØ li·ªáu c≈© v√†o ƒë√≥.")
    else:
        run_replay(DEFAULT_DATA_DIR)