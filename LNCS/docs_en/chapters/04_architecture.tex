\section{Proposed System Architecture}
The system follows a \textbf{Kappa-style streaming architecture} separated into two processing paths to address the "Latency vs. Accuracy" trade-off.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/full_pipeline.png}
    \caption{Proposed System Architecture: Dual-Path Processing (Fast Path vs. Slow Path).} \label{fig:pipeline}
\end{figure}

Fig. \ref{fig:pipeline} illustrates the end-to-end data flow. Raw data is ingested via Kafka, processed in micro-batches by Spark (Fast Path), and asynchronously enriched by Gemini Pro (Slow Path) before being served to the UI. This separation ensures that the heavy inference time of LLMs does not block the high-throughput ingestion pipeline.

\subsection{System Workflow}
The lifecycle of a social post in our system follows five stages:
\begin{enumerate}
    \item \textbf{Ingestion Layer:} Python crawlers, orchestrated by \textbf{Airflow}, continuously collect raw text from 40+ high-traffic Fanpages. Data is normalized to JSON format and pushed to the \textbf{Apache Kafka} topic \texttt{posts\_stream\_v1}, with \textbf{Zookeeper} handling cluster coordination.
    \item \textbf{Filtering (Spark Streaming):} The \textbf{Spark} engine consumes the Kafka stream in micro-batches. A Heuristic Guard drops routine noise, while a \textbf{Smart Query Constructor} expands potential trend keywords (e.g., "Sea Games" $\to$ "Sea Games 33 Schedule") to increase recall.
    \item \textbf{Clustering (Fast Path):} Valid social posts are vectorized using ONNX models and \textbf{matched to Anchor vectors} (News/Trends) using a \textbf{latency-optimized hybrid score} (cosine similarity with a heuristic keyword boost).
          We support a Cross-Encoder reranking module at the matching stage; however, to satisfy real-time constraints, the \textbf{online deployment uses a lightweight verification strategy} that only re-scores the winning candidate (Top-1) as a \textbf{verification gate}.
          In offline analysis or configurable modes, the system can be extended to rerank Top-$K$ candidates for higher precision.
          Posts that are rejected by the verification gate or fail the threshold are buffered as \textit{residual} for the \textbf{Discovery} step (HDBSCAN).

    \item \textbf{Intelligence (Slow Path):} Asynchronous \textbf{Python Workers} poll confirmed clusters from the \textbf{PostgreSQL} unified store. A cluster is considered \emph{significant} and eligible for LLM enrichment only if it reaches the impact threshold in terms of \textbf{distinct users} ($U(C)\ge \delta_{significant}$). Workers then query \textbf{Gemini Pro} to extract structured 5W1H summaries and deduplicate overlapping topics (see Appendix Table \ref{tab:prompt_refinement}).
    \item \textbf{Visualization:} The Dashboard polls the PostgreSQL database to display verified, enriched events in real-time.
\end{enumerate}

\noindent
\begin{center}
    \setlength{\fboxsep}{10pt}
    \fcolorbox{black}{gray!10}{%
        \begin{minipage}{0.95\textwidth}
            \textbf{\large End-to-End Processing Trace: "Typhoon Yagi Alert"}
            \vspace{0.2cm}
            \par
            To illustrate the pipeline, consider a single social post lifecycle:
            \begin{enumerate}
                \item \textbf{Input:} Crawler fetches raw text "Bão số 3 giật cấp 12..." from fanpage \textit{ThongTinChinhPhu}.
                \item \textbf{Ingestion:} Kafka producer serializes it to JSON and pushes to topic \texttt{posts\_stream\_v1} (Partition 0).
                \item \textbf{Filtration:} Spark Streaming consumes the message. The Heuristic Guard permits it, and Smart Query expands context to "Typhoon Yagi tracking".
                \item \textbf{Clustering:} The Onnx Worker maps it to vector $\mathbf{v}$. SAHC identifies the best-matching News Anchor "Typhoon Yagi" (Hybrid Score $> \lambda_{match}$) and attaches the post.
                \item \textbf{Intelligence:} The cluster grows beyond the significance threshold. An async worker triggers Gemini Pro, which returns a structured summary: "Typhoon Yagi approaching Quang Ninh."
                \item \textbf{Output:} The Dashboard receives the specific update and displays a "Public Risk" alert on the live map.
            \end{enumerate}
        \end{minipage}%
    }
\end{center}

\subsection{Fast Path vs. Slow Path Implementation}
\begin{itemize}
    \item \textbf{Ingestion (Kafka \& Zookeeper):}
          \begin{itemize}
              \item \textbf{Producer:} Serializes raw posts into JSON and publishes to the \texttt{posts\_stream\_v1} topic, handling retries and exponential backoff to ensure at-least-once delivery.
              \item \textbf{Broker:} Kafka acts as the distributed, fault-tolerant log storage, while \textbf{ZooKeeper} manages broker metadata, leader election, and partition assignment.
          \end{itemize}

    \item \textbf{Processing (Spark Structured Streaming):}
          \begin{itemize}
              \item Consumes events from Kafka with \textbf{exactly-once semantics} using checkpointing.
              \item Parses raw JSON payloads into Spark DataFrames for efficient columnar processing.
          \end{itemize}

    \item \textbf{Inference (Pandas UDF \& ONNX):}
          \begin{itemize}
              \item Uses PySpark’s \textbf{Pandas UDF} interface to vectorize text in batches, avoiding row-by-row Python serialization overhead.
              \item Invokes the \textbf{ONNX Runtime} engine on Worker nodes to compute embeddings (SBERT) for each micro-batch; \textbf{HDBSCAN} is triggered only on the \textit{unmatched/residual} set to discover emerging \textbf{Social-only} events.
          \end{itemize}

    \item \textbf{Slow Path Intelligence (Async Workers):}
          \begin{itemize}
              \item Designed to decouple heavy operations from the ingestion stream. Workers poll confirmed clusters and:
                    \begin{enumerate}
                        \item Query \textbf{Google Trends API} to fetch search volume (G-score), using the filter prompt in Appendix Table \ref{tab:prompt_trends}. This signal is treated as \textbf{confirmatory/lagging}: it typically accumulates after an event is already visible on social streams, and is therefore used to validate and prioritize clusters rather than to trigger initial detection.
                        \item Employ a \textbf{Context-Aware Prompting} strategy with Gemini Pro to synthesize coherent summaries.
                    \end{enumerate}
          \end{itemize}
\end{itemize}

\subsection{Latency Definition}
We distinguish between two types of latency in our system:
\begin{itemize}
    \item \textbf{Fast-Path Processing Latency:} measured from Kafka ingestion to cluster assignment within a Spark micro-batch.
    \item \textbf{End-to-End Alert Latency:} measured from data ingestion to dashboard update.
\end{itemize}

In our experimental setup, the Fast Path achieves an average processing latency of \textbf{2.4 seconds}, while the end-to-end alert latency remains \textbf{below 10 seconds}, as the Slow Path LLM enrichment is executed asynchronously and does not block real-time event detection and classification.
