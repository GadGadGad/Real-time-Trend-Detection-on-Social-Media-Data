import re
import os
import json
from rich.console import Console
from rich.progress import track
from dotenv import load_dotenv
import torch

load_dotenv()
console = Console()

class LLMRefiner:
    def __init__(self, provider="gemini", api_key=None, model_path=None, debug=False, batch_size=4):
        self.provider = provider
        self.enabled = False
        self.debug = debug
        self.model_name = (model_path or "").lower()  # Track model name for batch size decisions
        
        if provider == "gemini":
            try:
                import google.generativeai as genai
                self.api_key = api_key or os.getenv("GEMINI_API_KEY")
                if not self.api_key:
                    console.print("[yellow]‚ö†Ô∏è GEMINI_API_KEY not found. LLM Refinement disabled.[/yellow]")
                else:
                    genai.configure(api_key=self.api_key)
                    # Allow specifying model via model_path (e.g. 'gemini-1.5-pro')
                    gemini_model = model_path or "models/gemma-3-27b-it"
                    console.print(f"[cyan]‚ôä Using Gemini Model: {gemini_model}[/cyan]")
                    self.model = genai.GenerativeModel(gemini_model)
                    self.enabled = True
            except ImportError:
                console.print("[red]‚ùå google-generativeai not installed.[/red]")

        elif provider == "kaggle" or provider == "local":
            try:
                import torch
                from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig
                
                model_id = model_path or "google/gemma-2-2b-it"
                self.model_id = model_id.lower()
                console.print(f"[bold cyan]ü§ñ Loading {model_id} via Transformers...[/bold cyan]")
                
                # Use 4-bit quantization to fit larger models in Kaggle T4/P100
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float32,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=False, # Disable for max stability
                )

                self.tokenizer = AutoTokenizer.from_pretrained(model_id)
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                # Enforce limit to fix truncation warning and prevent OOB
                self.tokenizer.model_max_length = 4096
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_id,
                    quantization_config=bnb_config,
                    device_map="auto",
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
                # self.pipeline removal - using manual generate for stability
                self.enabled = True
            except Exception as e:
                console.print(f"[red]‚ùå Failed to load local model: {e}[/red]")

    @property
    def is_high_capacity_model(self):
        """Check if using a high-capacity model (gemini API) vs local model (gemma).
        Gemini API can handle much larger batch sizes than local gemma models.
        """
        # If provider is gemini and model_name contains 'gemini' (not 'gemma'), it's high capacity
        if self.provider == "gemini":
            # Check if model_name explicitly mentions gemma (local-like model)
            if "gemma" in self.model_name:
                return False
            return True  # gemini-1.5-pro, gemini-2.0-flash, etc.
        return False  # kaggle/local providers

    def _generate(self, prompt):
        return self._generate_batch([prompt])[0]

    def _generate_batch(self, prompts):
        if not prompts: return []
        
        if self.provider == "gemini":
            import concurrent.futures
            
            def get_content(p):
                import time
                import re as _re
                max_retries = 3
                
                for attempt in range(max_retries):
                    try:
                        # Safety settings to minimize refusals
                        safety_settings = [
                            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
                        ]
                        response = self.model.generate_content(p, safety_settings=safety_settings)
                        
                        # Handle Finish Reasons e.g. RECITATION (4)
                        if response.candidates and response.candidates[0].finish_reason != 1: # 1 = STOP
                            if self.debug: 
                                console.print(f"[dim yellow]Gemini Finish Reason: {response.candidates[0].finish_reason}[/dim yellow]")
                            # Attempt to extract partial text if available
                            if hasattr(response, 'text'): 
                                try: return response.text
                                except: pass
                            return ""
                            
                        return response.text
                    except Exception as e:
                        error_str = str(e)
                        
                        # Handle 429 Rate Limit with retry
                        if "429" in error_str or "quota" in error_str.lower():
                            # Try to parse recommended wait time
                            wait_match = _re.search(r'retry in (\d+\.?\d*)', error_str.lower())
                            wait_time = float(wait_match.group(1)) if wait_match else 30.0
                            wait_time = min(wait_time + 5, 60)  # Add buffer, cap at 60s
                            
                            if attempt < max_retries - 1:
                                console.print(f"[yellow]‚è≥ Rate limited. Waiting {wait_time:.0f}s before retry {attempt+2}/{max_retries}...[/yellow]")
                                time.sleep(wait_time)
                                continue
                        
                        if self.debug: console.print(f"[dim red]Gemini Error: {e}[/dim red]")
                        return ""
                
                return ""  # All retries failed

            # Use ThreadPoolExecutor for parallel API calls
            # Reduced workers to prevent Rate Limits and "Stuck" behavior
            with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                results = list(executor.map(get_content, prompts))
            return results
        else:
            results = []
            # Use progress bar for visible inference
            iterator = track(prompts, description="[cyan]ü§ñ Generating Responses...[/cyan]") if len(prompts) > 1 else prompts
            for prompt in iterator:
                try:
                    # Apply template
                    formatted = ""
                    try:
                        messages = [{"role": "user", "content": prompt}]
                        formatted = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                    except Exception:
                        if "qwen" in self.model_id:
                            formatted = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
                        else:
                            formatted = f"<start_of_turn>user\n{prompt}<end_of_turn>\n<start_of_turn>model\n"
                            
                    # Manual Generation (Bare Metal Stability)
                    inputs = self.tokenizer(formatted, return_tensors="pt", padding=True, truncation=True, max_length=4096).to(self.model.device)
                    
                    with torch.no_grad():
                        outputs = self.model.generate(
                            **inputs,
                            max_new_tokens=1024,
                            do_sample=False,
                            pad_token_id=self.tokenizer.eos_token_id
                        )
                        
                    # Decode only the new tokens
                    generated_tokens = outputs[0][inputs.input_ids.shape[1]:]
                    res_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
                    results.append(res_text)
                    
                except Exception as e:
                    console.print(f"[red]Generation Error: {e}[/red]")
                    results.append("")
            
            return results


    def _extract_json(self, text, is_list=False):
        """Robustly extract JSON from text even with markdown, newlines, or noise"""
        if not text or not text.strip():
            return None
            
        try:
            # Look for markdown blocks first
            code_blocks = re.findall(r"```(?:json)?\s*([\s\S]*?)\s*```", text)
            if code_blocks:
                content = code_blocks[0]
            else:
                # Fallback to finding brackets
                char_start, char_end = ('[', ']') if is_list else ('{', '}')
                start = text.find(char_start)
                end = text.rfind(char_end) + 1
                if start == -1 or end == 0: 
                    # Try to find at least the start
                    if start != -1:
                        content = text[start:]
                    elif is_list and text.find('{') != -1:
                        # Case: Model forgot [ ] but started with {
                        start = text.find('{')
                        content = text[start:]
                    else:
                        return None
                else:
                    content = text[start:end]
            
            # --- SANITIZATION STEP ---
            # 0. Ensure we have clean text - strip leading/trailing whitespace
            content = content.strip()
            
            # 1. Remove "..." if the model hallucinated it (as placeholder for truncation)
            content = content.replace("...", "")
            content = content.replace("‚Ä¶", "")  # Unicode ellipsis
            
            # 2. Normalize whitespace (convert all whitespace including tabs/newlines to single spaces)
            content = re.sub(r'\s+', ' ', content)
            
            # 3. Clean trailing commas (common LLM error)
            content = re.sub(r",\s*([\]}])", r"\1", content)
            
            # 4. Fix common LLM error: single quotes instead of double
            # Only apply if no double quotes exist (likely all single-quoted)
            if '"' not in content and "'" in content:
                content = content.replace("'", '"')

            try:
                return json.loads(content)
            except json.JSONDecodeError:
                # 5. If still failing, it might be truncated. Try to close it.
                if is_list:
                     # Attempt to wrap bare list if brackets missing
                    if not content.strip().startswith('['):
                        content = "[" + content
                    
                    if not content.strip().endswith(']'):
                         # Try rudimentary fixing
                        try: return json.loads(content + "]")
                        except: pass
                        try: return json.loads(content + "}]")
                        except: pass
                        try: return json.loads(content + "\"}]")
                        except: pass

                # 6. Recovery for OBJECTS (is_list=False)
                if not is_list:
                    # Attempt to close truncated JSON objects
                    # Try common closing patterns
                    candidates = ['}', '"}', '"]}', '"]}}', '"]}}', '']
                    for suffix in candidates:
                        try:
                            return json.loads(content + suffix)
                        except: pass
                    
                    # Try closing open quotes first if odd number of quotes
                    if content.count('"') % 2 != 0:
                        for suffix in candidates:
                            try:
                                return json.loads(content + '"' + suffix)
                            except: pass

                # 7. IMPROVED Last resort for LISTS: Use greedy regex for complete objects
                if is_list:
                    # Try to find objects with proper brace balancing
                    objects = []
                    brace_depth = 0
                    current_obj = ""
                    in_string = False
                    prev_char = ""
                    
                    for char in content:
                        if char == '"' and prev_char != '\\':
                            in_string = not in_string
                        
                        if not in_string:
                            if char == '{':
                                if brace_depth == 0:
                                    current_obj = ""
                                brace_depth += 1
                            elif char == '}':
                                brace_depth -= 1
                                if brace_depth == 0:
                                    current_obj += char
                                    objects.append(current_obj)
                                    current_obj = ""
                                    prev_char = char
                                    continue
                        
                        if brace_depth > 0:
                            current_obj += char
                        prev_char = char
                    
                    # If we captured objects, try to parse them
                    if objects:
                        fixed_json = "[" + ",".join(objects) + "]"
                        try: 
                            data = json.loads(fixed_json)
                            if self.debug: console.print(f"[dim green]DEBUG: Salvaged {len(data)} objects via brace-balanced parsing.[/dim green]")
                            return data
                        except:
                            pass
                    
                    # Fallback to simple regex
                    simple_objects = re.findall(r'\{[^{}]+\}', content)
                    if simple_objects:
                        fixed_json = "[" + ",".join(simple_objects) + "]"
                        try: 
                            data = json.loads(fixed_json)
                            if self.debug: console.print(f"[dim green]DEBUG: Salvaged {len(data)} objects via simple regex.[/dim green]")
                            return data
                        except: pass
                
                # 8. NESTED ARRAYS: Salvage complete inner [...] arrays from truncated response
                # This handles: [["kw1"], ["kw2"], ["kw3  <- incomplete
                # We extract all complete inner arrays
                # Check for nested arrays with optional whitespace: [ [ or [[
                if is_list and re.search(r'\[\s*\[', content):
                    inner_arrays = []
                    bracket_depth = 0
                    current_arr = ""
                    in_string = False
                    prev_char = ""
                    
                    for char in content:
                        # Track string state (respecting escaped quotes)
                        if char == '"' and prev_char != '\\':
                            in_string = not in_string
                        
                        # Handle brackets (only when not inside a string)
                        if not in_string:
                            if char == '[':
                                bracket_depth += 1
                                if bracket_depth == 2:  # Starting an inner array
                                    current_arr = "["
                                elif bracket_depth > 2:  # Nested bracket inside inner array
                                    current_arr += char
                            elif char == ']':
                                if bracket_depth == 2:  # Completing an inner array
                                    current_arr += "]"
                                    inner_arrays.append(current_arr)
                                    current_arr = ""
                                elif bracket_depth > 2:  # Nested bracket inside inner array
                                    current_arr += char
                                bracket_depth -= 1
                            else:
                                # Non-bracket char outside string, add if in inner array
                                if bracket_depth >= 2:
                                    current_arr += char
                        else:
                            # Inside a string, add to current array if we're in an inner array
                            if bracket_depth >= 2:
                                current_arr += char
                        
                        prev_char = char
                    
                    # If we captured complete inner arrays, parse them
                    if inner_arrays:
                        fixed_json = "[" + ",".join(inner_arrays) + "]"
                        try:
                            data = json.loads(fixed_json)
                            if self.debug: console.print(f"[dim green]DEBUG: Salvaged {len(data)} nested arrays from truncated response.[/dim green]")
                            return data
                        except Exception as e:
                            if self.debug: console.print(f"[dim yellow]DEBUG: Nested array salvage failed to parse: {e}. Arrays: {inner_arrays[:3]}...[/dim yellow]")
                            pass
                
                if self.debug: console.print(f"[dim red]DEBUG: Sanitization failed on: {content[:200]}...[/dim red]")
                return None
        except Exception as e:
            if self.debug:
                console.print(f"[dim red]DEBUG: JSON Parse error: {e}[/dim red]")
                console.print(f"[dim yellow]DEBUG: Raw text was: {text[:300]}...[/dim yellow]")
            return None

    def deduplicate_topics(self, topic_list):
        """
        Phase 4: Semantic Deduplication.
        Takes a list of topic names and returns a mapping {original: canonical}.
        """
        if not self.enabled or not topic_list:
            return {t: t for t in topic_list}

        unique_topics = list(set(topic_list))
        # No need to dedup if very few
        if len(unique_topics) < 2:
            return {t: t for t in topic_list}

        mapping = {t: t for t in topic_list}
        
        # Gemini API handles 100+ items easily, gemma/local models need smaller batches
        chunk_size = 100 if self.is_high_capacity_model else 20
        all_prompts = []
        chunks = []
        total_chunks = (len(unique_topics) + chunk_size - 1) // chunk_size
        for i in track(range(0, len(unique_topics), chunk_size), description="[cyan]Building dedup prompts...[/cyan]", total=total_chunks):
            chunk = unique_topics[i : i + chunk_size]
            chunks.append(chunk)
            chunk_str = "\n".join([f"- {t}" for t in chunk])
            
            prompt = f"""
                Vai tr√≤: Bi√™n t·∫≠p vi√™n Tin t·ª©c Cao c·∫•p.

                Nhi·ªám v·ª•:
                T·ª´ danh s√°ch d∆∞·ªõi ƒë√¢y, h√£y x√°c ƒë·ªãnh c√°c ti√™u ƒë·ªÅ c√πng ƒë·ªÅ c·∫≠p ƒë·∫øn M·ªòT s·ª± ki·ªán th·ª±c t·∫ø duy nh·∫•t.

                Hai ti√™u ƒë·ªÅ l√† C√ôNG M·ªòT S·ª∞ KI·ªÜN khi c√≥ ƒê√öNG 3 y·∫øu t·ªë:
                1. C√ôNG ƒê·ªäA ƒêI·ªÇM: "H√† N·ªôi" vs "H√† N·ªôi" ‚úì | "H√† N·ªôi" vs "TP.HCM" ‚úó
                2. C√ôNG TH·ªúI GIAN: "h√¥m nay" vs "h√¥m nay" ‚úì | "h√¥m nay" vs "tu·∫ßn tr∆∞·ªõc" ‚úó
                3. C√ôNG TH·ª∞C TH·ªÇ CH√çNH: "B√£o Yagi" vs "B√£o s·ªë 3" ‚úì | "B√£o Yagi" vs "B√£o Noru" ‚úó

                V√≠ d·ª• kh·ªõp/kh√¥ng kh·ªõp:
                - "Tai n·∫°n Qu·∫≠n 1" ‚â† "Tai n·∫°n Qu·∫≠n 7" (Kh√°c ƒë·ªãa ƒëi·ªÉm)
                - "Gi√° v√†ng tƒÉng h√¥m nay" ‚â† "Gi√° v√†ng tu·∫ßn tr∆∞·ªõc" (Kh√°c th·ªùi gian)
                - "Man Utd vs Liverpool" ‚â† "Arsenal vs Chelsea" (Kh√°c ƒë·ªôi b√≥ng)
                - "B√£o Yagi" = "C∆°n b√£o s·ªë 3 Yagi" (C√πng th·ª±c th·ªÉ - OK ƒë·ªÉ g·ªôp)

                QUY T·∫ÆC ƒê·∫¶U RA (NGHI√äM NG·∫∂T):
                - Ti√™u ƒë·ªÅ chu·∫©n (Canonical Title) PH·∫¢I l√† b·∫£n QUAY L·∫†I CH√çNH X√ÅC c·ªßa m·ªôt trong c√°c d√≤ng ƒë·∫ßu v√†o.
                - KH√îNG t·ª± t·∫°o ti√™u ƒë·ªÅ m·ªõi.
                - KH√îNG g·ªôp n·∫øu kh√¥ng ch·∫Øc ch·∫Øn.
                - Tr·∫£ v·ªÅ ƒë·ªëi t∆∞·ª£ng JSON: {{ "Ti√™u ƒë·ªÅ g·ªëc": "Ti√™u ƒë·ªÅ chu·∫©n" }}

                Danh s√°ch ti√™u ƒë·ªÅ ƒë·∫ßu v√†o:
                {chunk_str}

                ƒê·ªãnh d·∫°ng ƒë·∫ßu ra (Ch·ªâ JSON object):
            """
            all_prompts.append(prompt)
            
        if all_prompts:
            batch_texts = self._generate_batch(all_prompts)
            for i, text in enumerate(batch_texts):
                try:
                    results = self._extract_json(text, is_list=False)
                    if results:
                        for orig, canon in results.items():
                            if orig in mapping:
                                mapping[orig] = canon
                        if self.debug: 
                            console.print(f"[green]DEBUG: Deduped batch {i}: found {len(results)} mappings.[/green]")
                except Exception as e:
                    console.print(f"[red]Dedup error in batch {i}: {e}[/red]")
        
        return mapping
        
        return mapping

    def refine_trends(self, trends_dict):
        """
        Phase 6: Google Trends Refinement.
        Filters out generic/useless trends and merges duplicates.
        Returns: { "filtered": [...], "merged": { "variant": "canonical" } }
        """
        if not self.enabled or not trends_dict:
            return None

        trend_list = list(trends_dict.keys())
        
        # Categorical Grouping: Put related trends in the same batch for better merging
        keyword_groups = {
            "Sports": ["ƒë·∫•u v·ªõi", "vs", "cup", "b√≥ng ƒë√°", "t·ªâ s·ªë", "bxh", "ngo·∫°i h·∫°ng", "tr·ª±c ti·∫øp"],
            "Marketplace": ["gi√°", "v√†ng", "b·∫°c", "ti·ªÅn l∆∞∆°ng", "c√† ph√™", "xƒÉng"],
            "Lottery": ["x·ªï s·ªë", "s·ªë mi·ªÅn", "xs", "quay th·ª≠"],
            "Game": ["code", "wiki", "the forge", "riot", "honkai", "pubg", "roblox"],
            "General": []
        }
        
        buckets = {k: [] for k in keyword_groups.keys()}
        for t in trend_list:
            assigned = False
            t_lower = t.lower()
            for cat, kws in keyword_groups.items():
                if any(kw in t_lower for kw in kws):
                    buckets[cat].append(t)
                    assigned = True
                    break
            if not assigned:
                buckets["General"].append(t)

        all_filtered = []
        all_merged = {}
        
        console.print(f"[cyan]üßπ Refining {len(trend_list)} Google Trends with Categorical Grouping (Provider: {self.provider})...[/cyan]")
        
        all_prompts = []
        chunk_size = 300 if self.is_high_capacity_model else 100  # Gemini API handles massive lists
        
        for cat, items in buckets.items():
            if not items: continue
            for i in range(0, len(items), chunk_size):
                chunk = items[i : i + chunk_size]
                chunk_str = "\n".join([f"- {t}" for t in chunk])
                
                prompt = f"""
                    Role: Senior News Editor.
                        Context: Google Trending Searches in Vietnam.
                        Category hint: {cat}

                        Task:
                        1. FILTER: Remove terms that are clearly Generic, Utilities, or meaningless.
                           - NOISE: "x·ªï s·ªë", "k·∫øt qu·∫£", "th·ªùi ti·∫øt", "gi√° v√†ng", "l·ªãch v·∫°n ni√™n", "random chars"
                           - KEEP: "b√£o Yagi", "iPhone 16", "Man Utd vs Liverpool", "Blackpink"

                        2. MERGE: Group key terms referring to the EXACT SAME event.
                           - MUST use one of the input terms as the canonical term.
                           - Example: "l·ªãch thi ƒë·∫•u aff cup", "bxh aff cup" -> "AFF Cup 2024" (if present)
                           - Example: "gi√° xƒÉng h√¥m nay", "gi√° xƒÉng tƒÉng" -> "Gi√° xƒÉng d·∫ßu" (if present)

                        Input list:
                        {chunk_str}

                        Output (JSON ONLY):
                        {{
                        "filtered": ["term_to_remove", "term_to_remove"],
                        "merged": {{
                            "variant_term": "canonical_term"
                        }}
                        }}
                """
                all_prompts.append(prompt)
                
        if all_prompts:
            # Process one by one to show granular progress
            inference_batch_size = 1
            
            # Using rich progress track
            for i in track(range(0, len(all_prompts), inference_batch_size), description="[cyan]Processing Trend Batches...[/cyan]"):
                batch_prompts = all_prompts[i : i + inference_batch_size]
                batch_texts = self._generate_batch(batch_prompts)
                
                for text in batch_texts:
                    try:
                        results = self._extract_json(text, is_list=False)
                        if results:
                            all_filtered.extend(results.get("filtered", []))
                            all_merged.update(results.get("merged", {}))
                    except Exception as e:
                        console.print(f"[red]Trend Refine Parse Error: {e}[/red]")
        
        return {"filtered": all_filtered, "merged": all_merged}

    def filter_noise_trends(self, trend_list):
        """
        Ad-hoc filter for specific list of trends.
        """
        if not self.enabled: return []
        
        console.print(f"[cyan]üßπ Intelligent Noise Filtering via {self.provider} for {len(trend_list)} trends...[/cyan]")
        all_bad = []
        chunk_size = 500 if self.is_high_capacity_model else 50  # Gemini API handles 500+ items easily
        all_prompts = []
        total_chunks = (len(trend_list) + chunk_size - 1) // chunk_size
        
        for i in track(range(0, len(trend_list), chunk_size), description="[cyan]Building filter prompts...[/cyan]", total=total_chunks):
            chunk = trend_list[i:i+chunk_size]
            prompt = f"""
                Vai tr√≤: B·ªô l·ªçc ph√¢n lo·∫°i cho Google Trends (Vi·ªát Nam).
                Nhi·ªám v·ª•: Tr·∫£ v·ªÅ danh s√°ch c√°c t·ª´ kh√≥a l√† R√ÅC (NOISE) ho·∫∑c CHUNG CHUNG (GENERIC).

                ƒê·ªäNH NGHƒ®A R√ÅC (C·∫ßn lo·∫°i b·ªè):
                1. Th·ªùi ti·∫øt (Weather): "th·ªùi ti·∫øt h√¥m nay", "d·ª± b√°o m∆∞a", "aqi h√† n·ªôi" (TR·ª™ b√£o c√≥ t√™n nh∆∞ "B√£o Yagi")
                2. Ti·ªán √≠ch/D·ªãch v·ª•: "gi√° v√†ng", "gi√° xƒÉng", "l·ªãch √¢m", "x·ªï s·ªë", "xsmn", "vietlott"
                3. C√° c∆∞·ª£c/C·ªù b·∫°c: "bet88", "kubet", "soi c·∫ßu", "t·ª∑ l·ªá c∆∞·ª£c"
                4. C√¥ng ngh·ªá chung chung: "facebook", "gmail", "google", "login", "wifi"
                5. M∆° h·ªì/V√¥ nghƒ©a: "h√¨nh ·∫£nh", "video", "clip", "full", "hd", "review", "tin t·ª©c"
                6. Kh√°i ni·ªám qu√° r·ªông: "t√¨nh y√™u", "cu·ªôc s·ªëng", "h·ªçc t·∫≠p", "c√¥ng vi·ªác"

                ƒê·ªäNH NGHƒ®A S·ª∞ KI·ªÜN (C·∫ßn GI·ªÆ L·∫†I):
                - Nh√¢n v·∫≠t c·ª• th·ªÉ: "Taylor Swift", "Ph·∫°m Minh Ch√≠nh", "Quang H·∫£i"
                - V·ª• vi·ªác c·ª• th·ªÉ: "V·ª• ch√°y chung c∆∞ mini", "B√£o Yagi" (b√£o c√≥ t√™n ri√™ng)
                - Tr·∫≠n ƒë·∫•u/Gi·∫£i ƒë·∫•u: "MU vs Chelsea", "CKTG 2024"
                - S·∫£n ph·∫©m: "iPhone 15", "VinFast VF3"

                Danh s√°ch ƒë·∫ßu v√†o:
                {chunk}

                ƒê·∫ßu ra: M·∫£ng JSON ch·ª©a c√°c chu·ªói c·∫ßn LO·∫†I B·ªé.
                V√≠ d·ª•: ["th·ªùi ti·∫øt", "x·ªï s·ªë mi·ªÅn b·∫Øc"]
                """

            all_prompts.append(prompt)
            
        if all_prompts:
            if self.provider == 'gemini' and chunk_size > 1: # Optimize batch for Gemini
                 responses = [self._generate(p) for p in all_prompts] # Gemini SDK often better serial? actually _generate_batch handles it
            else:
                 responses = self._generate_batch(all_prompts)
                 
            for resp in responses:
                j = self._extract_json(resp, is_list=True)
                if j: all_bad.extend(j)
                
        return list(set(all_bad))
        

    def refine_cluster(self, cluster_name, posts, original_category=None, topic_type="Discovery", custom_instruction=None, keywords=None):
        if not self.enabled:
            return cluster_name, original_category, ""

        instruction = custom_instruction or """
            X√°c ƒë·ªãnh ti√™u ƒë·ªÅ v√† tr√≠ch xu·∫•t c·∫•u tr√∫c 5W1H. PH·∫¢I TR·∫¢ L·ªúI B·∫∞NG TI·∫æNG VI·ªÜT.

            QUY T·∫ÆC:
            1. Ti√™u ƒë·ªÅ (refined_title): Ti√™u ƒë·ªÅ tin t·ª©c ti·∫øng Vi·ªát s√∫c t√≠ch (‚â§ 15 t·ª´).
               - ∆Øu ti√™n c√°c s·ª± ki·ªán c·ª• th·ªÉ. Kh√¥ng gi·∫≠t g√¢n.
            2. T√≥m t·∫Øt (summary): CHI TI·∫æT L√Ä C·ª∞C K·ª≤ QUAN TR·ªåNG. Vi·∫øt m·ªôt ƒëo·∫°n vƒÉn D√ÄI, TO√ÄN DI·ªÜN (4-6 c√¢u).
               - Bao g·ªìm b·ªëi c·∫£nh, con s·ªë c·ª• th·ªÉ, tr√≠ch d·∫´n (n·∫øu c√≥) v√† h·ªá qu·∫£ t∆∞∆°ng lai.
               - KH√îNG b·∫Øt ƒë·∫ßu b·∫±ng "B√†i vi·∫øt n√≥i v·ªÅ..." hay "T√≥m t·∫Øt:". H√£y k·ªÉ c√¢u chuy·ªán tr·ª±c ti·∫øp.
               - PH·∫¢I VI·∫æT B·∫∞NG TI·∫æNG VI·ªÜT.
            3. 5W1H (Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát):
               - WHO: C√°c th·ª±c th·ªÉ/nh√¢n v·∫≠t ch√≠nh li√™n quan.
               - WHAT: T∆∞∆°ng t√°c ho·∫∑c s·ª± ki·ªán c·ªët l√µi.
               - WHERE: C√°c ƒë·ªãa ƒëi·ªÉm c·ª• th·ªÉ ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn.
               - WHEN: Khung th·ªùi gian/Ng√†y th√°ng.
               - WHY: Nguy√™n nh√¢n ho·∫∑c b·ªëi c·∫£nh.
               - N·∫æU KH√îNG BI·∫æT, ghi "N/A" nh∆∞ng h√£y C·ªê G·∫ÆNG tr√≠ch xu·∫•t.
            4. L·ªùi khuy√™n cho Nh√† n∆∞·ªõc (advice_state): ƒê∆∞a ra c√°c ki·∫øn ngh·ªã chi·∫øn l∆∞·ª£c cho c∆° quan ch·ª©c nƒÉng (v√≠ d·ª•: chi·∫øn l∆∞·ª£c truy·ªÅn th√¥ng, ƒëi·ªÅu ch·ªânh ch√≠nh s√°ch, qu·∫£n l√Ω kh·ªßng ho·∫£ng). PH·∫¢I VI·∫æT B·∫∞NG TI·∫æNG VI·ªÜT.
            5. L·ªùi khuy√™n cho Doanh nghi·ªáp (advice_business): ƒê∆∞a ra c√°c hi·ªÉu bi·∫øt c√≥ th·ªÉ h√†nh ƒë·ªông cho doanh nghi·ªáp (v√≠ d·ª•: th√¢m nh·∫≠p th·ªã tr∆∞·ªùng, gi·∫£m thi·ªÉu r·ªßi ro, thay ƒë·ªïi v·∫≠n h√†nh, t·∫≠n d·ª•ng c∆° h·ªôi). PH·∫¢I VI·∫æT B·∫∞NG TI·∫æNG VI·ªÜT.
            
            6. PH√ÇN LO·∫†I DANH M·ª§C (category) - CH·ªåN ƒê√öNG M·ªòT TRONG 7 LO·∫†I:
               - T1: Kh·ªßng ho·∫£ng & R·ªßi ro - Thi√™n tai, d·ªãch b·ªánh, tai n·∫°n th·∫£m kh·ªëc, kh·ªßng b·ªë, chi·∫øn tranh
                     V√ç D·ª§: ƒê·ªông ƒë·∫•t Nh·∫≠t B·∫£n, D·ªãch Covid, Ch√°y r·ª´ng, Xung ƒë·ªôt Ukraine-Nga
               - T2: Ch√≠nh s√°ch & Qu·∫£n tr·ªã - Lu·∫≠t m·ªõi, quy·∫øt ƒë·ªãnh ch√≠nh ph·ªß, b·∫ßu c·ª≠, ngo·∫°i giao
                     V√ç D·ª§: Qu·ªëc h·ªôi th√¥ng qua lu·∫≠t, Th·ªß t∆∞·ªõng thƒÉm n∆∞·ªõc ngo√†i, Ch√≠nh s√°ch thu·∫ø m·ªõi
               - T3: R·ªßi ro Uy t√≠n - Scandal, b√™ b·ªëi, tham nh≈©ng, l·ª´a ƒë·∫£o, ki·ªán t·ª•ng
                     V√ç D·ª§: Quan ch·ª©c b·ªã b·∫Øt, C√¥ng ty b·ªã ph·∫°t, Ngh·ªá sƒ© d√≠nh scandal
               - T4: C∆° h·ªôi Th·ªã tr∆∞·ªùng - Kinh t·∫ø, t√†i ch√≠nh, b·∫•t ƒë·ªông s·∫£n, ƒë·∫ßu t∆∞, startup
                     V√ç D·ª§: VN-Index tƒÉng, N√¥ng nghi·ªáp c√¥ng ngh·ªá cao, Kh·ªüi nghi·ªáp th√†nh c√¥ng
               - T5: VƒÉn h√≥a & Gi·∫£i tr√≠ - Th·ªÉ thao, phim ·∫£nh, √¢m nh·∫°c, l·ªÖ h·ªôi, du l·ªãch
                     V√ç D·ª§: SEA Games, Phim Vi·ªát ƒëo·∫°t gi·∫£i, L·ªÖ h·ªôi Xu√¢n, Vietnam Idol
               - T6: V·∫≠n h√†nh & D·ªãch v·ª• - Giao th√¥ng, y t·∫ø, gi√°o d·ª•c, ti·ªán √≠ch c√¥ng
                     V√ç D·ª§: Cao t·ªëc k·∫πt xe, B·ªánh vi·ªán qu√° t·∫£i, Tr∆∞·ªùng h·ªçc m·ªü c·ª≠a, M·∫•t ƒëi·ªán
               - T7: Tin ƒë·ªãnh k·ª≥ - Th·ªùi ti·∫øt, d·ª± b√°o, th·ªëng k√™ th∆∞·ªùng nh·∫≠t
                     V√ç D·ª§: D·ª± b√°o th·ªùi ti·∫øt, Gi√° v√†ng h√¥m nay, T·ª∑ gi√° USD

            Ph·∫£n h·ªìi NGHI√äM NG·∫∂T theo ƒë·ªãnh d·∫°ng JSON:
            {{
                "refined_title": "...",
                "category": "T1 ho·∫∑c T2 ho·∫∑c T3 ho·∫∑c T4 ho·∫∑c T5 ho·∫∑c T6 ho·∫∑c T7",
                "event_type": "Specific/Generic",
                "summary": "C√¢u chuy·ªán chi ti·∫øt ƒë·∫ßy ƒë·ªß v·ªÅ s·ª± ki·ªán (kho·∫£ng 100-150 t·ª´).",
                "overall_sentiment": "Positive/Negative/Neutral",
                "who": "...",
                "what": "...",
                "where": "...",
                "when": "...",
                "why": "...",
                "advice_state": "L·ªùi khuy√™n chi·∫øn l∆∞·ª£c cho c∆° quan ch·ª©c nƒÉng...",
                "advice_business": "L·ªùi khuy√™n th·ª±c ti·ªÖn cho doanh nghi·ªáp...",
                "reasoning": "Gi·∫£i th√≠ch t·∫°i sao ch·ªçn category n√†y..."
            }}
        """

        context_texts = [p.get('content', '')[:300] for p in posts[:5]]
        context_str = "\n---\n".join(context_texts)
        
        # Extract metadata
        dates = sorted(list(set([str(p.get('time') or p.get('published_at', '')).split('T')[0] for p in posts if p.get('time') or p.get('published_at')])))
        meta_info = f"Date Range: {dates[0]} to {dates[-1]}" if dates else "Date: Unknown"
        
        # Keywords
        kw_str = f"Keywords: {', '.join(keywords)}" if keywords else ""

        prompt = f"""
            Ph√¢n t√≠ch c·ª•m b√†i vi·∫øt m·∫°ng x√£ h·ªôi/tin t·ª©c n√†y t·ª´ Vi·ªát Nam. PH·∫¢I TR·∫¢ L·ªúI B·∫∞NG TI·∫æNG VI·ªÜT.
            T√™n g·ªëc: {cluster_name}
            Lo·∫°i ch·ªß ƒë·ªÅ: {topic_type}
            {meta_info}
            {kw_str}
            
            B√†i vi·∫øt m·∫´u:
            {context_str}

            {instruction}

            ƒê·ªãnh d·∫°ng tr·∫£ v·ªÅ NGHI√äM NG·∫∂T l√† JSON:
                {{
                    "refined_title": "Ti√™u ƒë·ªÅ ti·∫øng Vi·ªát",
                    "category": "T1/T2/.../T7",
                    "event_type": "Specific/Generic",
                    "summary": "T√≥m t·∫Øt chi ti·∫øt b·∫±ng ti·∫øng Vi·ªát...",
                    "overall_sentiment": "Positive/Negative/Neutral",
                    "who": "...",
                    "what": "...",
                    "where": "...",
                    "when": "...",
                    "why": "...",
                    "advice_state": "L·ªùi khuy√™n cho Nh√† n∆∞·ªõc b·∫±ng ti·∫øng Vi·ªát...",
                    "advice_business": "L·ªùi khuy√™n cho Doanh nghi·ªáp b·∫±ng ti·∫øng Vi·ªát...",
                    "reasoning": "Gi·∫£i th√≠ch b·∫±ng ti·∫øng Vi·ªát"
                }}
        """
        try:
            text = self._generate(prompt)
            data = self._extract_json(text, is_list=False)
            if data:
                return (
                    data.get('refined_title', cluster_name), 
                    data.get('category', original_category), 
                    data.get('reasoning', ""), 
                    data.get('event_type', "Specific"),
                    data.get('summary', ""),
                    data.get('overall_sentiment', 'Neutral'),
                    {
                        "who": data.get('who', 'N/A'),
                        "what": data.get('what', 'N/A'),
                        "where": data.get('where', 'N/A'),
                        "when": data.get('when', 'N/A'),
                        "why": data.get('why', 'N/A'),
                        "advice_state": data.get('advice_state', 'N/A'),
                        "advice_business": data.get('advice_business', 'N/A')
                    }
                )
            return cluster_name, original_category, "", "Specific", "", "Neutral", {}
        except Exception:
            return cluster_name, original_category, "", "Specific", "", "Neutral", {}

    def refine_batch(self, clusters_to_refine, custom_instruction=None, generate_summary=True):
        if not self.enabled or not clusters_to_refine:
            return {}

        instruction = custom_instruction or """
            Vai tr√≤: Bi√™n t·∫≠p vi√™n Tin t·ª©c Cao c·∫•p (Vi·ªát Nam).
            Nhi·ªám v·ª•: ƒê·∫∑t l·∫°i t√™n cho c·ª•m tin th√†nh m·ªôt ti√™u ƒë·ªÅ ti·∫øng Vi·ªát duy nh·∫•t, ch·∫•t l∆∞·ª£ng cao. PH·∫¢I TR·∫¢ L·ªúI B·∫∞NG TI·∫æNG VI·ªÜT.

            Quy t·∫Øc Ti√™u ƒë·ªÅ:
            1. S√∫c t√≠ch & Th·ª±c t·∫ø (‚â§ 15 t·ª´).
            2. Ph·∫£i ch·ª©a c√°c Th·ª±c th·ªÉ c·ª• th·ªÉ (Who/Where/What).
            3. Gi·ªçng vƒÉn trung t√≠nh (Kh√¥ng gi·∫≠t g√¢n).
            4. S·ª≠ d·ª•ng ti·∫øng Vi·ªát chu·∫©n (v√≠ d·ª•: "TP.HCM" thay v√¨ "S√†i G√≤n" trong b·ªëi c·∫£nh trang tr·ªçng).
            
            QUAN TR·ªåNG - X·ª≠ l√Ω C·ª•m tin H·ªón h·ª£p:
            - N·∫øu c√°c b√†i vi·∫øt ƒë·ªÅ c·∫≠p ƒë·∫øn nhi·ªÅu s·ª± ki·ªán KH√îNG LI√äN QUAN (v√≠ d·ª•: "Apple iPhone" V√Ä "L≈© l·ª•t ·ªü Hu·∫ø"):
              - KH√îNG k·∫øt h·ª£p ch√∫ng (v√≠ d·ª•: "Apple ra iPhone v√† L≈© l·ª•t ·ªü Hu·∫ø" l√† SAI).
              - CH·ªåN CH·ª¶ ƒê·ªÄ TH·ªêNG TR·ªä (ch·ªß ƒë·ªÅ c√≥ nhi·ªÅu b√†i vi·∫øt h∆°n ho·∫∑c gi√° tr·ªã tin t·ª©c cao h∆°n).
              - Ch·ªâ t·∫°o ti√™u ƒë·ªÅ cho ch·ªß ƒë·ªÅ th·ªëng tr·ªã ƒë√≥.
              - ƒê·ªÅ c·∫≠p ƒë·∫øn ch·ªß ƒë·ªÅ b·ªã lo·∫°i b·ªè trong tr∆∞·ªùng 'reasoning'.

            C·∫¢NH B√ÅO - C·ª•m tin kh√¥ng nh·∫•t qu√°n (KI·ªÇM TRA T·ª™NG B∆Ø·ªöC):
            1. X√°c ƒë·ªãnh CH·ª¶ ƒê·ªÄ C·ªêT L√ïI t·ª´ B√†i vi·∫øt 1 (B√†i vi·∫øt neo).
            2. V·ªõi m·ªói B√†i vi·∫øt 2-5, h·ªèi: "B√†i vi·∫øt n√†y c√≥ m√¥ t·∫£ C√ôNG M·ªòT s·ª± ki·ªán c·ª• th·ªÉ nh∆∞ B√†i vi·∫øt 1 kh√¥ng?"
               - C√ôNG: C√πng ƒë·ªãa ƒëi·ªÉm V√Ä c√πng lo·∫°i s·ª± c·ªë V√Ä c√πng khung th·ªùi gian.
               - KH√ÅC: Kh√°c ƒë·ªãa ƒëi·ªÉm HO·∫∂C kh√°c lo·∫°i s·ª± c·ªë HO·∫∂C kh√°c th·ªùi gian.
            3. N·∫øu KH√ÅC, th√™m s·ªë th·ª© t·ª± b√†i vi·∫øt ƒë√≥ v√†o outlier_ids.

            QUY T·∫ÆC T√ìM T·∫ÆT (summary):
            - VI·∫æT M·ªòT ƒêO·∫†N T√ìM T·∫ÆT D√ÄI, CHI TI·∫æT (4-6 c√¢u, ~100 t·ª´).
            - Bao g·ªìm b·ªëi c·∫£nh, c√°c nh√¢n v·∫≠t ch√≠nh v√† di·ªÖn bi·∫øn s·ª± vi·ªác.
            - PH·∫¢I VI·∫æT B·∫∞NG TI·∫æNG VI·ªÜT.

            QUY T·∫ÆC 5W1H (Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát):
            - Tr√≠ch xu·∫•t chi ti·∫øt c·ª• th·ªÉ cho Who/What/Where/When/Why.

            L·ªùi khuy√™n Chi·∫øn l∆∞·ª£c (advice_state, advice_business):
            - PH·∫¢I VI·∫æT B·∫∞NG TI·∫æNG VI·ªÜT.

            K·∫øt qu·∫£ tr·∫£ v·ªÅ JSON:
            {
                "id": 0,
                "refined_title": "Chu·ªói ti·∫øng Vi·ªát",
                "summary": "ƒêo·∫°n vƒÉn chi ti·∫øt b·∫±ng ti·∫øng Vi·ªát.",
                "overall_sentiment": "Positive/Negative/Neutral",
                "who": "...",
                "what": "...",
                "where": "...",
                "when": "...",
                "why": "...",
                "advice_state": "...",
                "advice_business": "...",
                "outlier_ids": [id1, id2],
                "reasoning": "Gi·∫£i th√≠ch b·∫±ng ti·∫øng Vi·ªát"
            }
        """

        # Chunking: Small LLMs (Gemma) or large batches can exceed context limits
        # [QUOTA OPTIMIZATION] For Gemini Free Tier, reduce chunk size to stay under token limits (e.g. 15k tokens/min)
        chunk_size = 10 if self.is_high_capacity_model else 3  # Gemini API can handle more clusters
        all_results = {}
        
        # Build prompts
        all_prompts = []
        cluster_ids_per_chunk = []
        total_chunks = (len(clusters_to_refine) + chunk_size - 1) // chunk_size

        for i in track(range(0, len(clusters_to_refine), chunk_size), description="[cyan]Building cluster prompts...[/cyan]", total=total_chunks):
            chunk = clusters_to_refine[i : i + chunk_size]
            cluster_ids_per_chunk.append([c['label'] for c in chunk])
            
            batch_str = ""
            for c in chunk:
                # Increase context for better reasoning
                context_list = []
                for j, p in enumerate(c['sample_posts'][:5]): # Up to 5 posts
                    p_text = p.get('content', '')[:500] # Up to 500 chars
                    context_list.append(f"[Post {j+1}] {p_text}")
                
                context = "\n".join(context_list)
                
                # Extract metadata
                dates = []
                for p in c['sample_posts']:
                    d = p.get('published_at') or p.get('time')
                    if d: dates.append(str(d).split('T')[0]) # YYYY-MM-DD
                
                date_context = ""
                if dates:
                    unique_dates = sorted(list(set(dates)))
                    if len(unique_dates) > 1:
                        date_context = f" [Timeframe: {unique_dates[0]} to {unique_dates[-1]}]"
                    else:
                        date_context = f" [Date: {unique_dates[0]}]"

                # Keywords
                kw_str = f"Keywords: {', '.join(c.get('keywords', []))}" if c.get('keywords') else ""

                batch_str += f"### Cluster ID: {c['label']}\nName: {c['name']}{date_context}\n{kw_str}\nContext Samples (Post 1 is Anchor):\n{context}\n\n"

            json_template = '[ {{"id": 0, "refined_title": "Title", "summary": "Detailed summary...", "overall_sentiment": "...", "who": "...", "what": "...", "where": "...", "when": "...", "why": "...", "advice_state": "...", "advice_business": "...", "outlier_ids": [], "reasoning": "..."}} ]'
            
            prompt = f"""
            Analyze nh·ªØng {len(chunk)} news/social clusters n√†y t·ª´ Vi·ªát Nam.
            {instruction}

            RULES:
            1. Output ONLY a JSON array, nothing else
            2. Start your response with [ and end with ]
            3. Each cluster must have: id, refined_title, summary, outlier_ids, reasoning
            4. outlier_ids are the post numbers (1, 2, 3, 4, 5) from context that DON'T match Post 1.

            Input Clusters:
            {batch_str}

            Respond with ONLY this JSON (no other text):
            {json_template}
            """
            all_prompts.append(prompt)

        if all_prompts:
            batch_texts = self._generate_batch(all_prompts)
            for i, text in enumerate(batch_texts):
                try:
                    results = self._extract_json(text, is_list=True)
                    if results:
                        for item in results:
                            if isinstance(item, dict) and 'id' in item:
                                # Ensure minimal schema to prevent KeyErrors
                                sane_item = {
                                    'id': item['id'],
                                    'refined_title': item.get('refined_title', f"Cluster {item['id']}"),
                                    'summary': item.get('summary', 'No summary provided'),
                                    'overall_sentiment': item.get('overall_sentiment', 'Neutral'),
                                    'who': item.get('who', 'N/A'),
                                    'what': item.get('what', 'N/A'),
                                    'where': item.get('where', 'N/A'),
                                    'when': item.get('when', 'N/A'),
                                    'why': item.get('why', 'N/A'),
                                    'advice_state': item.get('advice_state', 'N/A'),
                                    'advice_business': item.get('advice_business', 'N/A'),
                                    'outlier_ids': item.get('outlier_ids', []),
                                    'reasoning': item.get('reasoning', 'No reasoning provided')
                                }
                                all_results[item['id']] = sane_item
                        
                        # Log a sample to show it's working
                        # Find first valid dict result for sample logging
                        valid_samples = [r for r in results if isinstance(r, dict) and 'id' in r]
                        if valid_samples:
                            sample = valid_samples[0]
                            console.print(f"      ‚ú® [green]Refined {len(valid_samples)} clusters. Sample ID {sample.get('id')}: {sample.get('refined_title')}[/green]")
                        else:
                            console.print(f"[yellow]‚ö†Ô∏è Chunk {i+1}: Parsed {len(results)} items but none were valid cluster dicts[/yellow]")
                    else:
                        console.print(f"[yellow]‚ö†Ô∏è Could not find JSON list in LLM response for chunk {i+1}[/yellow]")
                        if self.debug:
                            # Show first 500 chars of response for debugging
                            console.print(f"[dim yellow]DEBUG Raw Response (first 500 chars):[/dim yellow]")
                            console.print(f"[dim]{text[:500]}[/dim]")
                except Exception as e:
                    console.print(f"[red]Batch LLM error in chunk {i+1}: {type(e).__name__}: {e}[/red]")
                    # Show response preview for debugging even without debug mode
                    console.print(f"[dim red]Response preview: {text[:200] if text else 'empty'}...[/dim red]")
                    if self.debug:
                        import traceback
                        console.print(f"[dim red]{traceback.format_exc()}[/dim red]")
        
        return all_results

    def classify_batch(self, topic_data_list):
        """
        Classify a batch of topics into Categories (A/B/C) and Event Types (Specific/Generic).
        
        Args:
            topic_data_list: List of dicts, each containing:
                - id: Unique ID
                - label: Refined title
                - reasoning: Reasoning from Phase 3
                
        Returns:
            Dict mapping id -> {category, event_type, reasoning}
        """
        if not self.enabled:
            return {item['id']: {'category': 'T5', 'event_type': 'Specific', 'reasoning': 'LLM Disabled'} for item in topic_data_list}

        results = {}
        batch_size = 10  # Process 10 classifications at a time
        
        # Prepare batches
        batches = [topic_data_list[i:i + batch_size] for i in range(0, len(topic_data_list), batch_size)]
        
        all_prompts = []
        all_items_ordered = []
        
        for batch in batches:
            # Construct prompt for the batch
            batch_items = []
            for item in batch:
                # Defensive check for required keys
                try:
                    batch_items.append({
                        "id": item.get('id', item.get('final_topic', 'unknown')),
                        "title": item.get('label', item.get('final_topic', 'Unknown Topic')),
                        "context": item.get('reasoning', '')[:200]
                    })
                except Exception:
                    continue
            
            if not batch_items:
                continue

            batch_str = json.dumps(batch_items, ensure_ascii=False, indent=2)
            
            prompt = f"""
            Role: Crisis & Event Classifier for Vietnam.
            
            Task: Classify each topic into one of the following 7 Usage Groups:
            - T1 (Crisis & Public Risk): Accidents, fires, natural disasters, epidemics, riots.
            - T2 (Policy & Governance): New regulations, policy announcements, government statements.
            - T3 (Reputation & Trust): Scandals, accusations, boycotts, controversies.
            - T4 (Market Opportunity): Product trends, lifestyle changes, tech adoption.
            - T5 (Cultural & Attention): Memes, celebrities, entertainment, viral noise.
            - T6 (Operational Pain): Traffic, power outages, public service failures.
            - T7 (Routine Signals): Weather updates, lottery, daily sports results.
               
            2. EVENT TYPE:
               - Specific: A concrete event with a distinct start/end and clear actors (e.g., "B√£o Yagi", "V·ª• ch√°y chung c∆∞ A", "Khai m·∫°c h·ªôi ngh·ªã X").
               - Generic: Broad topics, recurring reports, or vague discussions (e.g., "T√¨nh h√¨nh th·ªùi ti·∫øt", "Gi√° xƒÉng h√¥m nay", "Chuy·ªán ƒë·ªùi th∆∞·ªùng", "Th√¥ng tin th·ªã tr∆∞·ªùng"). 
               - RULE: If it's a routine update without a "breaking" news point, mark as GENERIC.
               
            Input Topics:
            {batch_str}
            
            Output: JSON Object mapping ID -> Classification.
            Example:
            {{
                "0": {{ 
                    "category": "T1", 
                    "event_type": "Specific", 
                    "overall_sentiment": "Positive/Negative/Neutral",
                    "summary": "Short context of the event.",
                    "reasoning": "..." 
                }}
            }}
            """
            all_prompts.append(prompt)
            all_items_ordered.extend(batch)
            
        # Execute batch generation
        if not all_prompts: return {}
        
        console.print(f"[cyan]üõ°Ô∏è Classifying {len(topic_data_list)} topics in {len(batches)} batches...[/cyan]")
        
        responses = self._generate_batch(all_prompts)
        
        # Process results
        current_idx = 0
        for i, resp in enumerate(responses):
            batch_items = batches[i]
            parsed = self._extract_json(resp, is_list=False)
            
            if not parsed:
                # Fallback if parsing fails
                for item in batch_items:
                    results[item['id']] = {"category": "T5", "event_type": "Specific", "reasoning": "Parse Error"}
                continue
                
            for item in batch_items:
                # ID might be int or str in JSON keys
                item_id = item.get('id') or item.get('final_topic', 'unknown')
                key = str(item_id)
                if key in parsed:
                    info = parsed[key]
                    results[item_id] = {
                        "category": info.get("category", "T5"),
                        "event_type": info.get("event_type", "Specific"),
                        "overall_sentiment": info.get("overall_sentiment", "Neutral"),
                        "summary": info.get("summary", ""),
                        "reasoning": info.get("reasoning", "")
                    }
                else:
                    results[item_id] = {"category": "T5", "event_type": "Specific", "overall_sentiment": "Neutral", "summary": "", "reasoning": "Missing in response"}

        return results

    def summarize_text(self, text, max_words=100):
        """
        Summarize a long text into a concise paragraph.
        """
        if not self.enabled or not text: return text
        
        prompt = f"""
    Role: Senior Editor.
    Task: Summarize the following article in Vietnamese (max {max_words} words).
    Keep the main entities, numbers, and key events. Delete fluff.

    Input:
    {text[:4000]} # Limit input to avoid token overflow even on LLM side

    Result:
    """
        try:
            summary = self._generate(prompt)
            # Basic cleanup
            return summary.replace("Summary:", "").strip()
        except Exception:
            return text[:500] # Fallback to truncation
