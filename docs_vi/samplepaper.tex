% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[utf8]{inputenc}
\usepackage[T1,T5]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[a4paper,left=30mm,right=30mm]{geometry}
%
\begin{document}
%
\title{Hệ thống Phát hiện Sự kiện Thời gian Thực: Tích hợp Luồng Mạng Xã hội và Tin tức Chính thống với Trí tuệ LLM}
\titlerunning{Hệ thống Phát hiện Sự kiện Thời gian Thực}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Tăng Nhất\inst{1} \and
    Lê Minh Nhựt\inst{1} \and
    Đỗ Trọng Hợp\inst{2} \and
    Nguyễn Ngọc Quý\inst{2}}
%

% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Khoa Khoa học Máy tính\\
    Trường Đại học Thông tin, VNU-HCM\\
    \email{\{22521035, 22521060\}@gm.uit.edu.vn} \and
    Khoa Khoa học Máy tính\\
    Trường Đại học Thông tin, VNU-HCM\\
    \email{\{hopdt, quynn\}@uit.edu.vn}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
    Trong thời đại quá tải thông tin, việc phát hiện và hiểu nhanh các sự kiện mới nổi từ luồng mạng xã hội là vô cùng quan trọng cho an toàn công cộng và thông tin thị trường.
    Tuy nhiên, các hệ thống thực tế phải đối mặt với hai thách thức cốt lõi: (i) \textit{sự mơ hồ ngữ nghĩa} trong các bài đăng ngắn, nhiễu, và (ii) \textit{khoảng trống đánh giá} do thiếu nhãn trong dữ liệu luồng.
    Chúng tôi trình bày một \textbf{hệ thống phát hiện sự kiện thời gian thực} được xây dựng trên chiến lược \textbf{Phân cụm Phân cấp Nhận thức Mạng xã hội (SAHC)} mới lạ và \textbf{kiến trúc hai đường} kết hợp Spark/Kafka cho phát hiện độ trễ thấp với các worker LLM bất đồng bộ cho tinh chỉnh ngữ nghĩa sâu.
    Trên tập con \textbf{Mini-Ground Truth} (N=300 bài đăng được gán nhãn thủ công), pipeline lai đầy đủ đạt \textbf{NMI = 0.5978}.
    Trên bộ dữ liệu đầy đủ gồm 7{,}605 mục, chúng tôi báo cáo các chỉ số phân cụm nội tại (Silhouette, Davies--Bouldin, tỷ lệ nhiễu) và giao thức \textbf{LLM-as-a-Judge} để đánh giá tính nhất quán ngữ nghĩa ở quy mô lớn.

    \keywords{Phát hiện Sự kiện \and Khai phá Mạng Xã hội \and Xử lý Luồng Thời gian Thực \and Mô hình Ngôn ngữ Lớn \and Phân cụm Lai.}
\end{abstract}

%
%
%
\section{Giới thiệu}
Các nền tảng mạng xã hội đã trở thành nguồn thông tin thời gian thực chính. Tuy nhiên, việc xử lý dữ liệu này đặt ra những thách thức độc đáo: (1) \textbf{Tỷ lệ Nhiễu Cao:} Tin tức có giá trị thường bị chìm trong spam và các cuộc trò chuyện hàng ngày (ví dụ: "Kết quả xổ số", "Thời tiết"); (2) \textbf{Sự Mơ hồ Ngữ nghĩa:} Các cộng đồng khác nhau sử dụng từ vựng hoàn toàn khác nhau để mô tả cùng một sự kiện (ví dụ: "Bão Yagi" so với "Bão số 3"); và (3) \textbf{Thiếu Xác minh:} Tin đồn lan truyền nhanh hơn việc đính chính thực tế.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/dashboard.png}
    \caption{Bảng điều khiển Hệ thống: Trực quan hóa thời gian thực các sự kiện xu hướng và cảm xúc của chúng.} \label{fig:dashboard}
\end{figure}

Như minh họa trong Hình \ref{fig:dashboard}, bảng điều khiển hệ thống tích hợp ba chế độ xem chính: (1) Danh sách xếp hạng thời gian thực các sự kiện xu hướng; (2) Biểu đồ chuỗi thời gian theo dõi sự tiến hóa cảm xúc; và (3) Bản đồ địa lý làm nổi bật các điểm nóng sự kiện. Giao diện này cho phép các bên liên quan nhanh chóng nắm bắt "Cái gì, Ở đâu và Như thế nào" của các vấn đề mới nổi.

\noindent\textbf{Khoảng trống Nghiên cứu.}
Các pipeline phát hiện sự kiện hiện có thường tối ưu hóa cho (i) phát hiện luồng độ trễ thấp sử dụng phân cụm thống kê hoặc dựa trên embedding nhẹ, hoặc (ii) tính nhất quán ngữ nghĩa cao và khả năng diễn giải sử dụng suy luận neural/LLM nặng hơn.
Tuy nhiên, các triển khai thực tế đòi hỏi \emph{cả ba} đồng thời: \textbf{(a) độ trễ thời gian thực}, \textbf{(b) nhóm sự kiện mạch lạc với văn bản ngắn nhiễu}, và \textbf{(c) đánh giá có khả năng mở rộng} khi nhãn ground-truth khan hiếm trong luồng.
Điều này tạo ra khoảng cách giữa \emph{ràng buộc thời gian thực cấp hệ thống} và \emph{sự chặt chẽ ngữ nghĩa/đánh giá}.

\noindent\textbf{Câu hỏi Nghiên cứu.}
Chúng tôi nghiên cứu các câu hỏi sau:
\begin{itemize}
    \item \textbf{RQ1:} Làm thế nào chúng ta có thể phát hiện các sự kiện mới nổi từ luồng mạng xã hội nhiễu với ràng buộc độ trễ nghiêm ngặt ($<10$s) trong khi giảm thiểu sự phân mảnh và nhiễu?
    \item \textbf{RQ2:} Làm thế nào chúng ta có thể cải thiện tính nhất quán ngữ nghĩa và khả năng diễn giải sự kiện mà không chặn pipeline luồng?
    \item \textbf{RQ3:} Làm thế nào chúng ta có thể đánh giá chất lượng phân cụm ở quy mô khi nhãn người đầy đủ không có sẵn cho dữ liệu luồng?
\end{itemize}

Để trả lời các câu hỏi này, chúng tôi đề xuất một \textbf{Dual-Path Architecture} tách biệt phát hiện độ trễ thấp khỏi tinh chỉnh ngữ nghĩa.
Trên Fast Path, chúng tôi thực hiện gắn kết và khám phá luồng thông qua chiến lược \textbf{Phân cụm Phân cấp Nhận thức Mạng xã hội (SAHC)} để giảm nhiễu và phân mảnh quá mức.
Trên Slow Path, các worker LLM bất đồng bộ tạo các tóm tắt cấu trúc 5W1H và hỗ trợ kiểm soát chất lượng ngữ nghĩa.
Để lấp khoảng trống đánh giá, chúng tôi kết hợp giao thức \textbf{Mini-Ground Truth} (nhãn thủ công trên tập con nhỏ) với quy trình \textbf{LLM-as-a-Judge} để đánh giá tính nhất quán ngữ nghĩa ở quy mô.

Các đóng góp cụ thể của chúng tôi bao gồm:
\begin{enumerate}
    \item Một \textbf{Dual-Path Architecture} cân bằng độ trễ < 10s cho phát hiện và suy luận LLM sâu cho insight.
    \item Một thuật toán \textbf{Phân cụm Phân cấp Nhận thức Mạng xã hội (SAHC)} sử dụng neo tin tức để giảm nhiễu khoảng ~50\% so với HDBSCAN gốc cùng embedding \& tiền xử lý.
    \item Một đánh giá chặt chẽ sử dụng cả bộ dữ liệu \textbf{Mini-Ground Truth} (N=300) và các chỉ số \textbf{LLM-as-a-Judge}.
\end{enumerate}

\section{Công trình Liên quan}
Phát hiện sự kiện từ mạng xã hội đã phát triển đáng kể trong thập kỷ qua, tiến triển từ các phương pháp thống kê đến các phương pháp neural và, gần đây nhất, đến các hệ thống được tăng cường bởi Mô hình Ngôn ngữ Lớn (LLM).
Chúng tôi tổ chức tài liệu liên quan thành bốn loại: (1) các phương pháp thống kê truyền thống, (2) các phương pháp neural và dựa trên embedding, (3) các phương pháp LLM và Tạo sinh Tăng cường Truy xuất (RAG), và (4) các kiến trúc luồng thời gian thực.
Bảng \ref{tab:related_work} cung cấp tóm tắt so sánh các phương pháp đại diện.

\subsection{Các Phương pháp Thống kê Truyền thống}
Các hệ thống phát hiện sự kiện ban đầu phụ thuộc nhiều vào phân tích tần suất từ và mô hình hóa chủ đề.
Các phương pháp \textbf{TF-IDF và BM25} \cite{ref_bm25} xác định các từ khóa bùng nổ lệch khỏi mô hình tần suất bình thường, cho phép phát hiện nhanh các chủ đề xu hướng.
\textbf{Latent Dirichlet Allocation (LDA)} \cite{ref_lda} và các biến thể trực tuyến của nó mô hình hóa tài liệu như hỗn hợp các chủ đề ẩn, cho phép khám phá không giám sát các chủ đề sự kiện.
\textbf{EDCoW} \cite{ref_edcow} giới thiệu xử lý tín hiệu dựa trên wavelet để phát hiện các từ khóa ``bùng nổ'' trong luồng Twitter, sử dụng đồ thị đồng xuất hiện để phân cụm các thuật ngữ liên quan thành sự kiện.

\textbf{Điểm mạnh:} Các phương pháp này hiệu quả về tính toán và có thể diễn giải, làm cho chúng phù hợp với các kịch bản thông lượng cao.
\textbf{Hạn chế:} Chúng gặp khó khăn với các văn bản mạng xã hội ngắn, nhiễu nơi sự đồng xuất hiện từ thưa thớt, và chúng không thể nắm bắt sự tương đồng ngữ nghĩa giữa các biểu thức khác nhau về từ vựng (ví dụ: ``Bão Yagi'' so với ``Bão số 3'').

\subsection{Các Phương pháp Neural và Dựa trên Embedding}
Sự ra đời của học sâu cho phép biểu diễn ngữ nghĩa của văn bản, cải thiện đáng kể chất lượng phát hiện sự kiện.
\textbf{BERT} \cite{ref_bert} và các biến thể của nó cung cấp nhúng từ ngữ cảnh hóa nắm bắt ý nghĩa tinh tế.
\textbf{Sentence-BERT (SBERT)} \cite{ref_sbert} thay đổi BERT cho tính toán tương đồng cấp câu hiệu quả sử dụng mạng Siamese, cho phép tìm kiếm lân cận nhanh trong không gian embedding.
\textbf{BERTopic} \cite{ref_bertopic} kết hợp embeddings transformer với phân cụm (HDBSCAN) và TF-IDF dựa trên lớp để tạo biểu diễn chủ đề mạch lạc.

Các thuật toán phân cụm tạo thành xương sống của nhiều hệ thống phát hiện sự kiện neural.
\textbf{K-Means} \cite{ref_kmeans} phân chia dữ liệu thành $k$ cụm cầu nhưng đòi hỏi số cụm được xác định trước và không thể xử lý nhiễu.
\textbf{DBSCAN} \cite{ref_dbscan} xác định các cụm hình dạng tùy ý dựa trên mật độ và gán nhãn rõ ràng các điểm nhiễu, nhưng sử dụng ngưỡng mật độ toàn cục thất bại khi mật độ cụm thay đổi.
\textbf{HDBSCAN} \cite{ref_hdbscan} mở rộng DBSCAN với ước lượng mật độ phân cấp, tự động xác định các cụm có mật độ khác nhau trong khi cô lập nhiễu---làm cho nó đặc biệt phù hợp với dữ liệu mạng xã hội nơi các cụm sự kiện thể hiện mức độ tương tác đa dạng.

\textbf{Điểm mạnh:} Các phương pháp dựa trên embedding nắm bắt các mối quan hệ ngữ nghĩa và xử lý biến đổi từ vựng hiệu quả.
\textbf{Hạn chế:} Các embeddings chiều cao gây ra chi phí tính toán đáng kể, và suy luận thời gian thực vẫn còn là thách thức ở quy mô. Ngoài ra, các phương pháp này thường đòi hỏi điều chỉnh siêu tham số cẩn thận (ví dụ: ngưỡng cụm) cho từng miền.

\subsection{LLM và Tạo sinh Tăng cường Truy xuất}
Các Mô hình Ngôn ngữ Lớn gần đây đã chuyển đổi các nhiệm vụ NLP thông qua khả năng suy luận nổi bật \cite{ref_llm_reasoning}.
\textbf{GPT-4, Gemini và Claude} thể hiện hiệu suất zero-shot mạnh mẽ trên các nhiệm vụ phân loại, tóm tắt và trích xuất thông tin mà không cần tinh chỉnh riêng cho nhiệm vụ.
Tuy nhiên, LLMs gặp phải vấn đề ``ảo giác''---tạo nội dung có vẻ hợp lý nhưng không chính xác về thực tế---đặc biệt khi hoạt động mà không có ngữ cảnh nền tảng.

\textbf{Tạo sinh Tăng cường Truy xuất (RAG)} \cite{ref_rag} giải quyết hạn chế này bằng cách điều kiện hóa câu trả lời LLM trên các tài liệu liên quan được truy xuất.
Trong ngữ cảnh phát hiện sự kiện, RAG cho phép căn cứ tóm tắt LLM trên các điểm dữ liệu xã hội đã được xác minh, cải thiện độ chính xác thực tế.
Một số công trình gần đây đã khám phá trích xuất và phân loại sự kiện dựa trên LLM, mặc dù hầu hết tập trung vào xử lý theo lô ngoại tuyến thay vì luồng thời gian thực.

\textbf{Điểm mạnh:} LLMs cung cấp hiểu biết ngữ nghĩa vượt trội và có thể tạo các tóm tắt sự kiện có thể diễn giải với trích xuất 5W1H cấu trúc (Ai, Cái gì, Khi nào, Ở đâu, Tại sao, Như thế nào).
\textbf{Hạn chế:} Suy luận LLM tốn kém tính toán (thường chậm hơn 100-1000$\times$ so với phân loại dựa trên embedding), làm cho việc tích hợp trực tiếp vào các pipeline luồng độ trễ thấp không khả thi.

\subsection{Các Kiến trúc Luồng Thời gian Thực}
Phát hiện sự kiện thời gian thực đòi hỏi các kiến trúc chuyên biệt cân bằng thông lượng, độ trễ và độ chính xác.
\textbf{Sakaki và cộng sự} \cite{ref_twitter_earthquake} tiên phong trong phát hiện động đất thời gian thực từ Twitter sử dụng bộ lọc Kalman và bộ lọc hạt để theo dõi sự lan truyền sự kiện.
\textbf{TwitterNews+} \cite{ref_streaming_survey} đề xuất một framework kết hợp phân cụm tăng dần với nhận dạng thực thể được đặt tên cho phát hiện sự kiện tin tức từ luồng Twitter.

Các hệ thống luồng hiện đại thường sử dụng \textbf{Kiến trúc Lambda} (lớp batch + luồng) hoặc \textbf{Kiến trúc Kappa} đơn giản hơn (chỉ luồng), sử dụng các message broker như Apache Kafka cho thu nhập chống lỗi và các bộ xử lý luồng như Spark Streaming hoặc Flink cho tính toán thời gian thực.

\textbf{Điểm mạnh:} Các kiến trúc này đạt độ trễ dưới giây và khả năng mở rộng theo chiều ngang.
\textbf{Hạn chế:} Hầu hết các phương pháp luồng hy sinh chiều sâu ngữ nghĩa để đổi lấy tốc độ, sử dụng các đặc trưng nhẹ (từ khóa, hashtag) thay vì hiểu biết ngữ nghĩa đầy đủ.

\subsection{Khoảng trống Nghiên cứu và Đóng góp của Chúng tôi}
Như tóm tắt trong Bảng \ref{tab:related_work}, các phương pháp hiện có thường tối ưu hóa cho \emph{hoặc} phát hiện luồng độ trễ thấp \emph{hoặc} tính nhất quán ngữ nghĩa cao với suy luận LLM---nhưng các triển khai thực tế đòi hỏi \textbf{cả hai} đồng thời.
\textbf{Dual-Path Architecture} của chúng tôi giải quyết khoảng trống này bằng cách tách rời phân cụm nhanh (SAHC trên Fast Path) khỏi làm giàu ngữ nghĩa sâu (LLM trên Slow Path), đạt độ trễ thời gian thực trong khi duy trì khả năng diễn giải sự kiện.
Hơn nữa, chúng tôi giới thiệu giao thức \textbf{LLM-as-a-Judge} để giải quyết khoảng trống đánh giá khi nhãn ground-truth khan hiếm trong các kịch bản luồng.

\begin{table}
    \caption{So sánh các Phương pháp Phát hiện Sự kiện}\label{tab:related_work}
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{l c c c c c}
            \toprule
            \textbf{Approach}                        & \textbf{Semantic}      & \textbf{Real-time}  & \textbf{Noise}      & \textbf{Interpretable} & \textbf{Scalable}   \\
                                                     & \textbf{Understanding} & \textbf{Capable}    & \textbf{Handling}   & \textbf{Output}        &                     \\
            \midrule
            TF-IDF / BM25 \cite{ref_bm25}            & Low                    & \checkmark          & Limited             & --                     & \checkmark          \\
            LDA \cite{ref_lda}                       & Medium                 & --                  & Limited             & \checkmark             & \checkmark          \\
            EDCoW \cite{ref_edcow}                   & Low                    & \checkmark          & Limited             & --                     & \checkmark          \\
            BERT + Clustering \cite{ref_bert}        & High                   & --                  & Medium              & --                     & --                  \\
            BERTopic \cite{ref_bertopic}             & High                   & --                  & \checkmark          & \checkmark             & --                  \\
            TwitterNews+ \cite{ref_streaming_survey} & Medium                 & \checkmark          & Medium              & --                     & \checkmark          \\
            \midrule
            \textbf{Ours (SAHC + LLM)}               & \textbf{High}          & \textbf{\checkmark} & \textbf{\checkmark} & \textbf{\checkmark}    & \textbf{\checkmark} \\
            \bottomrule
        \end{tabular}%
    }
\end{table}

\section{Kiến thức Nền tảng}

\subsection{Các Mô hình Ngôn ngữ Lớn và Tạo sinh Tăng cường Truy xuất}
Các Mô hình Ngôn ngữ Lớn (LLM) như GPT-4 và Gemini đã cách mạng hóa Xử lý Ngôn ngữ Tự nhiên (NLP) bằng cách thể hiện khả năng suy luận phát sinh. Được xây dựng trên kiến trúc Transformer \cite{ref_bert}, các mô hình này sử dụng cơ chế self-attention để nắm bắt các phụ thuộc tầm xa trong dữ liệu văn bản.
\begin{equation}
    Attention(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}
Mặc dù có khả năng mạnh mẽ, LLMs mắc phải tình trạng ``ảo giác''---tạo ra các khẳng định có vẻ hợp lý nhưng thực tế không chính xác. Để giảm thiểu điều này trong các ứng dụng theo miền cụ thể, Tạo sinh Tăng cường Truy xuất (RAG) được sử dụng. RAG tăng cường quá trình tạo sinh bằng cách điều kiện hóa LLM trên các tài liệu liên quan được truy xuất $D = \{d_1, ..., d_k\}$ trước khi tạo phản hồi $y$:
\begin{equation}
    P(y|x) = \sum_{z \in D} P(z|x)P(y|x,z)
\end{equation}
Trong hệ thống của chúng tôi, chúng tôi điều chỉnh mô hình này bằng cách coi ``Các cụm'' là ngữ cảnh được truy xuất $z$, neo tóm tắt của LLM vào các điểm dữ liệu xã hội đã được xác minh.

\subsection{Các Kiến trúc Luồng Thời gian Thực}
Phát hiện sự kiện hiện đại đòi hỏi xử lý dữ liệu với tốc độ và khối lượng cao. \textbf{Kiến trúc Kappa} đơn giản hóa Kiến trúc Lambda truyền thống bằng cách coi mọi thứ như một luồng, sử dụng message broker dựa trên log mạnh mẽ như Apache Kafka.
Kafka phân vùng dữ liệu qua các broker phân tán, đảm bảo khả năng chịu lỗi và thông lượng cao. Consumers đọc dữ liệu từ các phân vùng này sử dụng offsets $O$, duy trì đảm bảo xử lý exactly-once:
\begin{equation}
    \text{Lag}(t) = \text{Offset}_{produce}(t) - \text{Offset}_{consume}(t)
\end{equation}
Để xử lý, \textbf{Spark Structured Streaming} coi các luồng dữ liệu trực tiếp như một bảng đầu vào không giới hạn. Nó xử lý dữ liệu văn bản theo micro-batches (ví dụ: khoảng 200ms), cho phép áp dụng các thao tác giống batch (như aggregations và joins) trên dữ liệu luồng với độ trễ tối thiểu.

\subsection{Phân cụm Dựa trên Mật độ (HDBSCAN)}
Khác với các phương pháp dựa trên centroid (K-Means) giả định các cụm hình cầu, Phân cụm Phân cấp Dựa trên Mật độ với Nhiễu (HDBSCAN) \cite{ref_hdbscan} xác định các cụm có mật độ và hình dạng khác nhau.
HDBSCAN dựa vào một metric khoảng cách được biến đổi để phân tách nhiễu thưa khỏi các cụm dày đặc. \textbf{Core Distance} của điểm $p$, ký hiệu là $d_{core}(p)$, được định nghĩa là khoảng cách đến lân cận thứ $k$. Để đảm bảo các cụm tiềm năng mạnh mẽ, \textbf{Mutual Reachability Distance} giữa các điểm $p$ và $q$ được hình thức hóa như:
\begin{equation}
    d_{mreach}(p, q) = \max \{ d_{core}(p), d_{core}(q), d(p, q) \}
\end{equation}
Metric này hiệu quả ``đẩy'' các điểm thưa ra xa nhau. Một Cây Bao trùm Tối thiểu (MST) sau đó được xây dựng sử dụng $d_{mreach}$ làm trọng số cạnh. Bằng cách lặp đi lặp lại loại bỏ các cạnh có trọng số cao nhất, HDBSCAN xây dựng một phân cấp các thành phần liên thông, trích xuất các cụm ổn định tồn tại qua một phạm vi rộng các ngưỡng mật độ. Thuộc tính này rất quan trọng cho dữ liệu mạng xã hội, nơi các cụm sự kiện (``Xu hướng Viral'') thường có mật độ cao hơn nhiều so với nhiễu nền (``Trò chuyện Hàng ngày'').

\subsection{Phân loại Sự kiện và Hệ thống Taxonomy}
Để đảm bảo các insight có thể hành động, chúng tôi vượt ra ngoài sentiment nhị phân đến một taxonomy dựa trên sử dụng (T1-T7), phân loại các sự kiện theo giá trị của chúng đối với các bên liên quan cụ thể:

\begin{itemize}
    \item \textbf{T1. Khủng hoảng \& Rủi ro Công cộng} (Đối tượng: Dịch vụ Khẩn cấp)\\
          \emph{Câu hỏi Cốt lõi: Có cần can thiệp ngay lập tức không?} Bao gồm tai nạn, thảm họa, và bạo loạn.

    \item \textbf{T2. Tín hiệu Chính sách} (Đối tượng: Chính phủ)\\
          \emph{Câu hỏi Cốt lõi: Công chúng phản ứng thế nào với các chính sách mới?} Bao gồm luật mới và tuyên bố chính thức.

    \item \textbf{T3. Rủi ro Danh tiếng} (Đối tượng: Cơ quan PR)\\
          \emph{Câu hỏi Cốt lõi: Niềm tin công chúng có bị tổn hại không?} Bao gồm scandal, tẩy chay, và tranh cãi.

    \item \textbf{T4. Cơ hội Thị trường} (Đối tượng: Đội ngũ Marketing)\\
          \emph{Câu hỏi Cốt lõi: Có nhu cầu có thể kiếm tiền không?} Bao gồm sản phẩm viral và xu hướng lối sống mới nổi.

    \item \textbf{T5. Xu hướng Văn hóa} (Đối tượng: Nhà sáng tạo Nội dung)\\
          \emph{Câu hỏi Cốt lõi: Xu hướng này có đáng theo để thu hút sự chú ý không?} Bao gồm meme và sự kiện giải trí.

    \item \textbf{T6. Điểm Đau Vận hành} (Đối tượng: Nhà Vận hành Dịch vụ)\\
          \emph{Câu hỏi Cốt lõi: Mọi người đang phàn nàn về điều gì?} Bao gồm giao thông, sự cố, và lỗi dịch vụ.

    \item \textbf{T7. Thường ngày/Nhiễu} (Đối tượng: Bộ lọc Hệ thống)\\
          \emph{Câu hỏi Cốt lõi: Có nên lọc điều này không?} Bao gồm thời tiết hàng ngày, thể thao thường xuyên, và kết quả xổ số.
\end{itemize}

\section{Kiến trúc Hệ thống Đề xuất}
Hệ thống tuân theo \textbf{kiến trúc luồng kiểu Kappa} được tách thành hai đường xử lý để giải quyết sự đánh đổi "Độ trễ vs. Độ chính xác".

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/full_pipeline.png}
    \caption{Kiến trúc Hệ thống Đề xuất: Xử lý Hai Đường (Fast Path vs. Slow Path).} \label{fig:pipeline}
\end{figure}

Hình \ref{fig:pipeline} minh họa luồng dữ liệu toàn trình. Dữ liệu thô được thu nhập qua Kafka, xử lý theo micro-batch bởi Spark (Fast Path), và làm giàu bất đồng bộ bởi Gemini Pro (Slow Path) trước khi được phục vụ đến UI. Sự tách biệt này đảm bảo rằng thời gian suy luận nặng của LLMs không chặn pipeline thu nhập thông lượng cao.

\subsection{Quy trình Hệ thống}
Vòng đời của một bài đăng mạng xã hội trong hệ thống của chúng tôi tuân theo năm giai đoạn:
\begin{enumerate}
    \item \textbf{Lớp Thu nhập:} Python crawlers, được điều phối bởi \textbf{Airflow}, liên tục thu thập văn bản thô từ hơn 40 Fanpages có lưu lượng cao. Dữ liệu được chuẩn hóa sang định dạng JSON và đẩy đến topic \textbf{Apache Kafka} \texttt{posts\_stream\_v1}, với \textbf{Zookeeper} xử lý điều phối cụm.
    \item \textbf{Lọc (Spark Streaming):} Engine \textbf{Spark} tiêu thụ luồng Kafka theo micro-batches. Heuristic Guard loại bỏ nhiễu thường ngày, trong khi \textbf{Smart Query Constructor} mở rộng các từ khóa xu hướng tiềm năng (ví dụ: ``Sea Games'' $\to$ ``Lịch Sea Games 33'') để tăng recall.
    \item \textbf{Phân cụm (Fast Path):} Các bài đăng xã hội hợp lệ được vector hóa sử dụng mô hình ONNX và \textbf{được khớp với các vector Neo} (Tin tức/Xu hướng) sử dụng \textbf{điểm lai tối ưu độ trễ} (cosine similarity với tăng cường từ khóa heuristic).
          Chúng tôi hỗ trợ module reranking Cross-Encoder ở giai đoạn khớp; tuy nhiên, để thỏa mãn các ràng buộc thời gian thực, \textbf{triển khai online sử dụng chiến lược xác minh nhẹ} chỉ tính lại điểm ứng viên chiến thắng (Top-1) như một \textbf{cổng xác minh}.
          Trong phân tích offline hoặc các chế độ có thể cấu hình, hệ thống có thể được mở rộng để rerank Top-$K$ ứng viên cho độ chính xác cao hơn.
          Các bài đăng bị cổng xác minh từ chối hoặc không đạt ngưỡng được đệm là \textit{dư} cho bước \textbf{Khám phá} (HDBSCAN).

    \item \textbf{Trí tuệ (Slow Path):} \textbf{Python Workers} bất đồng bộ poll các cụm đã xác nhận từ kho lưu trữ thống nhất \textbf{PostgreSQL}. Một cụm được coi là \emph{quan trọng} và đủ điều kiện cho làm giàu LLM chỉ khi đạt ngưỡng tác động về \textbf{người dùng riêng biệt} ($U(C)\ge \delta_{significant}$). Workers sau đó truy vấn \textbf{Gemini Pro} để trích xuất tóm tắt 5W1H có cấu trúc và khử trùng lặp các chủ đề chồng chéo (xem Phụ lục Bảng \ref{tab:prompt_refinement}).
    \item \textbf{Trực quan hóa:} Dashboard poll cơ sở dữ liệu PostgreSQL để hiển thị các sự kiện đã xác minh, đã làm giàu theo thời gian thực.
\end{enumerate}

\noindent
\begin{center}
    \setlength{\fboxsep}{10pt}
    \fcolorbox{black}{gray!10}{%
        \begin{minipage}{0.95\textwidth}
            \textbf{\large Theo dõi Xử lý Đầu-Cuối: ``Cảnh báo Bão Yagi''}
            \vspace{0.2cm}
            \par
            Để minh họa pipeline, xem xét vòng đời của một bài đăng xã hội:
            \begin{enumerate}
                \item \textbf{Đầu vào:} Crawler lấy văn bản thô ``Bão số 3 giật cấp 12...'' từ fanpage \textit{ThongTinChinhPhu}.
                \item \textbf{Thu nhập:} Kafka producer serialize thành JSON và đẩy đến topic \texttt{posts\_stream\_v1} (Partition 0).
                \item \textbf{Lọc:} Spark Streaming tiêu thụ tin nhắn. Heuristic Guard cho phép, và Smart Query mở rộng ngữ cảnh thành ``Theo dõi Bão Yagi''.
                \item \textbf{Phân cụm:} ONNX Worker ánh xạ thành vector $\mathbf{v}$. SAHC xác định News Anchor khớp tốt nhất ``Bão Yagi'' (Hybrid Score $> \lambda_{match}$) và gắn bài đăng.
                \item \textbf{Trí tuệ:} Cụm phát triển vượt ngưỡng quan trọng. Async worker kích hoạt Gemini Pro, trả về tóm tắt có cấu trúc: ``Bão Yagi tiến gần Quảng Ninh.''
                \item \textbf{Đầu ra:} Dashboard nhận cập nhật cụ thể và hiển thị cảnh báo ``Rủi ro Công cộng'' trên bản đồ trực tiếp.
            \end{enumerate}
        \end{minipage}%
    }
\end{center}

\subsection{Triển khai Fast Path vs. Slow Path}
\begin{itemize}
    \item \textbf{Thu nhập (Kafka \& Zookeeper):}
          \begin{itemize}
              \item \textbf{Producer:} Serialize các bài đăng thô thành JSON và xuất bản đến topic \texttt{posts\_stream\_v1}, xử lý retries và exponential backoff để đảm bảo giao hàng at-least-once.
              \item \textbf{Broker:} Kafka hoạt động như lưu trữ log phân tán, chịu lỗi, trong khi \textbf{ZooKeeper} quản lý metadata broker, bầu cử leader, và phân công partition.
          \end{itemize}

    \item \textbf{Xử lý (Spark Structured Streaming):}
          \begin{itemize}
              \item Tiêu thụ các sự kiện từ Kafka với \textbf{ngữ nghĩa exactly-once} sử dụng checkpointing.
              \item Phân tích cú pháp JSON thô thành Spark DataFrames cho xử lý cột hiệu quả.
          \end{itemize}

    \item \textbf{Suy luận (Pandas UDF \& ONNX):}
          \begin{itemize}
              \item Sử dụng giao diện \textbf{Pandas UDF} của PySpark để vector hóa văn bản theo batches, tránh overhead serialization Python theo từng dòng.
              \item Kích hoạt engine \textbf{ONNX Runtime} trên các Worker nodes để tính toán embeddings (SBERT) cho mỗi micro-batch; \textbf{HDBSCAN} chỉ được kích hoạt trên tập \textit{không khớp/dư} để khám phá các sự kiện \textbf{Chỉ-Xã-hội} mới nổi.
          \end{itemize}

    \item \textbf{Trí tuệ Slow Path (Async Workers):}
          \begin{itemize}
              \item Được thiết kế để tách rời các thao tác nặng khỏi luồng thu nhập. Workers poll các cụm đã xác nhận và:
                    \begin{enumerate}
                        \item Truy vấn \textbf{Google Trends API} để lấy khối lượng tìm kiếm (G-score), sử dụng prompt lọc trong Phụ lục Bảng \ref{tab:prompt_trends}. Tín hiệu này được coi là \textbf{xác nhận/trễ}: nó thường tích lũy sau khi một sự kiện đã hiển thị trên các luồng xã hội, và do đó được sử dụng để xác thực và ưu tiên các cụm thay vì kích hoạt phát hiện ban đầu.
                        \item Sử dụng chiến lược \textbf{Prompting Nhận thức Ngữ cảnh} với Gemini Pro để tổng hợp các tóm tắt mạch lạc.
                    \end{enumerate}
          \end{itemize}
\end{itemize}

\subsection{Định nghĩa Độ trễ}
Chúng tôi phân biệt giữa hai loại độ trễ trong hệ thống của chúng tôi:
\begin{itemize}
    \item \textbf{Độ trễ Xử lý Fast-Path:} đo từ thu nhập Kafka đến gán cụm trong một Spark micro-batch.
    \item \textbf{Độ trễ Cảnh báo Đầu-Cuối:} đo từ thu nhập dữ liệu đến cập nhật dashboard.
\end{itemize}

Trong thiết lập thử nghiệm của chúng tôi, Fast Path đạt độ trễ xử lý trung bình \textbf{2.4 giây}, trong khi độ trễ cảnh báo đầu-cuối vẫn \textbf{dưới 10 giây}, vì làm giàu LLM Slow Path được thực thi bất đồng bộ và không chặn phát hiện sự kiện thời gian thực.

\section{Phương pháp}

\subsection{Định nghĩa Bài toán Hình thức}
Cho $\mathcal{S} = \{p_1, p_2, ..., p_n\}$ là một luồng liên tục các bài đăng mạng xã hội, nơi mỗi bài đăng $p_t$ tại thời điểm $t$ là một tuple $(u_t, \tau_t, \textbf{v}_t)$, đại diện cho người dùng, dấu thời gian, và vector embedding ngữ nghĩa tương ứng.
Mục tiêu của phát hiện sự kiện thời gian thực là ánh xạ luồng này thành một tập các cụm rời rạc $\mathcal{C} = \{C_1, C_2, ..., C_k\}$, nơi mỗi cụm $C_i$ đại diện cho một sự kiện thực tế.
Một cụm sự kiện hợp lệ $C_i$ phải thỏa mãn hai điều kiện:
\begin{enumerate}
    \item \textbf{Kết dính:} $\forall p_a, p_b \in C_i, \text{sim}(\textbf{v}_a, \textbf{v}_b) \geq \theta_{sim}$
    \item \textbf{Tác động:} $U(C_i) \geq \delta_{min}$, nơi $U(C_i)$ là số \emph{người dùng riêng biệt} thảo luận sự kiện:
          \begin{equation}
              U(C_i) = \left|\left\{u_t \;\middle|\; p_t \in C_i\right\}\right|.
          \end{equation}

\end{enumerate}

Trong thực tế, chúng tôi sử dụng $\delta_{significant}$ như ngưỡng tác động vận hành trong hệ thống luồng (thường $\delta_{significant}\ge \delta_{min}$) để quyết định khi nào một cụm được đề bạt cho làm giàu và trực quan hóa.

\subsection{Tiền xử lý và Chuẩn hóa Dữ liệu}

Để đảm bảo đầu vào chất lượng cao cho engine phân cụm, dữ liệu thô từ cả hai đường trải qua các bước tiền xử lý cụ thể:

\begin{itemize}
    \item \textbf{Fast Path (Tốc độ Xã hội):} Với bản chất thông lượng cao của luồng xã hội, chúng tôi áp dụng làm sạch tối thiểu, tối ưu độ trễ:
          \begin{itemize}
              \item \textbf{Lọc Heuristic:} Chúng tôi loại bỏ các bài đăng có độ dài $<20$ ký tự (ngữ cảnh ngữ nghĩa không đủ cho embedding) hoặc $>800$ ký tự (khả năng cao là spam tiếp thị hoặc boilerplate không liên quan), và xóa các mẫu ``credit'' đã biết (ví dụ: ``cre:'', ``via:'') không đóng góp vào nhận dạng sự kiện.
              \item \textbf{Chuẩn hóa Unicode:} Chuyển đổi tất cả văn bản sang định dạng NFC và chuyển chữ thường để xử lý dấu tiếng Việt nhất quán.
          \end{itemize}

    \item \textbf{Slow Path (Độ chính xác Tin tức):} Dữ liệu tin tức, được sử dụng làm neo độ tin cậy cao, trải qua phân tích cấu trúc sâu:
          \begin{itemize}
              \item \textbf{Trích xuất Cấu trúc:} Chúng tôi sử dụng Regex để tách metadata (Tác giả, Ngày, Địa điểm) khỏi nội dung, đảm bảo chỉ câu chuyện cốt lõi ảnh hưởng đến centroid.
              \item \textbf{Chèn Bí danh (Thesaurus):} Để nối liền khoảng cách từ vựng giữa tin tức chính thức và bài đăng xã hội không chính thức, chúng tôi chèn các bí danh đã biết vào các vector tin tức (ví dụ: ánh xạ ``Bão số 3'' $\to$ ``Bão Yagi'').
          \end{itemize}
\end{itemize}

\subsection{Phân cụm Phân cấp Nhận thức Xã hội (SAHC)}

Phân cụm tiêu chuẩn (K-Means, DBSCAN) thường thất bại trên dữ liệu xã hội do nhiễu. SAHC hoạt động theo ba giai đoạn:
\begin{enumerate}
    \item \textbf{Giai đoạn 1: Neo Tin tức.} Chúng tôi trước tiên phân cụm các bài báo Tin tức đã xác minh để tạo thành ``Anchor Centroids''. Chúng đại diện cho các sự kiện độ tin cậy cao.
    \item \textbf{Giai đoạn 2: Gắn Xã hội.} Các bài đăng xã hội đến được ánh xạ đến các neo này sử dụng Cosine Similarity.
    \item \textbf{Giai đoạn 3: Khám phá Xã hội.} Các bài đăng không khớp với bất kỳ neo nào trải qua lượt phân cụm dựa trên mật độ thứ hai (HDBSCAN) để phát hiện các xu hướng ``Chỉ-Xã-hội'' (không được tin tức đưa).
\end{enumerate}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/clustering_pipeline.png}
    \caption{Quy trình Phân cụm Phân cấp Nhận thức Xã hội (SAHC).} \label{fig:clustering}
\end{figure}

Quy trình SAHC được trực quan hóa trong Hình \ref{fig:clustering}. Các bài báo tin tức trước tiên thiết lập các centroid ``Neo'' (Nút Xanh dương). Các bài đăng Xã hội đến (Nút Xanh lá) sau đó được hút đến các neo này dựa trên độ tương đồng ngữ nghĩa. Nhiễu dư cuối cùng được lọc hoặc phân cụm thành các sự kiện mới bởi HDBSCAN, giảm hiệu quả spam.

\begin{algorithm}
    \caption{SAHC-A: Anchor-First Dual-Path Event Detection}
    \begin{algorithmic}[1]
        \Require Incoming micro-batch $B$, Active Anchors $\mathcal{A}$, Global Residual Buffer $\mathcal{G}$
        \Ensure Updated Events $\mathcal{E}$, Updated Buffer $\mathcal{G}$

        \State $\mathcal{R}_{batch} \leftarrow \emptyset$ \Comment{Unmatched posts in this batch}

        \State \textbf{// Path 1: Anchor Matching (Fast Path)}
        \For{each post $p \in B$}
        \State $\mathbf{v}_p \leftarrow \text{Encoder}(p_{text})$
        \State $a^* \leftarrow \arg\max_{a \in \mathcal{A}} \cos(\mathbf{v}_p,\mathbf{v}_a)$ \Comment{Fast retrieval by cosine}
        \State $vector\_sim \leftarrow \cos(\mathbf{v}_p,\mathbf{v}_{a^*})$
        \State $keyword\_score \leftarrow \text{KW}(p,a^*)$ \Comment{Keyword overlap in $[0,1]$}
        \State $score \leftarrow 0.7 \cdot vector\_sim + 0.3 \cdot keyword\_score$
        \If{$keyword\_score > 0.5$}
        \State $score \leftarrow score + 0.1$ \Comment{Conditional boost}
        \EndIf

        \If{$score \ge \lambda_{match}$}
        \State Add $p$ to Event $E_{a^*}$
        \State Update Centroid: $\mathbf{v}_{a^*} \leftarrow (1-\alpha)\mathbf{v}_{a^*} + \alpha\mathbf{v}_p$ \Comment{Moving Average}
        \Else
        \State $\mathcal{R}_{batch} \leftarrow \mathcal{R}_{batch} \cup \{p\}$
        \EndIf
        \EndFor

        \State \textbf{// Path 2: Social Discovery (Slow Path)}
        \State $\mathcal{G} \leftarrow \mathcal{G} \cup \mathcal{R}_{batch}$ \Comment{Merge with global buffer}
        \If{$|\mathcal{G}| \ge N_{min\_clustering}$}
        \State $\mathcal{C}_{new} \leftarrow \text{HDBSCAN}(\mathcal{G}, \min_{pts}=5, \epsilon=0.3)$
        \For{each cluster $c \in \mathcal{C}_{new}$}
        \State $U(c) \leftarrow \left|\{u(p)\;|\; p \in c\}\right|$ \Comment{Distinct user count}
        \If{$U(c) \ge \delta_{significant}$}
        \State Initialize new Event $E_{new}$ from $c$
        \State Remove points in $c$ from $\mathcal{G}$
        \EndIf
        \EndFor

        \State Prune old points from $\mathcal{G}$ \Comment{Time-decay cleanup}
        \EndIf

    \end{algorithmic}
\end{algorithm}

\textbf{Khớp Hybrid Tối ưu Độ trễ:}
Để cân bằng hiểu biết ngữ nghĩa và khớp thực thể chính xác dưới các ràng buộc thời gian thực, chúng tôi sử dụng cơ chế tính điểm có trọng số với tăng cường có điều kiện.
Ngoài ra, chúng tôi tích hợp module Cross-Encoder tùy chọn: trong cấu hình online hiện tại, nó hoạt động như \textbf{cổng xác minh/khôi phục} bằng cách chỉ tính lại điểm ứng viên hàng đầu khi cần, thay vì thực hiện reranking Top-$K$ đầy đủ. Điều này duy trì độ trễ thấp trong khi giảm gắn sai.
\begin{equation}
    \label{eq:hybrid_real}
    \text{Score}(p,a) = w_v \cdot \cos(\mathbf{v}_p,\mathbf{v}_a) + w_k \cdot \text{KW}(p,a) + \delta_{boost} \cdot \mathbb{I}(\text{KW}(p,a) > \tau),
\end{equation}
nơi $w_v=0,7$ và $w_k=0,3$ là các trọng số thực nghiệm, $\text{KW}(p,a)\in[0,1]$ là tỷ lệ trùng lặp từ khóa, và tăng cường cố định $\delta_{boost}=0,1$ được áp dụng khi trùng lặp vượt $\tau=0,5$. Thiết kế này vẫn tương thích với các ràng buộc thời gian thực trong khi giảm gắn sai giữa các sự kiện khác biệt về từ vựng.

\subsection{Logic Tính Điểm Xu hướng}
Để xác định sự kiện nào đang ``Trending'', chúng tôi tính Điểm Xu hướng Thống nhất ($T$) cho mỗi cụm dựa trên ba tín hiệu có trọng số:
\begin{equation}
    T = 0.4 \cdot G + 0.35 \cdot N + 0.25 \cdot F
\end{equation}
Trong thiết lập của chúng tôi, luồng xã hội cung cấp các tín hiệu bùng nổ sớm nhất, trong khi Google Trends thường đạt đỉnh sau. Do đó, $G$ được sử dụng chủ yếu để xác nhận và ưu tiên các cụm ứng viên đã được phát hiện trên Fast Path.

Nơi các thành phần đại diện cho:
\begin{itemize}
    \item \textbf{G (Khối lượng Tìm kiếm Google):} Điểm quan tâm tìm kiếm chuẩn hóa từ Google Trends, phục vụ như \textbf{tín hiệu xác nhận (trễ)} của sự chú ý công chúng.
    \item \textbf{N (Khối lượng Tin tức):} Số lượng cụm tin tức chính thống riêng biệt liên kết với sự kiện này.
    \item \textbf{F (Tương tác Facebook):} Tổng có trọng số của tương tác ($Likes + 2 \cdot Comments + 3 \cdot Shares$).
\end{itemize}
Để xử lý phân phối power-law của dữ liệu xã hội, mỗi thành phần được chuẩn hóa sử dụng thang \textbf{Log-Min-Max} trước khi cho trọng số:
\begin{equation}
    S_{component} = 100 \cdot \frac{\log_{10}(1 + v)}{\log_{10}(1 + v_{max})}
\end{equation}
Nơi $v$ là giá trị thô và $v_{max}$ là trần thực nghiệm ($G_{max}=10^6$, $N_{max}=10$, $F_{max}=2\cdot10^4$). Cụ thể cho Facebook ($F$), các tương tác được cho trọng số: $v_F = Likes + 2 \cdot Comments + 3 \cdot Shares$. Điều này đảm bảo rằng các sự kiện tương tác cao được phân loại đúng ngay cả với khối lượng thấp. Một cụm chỉ được đưa lên dashboard khi $T > Ngưỡng$.

\section{Dữ liệu và Thiết lập Thực nghiệm}

\subsection{Nguồn Dữ liệu}
Hệ thống của chúng tôi giám sát hai luồng dữ liệu chính, được thống nhất vào một schema chung để xử lý:
\begin{itemize}
    \item \textbf{Mạng Xã hội (Facebook):} Thu thập từ các Fanpage có tương tác cao (ví dụ: Theanh28, ThongTinChinhPhu). Các trường dữ liệu bao gồm: \texttt{pageName}, \texttt{postId}, \texttt{time} (ISO-8601), nội dung \texttt{text}, và các chỉ số tương tác (\texttt{likes}, \texttt{comments}, \texttt{shares}).
    \item \textbf{Tin tức Chính thống:} Các bài báo được crawl từ các nguồn đã xác minh (VnExpress, Tuổi Trẻ). Các trường chính: \texttt{ArticleID}, \texttt{URL}, \texttt{Title}, \texttt{Content}, và \texttt{PublishTime}.
    \item \textbf{Google Trends:} Các từ khóa tìm kiếm thời gian thực hoạt động như \textbf{tín hiệu xác nhận/trễ} để xác thực và ưu tiên các cụm ứng viên được phát hiện từ luồng xã hội.
\end{itemize}

Bộ dữ liệu thử nghiệm bao gồm \textbf{7.605 mục duy nhất} được thu thập từ 8 đến 22 tháng 12, 2025, bao gồm \textbf{4.644 bài báo tin tức được crawl} và \textbf{2.961 bài đăng mạng xã hội}.
Sau khi trích xuất nội dung, \textbf{4.603 bài báo tin tức} chứa văn bản body hợp lệ và được sử dụng cho neo và embedding. Ở đây, \textbf{2.961} biểu thị \emph{các bài đăng xã hội duy nhất được sử dụng trong thử nghiệm} sau khi khử trùng lặp và kiểm tra hợp lệ cơ bản (khối lượng crawl thô được báo cáo như một xấp xỉ).

\subsection{Phân tích Dữ liệu Khám phá}
Bảng \ref{tab:data_dist} trình bày phân phối bộ dữ liệu thử nghiệm của chúng tôi. Chúng tôi quan sát thấy một ``Khoảng cách Sẵn có'' đáng kể: trong khi chúng tôi crawl hơn 9.400 URL tin tức, chỉ \textbf{4.603 bài báo (48,6\%)} chứa văn bản body có thể trích xuất, trong khi \textbf{bài đăng mạng xã hội (2.961 mục)} đạt \emph{gần như hoàn toàn} về tính sẵn có.
Hơn nữa, có sự phân kỳ cấu trúc: Bài báo tin tức trung bình $>500$ từ với ngữ pháp chính thức, trong khi bài đăng xã hội trung bình $<50$ từ với tiếng lóng thường xuyên. Sự phân đôi này đòi hỏi thiết kế ``Hai Đường'' của chúng tôi, nơi \textbf{Fast Path} xử lý luồng xã hội văn bản ngắn, tốc độ cao, và \textbf{Slow Path} tận dụng LLMs để tổng hợp ngữ cảnh sâu hơn tìm thấy trong các neo tin tức.

\begin{table}
    \caption{Phân phối Dataset và Tính Sẵn có}\label{tab:data_dist}
    \centering
    \begin{tabular}{l c c c}
        \toprule
        \textbf{Loại Nguồn} & \textbf{URL Đã Crawl} & \textbf{Nội dung Hợp lệ} & \textbf{Độ dài TB (từ)} \\
        \midrule
        Tin tức Chính thống & $\approx 9.400$       & 4.603 (48,6\%)           & $> 500$                 \\
        Mạng Xã hội         & $\approx 3.000$       & 2.961 ($\approx 98,7\%$) & $< 50$                  \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Triển khai và Môi trường Vận hành}
Để đảm bảo tính tái lập và khả năng mở rộng, hệ thống được container hóa sử dụng \textbf{Docker Compose} với các dịch vụ cốt lõi sau:
\begin{itemize}
    \item \textbf{Ngăn xếp Phần mềm:} Apache Kafka 7.5.0, Apache Spark 3.5.0, PostgreSQL 15, MongoDB 6.0.
    \item \textbf{Phần cứng:} Tối ưu cho triển khai nút đơn (8GB RAM, 4 vCPUs) hoặc cụm Kubernetes.
    \item \textbf{Suy luận:} Tăng tốc ONNX Runtime trên GPU NVIDIA T4.
\end{itemize}

\subsubsection{Lợi ích Triển khai}
Kiến trúc microservices container hóa cung cấp ba lợi thế chính:
\begin{enumerate}
    \item \textbf{Khả năng Mở rộng:} Việc tách rời Thu nhập (Kafka) và Xử lý (Spark/LLM) cho phép mở rộng độc lập. Trong các sự kiện lưu lượng cao (ví dụ: bão), chúng tôi có thể thêm ngang các Spark Workers mà không cần cấu hình lại lớp thu nhập.
    \item \textbf{Hiệu quả Chi phí:} Bằng cách chuyển phân cụm batch nặng sang phần cứng hàng hóa cục bộ (CPU) và chỉ gọi API LLM trả phí cho các cụm đã xác minh (0,01\% khối lượng), chúng tôi giảm chi phí vận hành khoảng 400 lần so với các phương pháp dựa trên LLM thuần túy.
    \item \textbf{Tính Tái lập:} Toàn bộ ngăn xếp được định nghĩa trong \texttt{docker-compose.yml}, đảm bảo rằng môi trường ``Demo'' giống hệt ``Sản xuất'', loại bỏ vấn đề ``chạy được trên máy tôi''.
\end{enumerate}

\subsection{Dataset Construction Strategy}
To train our specialized models without incurring high labeling costs, we employed a \textbf{Hybrid Labeling Strategy}:
\begin{enumerate}
    \item \textbf{Học Tự giám sát:} Đối với Cross-Encoder Reranker, chúng tôi khai thác ``Hard Negatives'' (từ khóa tương tự nhưng ngữ nghĩa khác) để tạo 877 cặp chất lượng cao.
          Reranker này được sử dụng như thành phần khớp bổ trợ (xác minh / khôi phục) và như module benchmark offline.
    \item \textbf{Học Tích cực với LLM:} Đối với phân loại Sentiment và Taxonomy, chúng tôi sử dụng Gemini Pro để gán nhãn giả khoảng 10k mẫu, sau đó xem xét thủ công các điểm độ tin cậy thấp. Điều này mang lại \textbf{4.630 mẫu cho Sentiment} và \textbf{3.687 mẫu cho Taxonomy} huấn luyện.
\end{enumerate}

\begin{figure}[h]
    \centering
    \begin{verbatim}
{"text": "Vụ xả súng tại bãi biển Bondi khiến 15 người thiệt mạng.", 
 "label_sentiment": "Neutral", "label_taxonomy": "T1 - Crisis & Public Risk"}
{"text": ["Bão số 15 Koto đang ở đâu?", "Áp thấp nhiệt đới mạnh lên thành bão."], 
 "label_relevance": 1.0}
\end{verbatim}
    \caption{Ví dụ Dữ liệu Huấn luyện JSONL cho Phân loại và Reranking.} \label{fig:jsonl}
\end{figure}

\subsection{Lựa chọn Mô hình và Tinh chỉnh}
Chúng tôi tinh chỉnh các mô hình cụ thể cho tiếng Việt để phục vụ làm xương sống của hệ thống. Để đảm bảo tính mạnh mẽ, chúng tôi xây dựng một \textbf{Bộ Kiểm tra Stress} chứa 20 mẫu khó mỗi loại, đặc biệt nhắm vào các trường hợp biên như ``Tiếng lóng'' (ngôn ngữ mạng xã hội), ``Từ đồng nghĩa Bão'' (đặt tên sự kiện mơ hồ), và ``Chồng chéo Miền''. Bảng \ref{tab:training_config} chi tiết các siêu tham số huấn luyện cụ thể, và Bảng \ref{tab:models} hiển thị các metric hiệu suất cuối cùng trên bộ kiểm tra nghiêm ngặt này.

\begin{table}
    \caption{Cấu hình Huấn luyện Chi tiết và Siêu tham số}\label{tab:training_config}
    \centering
    \begin{tabular}{l l l l}
        \toprule
        \textbf{Mục tiêu Mô hình} & \textbf{Kiến trúc Cơ sở} & \textbf{Kích thước Dataset} & \textbf{Cấu hình Huấn luyện}    \\
        \midrule
        Sentiment                 & \texttt{uitnlp/visobert} & 4.630 (3 lớp)               & Epochs: 20, Batch: 32, LR: 2e-5 \\
        Taxonomy                  & \texttt{uitnlp/visobert} & 3.687 (7 lớp)               & Epochs: 20, Batch: 16, LR: 2e-5 \\
        Reranker                  & \texttt{ms-marco-MiniLM} & 877 cặp (Gold)              & Contractive Loss, Epochs: 20    \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/embedding_benchmark.png}
    \caption{Kết quả benchmark so sánh các Mô hình Embedding trên các trường hợp biên khác nhau.} \label{fig:benchmark}
\end{figure}



Hình \ref{fig:benchmark} so sánh độ ổn định (khoảng cách giữa cặp dương/âm) của 5 mô hình tiên tiến nhất. Chúng tôi định nghĩa Khoảng cách Ổn định theo toán học như:
\begin{equation}
    Gap = Sim(q, pos) - Sim(q, neg)
\end{equation}
\texttt{vietnamese-document-embedding} (thanh xanh) duy trì biên độ lớn nhất nhất quán trên các trường hợp biên như ``Từ đồng nghĩa Bão'' và ``Tiếng lóng'', xác nhận việc lựa chọn nó làm mô hình xương sống của chúng tôi.

Kết quả cho thấy \texttt{dangvantuan/vietnamese-document-embedding} đạt độ ổn định tốt nhất (Gap Trung bình 0,265). Bảng \ref{tab:embedding_benchmark} chi tiết hiệu suất trên các thách thức ngôn ngữ cụ thể, được định nghĩa như sau:

\begin{itemize}
    \item \textbf{Gap Trung bình:} Biên độ ổn định trung bình trên tất cả các trường hợp kiểm tra.
    \item \textbf{Bão:} Khả năng nhóm các tên bão đa dạng (ví dụ: ``Bão Yagi'', ``Bão số 3'').
    \item \textbf{Miền:} Truy xuất chéo miền (ví dụ: khớp truy vấn ``Y tế'' với bài đăng ``Bệnh viện'').
    \item \textbf{Danh mục:} Phân biệt giữa các loại sự kiện (ví dụ: ``Lũ lụt'' vs. ``Kẹt xe'').
    \item \textbf{Tiếng lóng:} Xử lý teen-code và tiếng lóng internet Việt Nam (ví dụ: ``bão'' vs ``b4o'').
    \item \textbf{Viết tắt:} Giải quyết các viết tắt phổ biến (ví dụ: ``HCM'' $\rightarrow$ ``Thành phố Hồ Chí Minh'').
\end{itemize}

\textbf{Phân tích:} Trong khi \texttt{vn-sbert} xuất sắc ở các nhiệm vụ nặng từ khóa như nhóm ``Bão'' (Gap: 0,355), nó thất bại hoàn toàn ở ``Tiếng lóng'' (-0,005), chỉ ra rằng nó coi tiếng lóng là nhiễu. Ngược lại, \texttt{vn-doc-embedding} duy trì hiệu suất mạnh mẽ trên ``Tiếng lóng'' (0,283) và ``Viết tắt'' (0,419), làm cho nó trở thành lựa chọn ưu việt để xử lý văn bản mạng xã hội thô nơi ngôn ngữ không chính thức phổ biến.

\begin{table}
    \caption{Embedding Stability Gap Analysis (Metric: Cosine Distance Diff)}\label{tab:embedding_benchmark}
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{l c c c c c c}
            \toprule
            \textbf{Model Identifier}                   & \textbf{Avg Gap} $\uparrow$ & \textbf{Storm} $\uparrow$ & \textbf{Domain} $\uparrow$ & \textbf{Category} $\uparrow$ & \textbf{Slang}  $\uparrow$ & \textbf{Abbrev}  $\uparrow$ \\
            \midrule
            \textbf{vn-doc-embedding*} \cite{ref_vndoc} & \textbf{0.265}              & 0.239                     & 0.224                      & \textbf{0.162}               & \textbf{0.283}             & \textbf{0.419}              \\
            vn-bi-encoder                               & 0.193                       & 0.155                     & 0.213                      & 0.107                        & 0.108                      & 0.381                       \\
            vn-sbert \cite{ref_visobert}                & 0.188                       & \textbf{0.355}            & 0.172                      & 0.033                        & -0.005                     & 0.386                       \\
            bge-m3 \cite{ref_bge}                       & 0.160                       & -0.003                    & \textbf{0.269}             & 0.070                        & 0.192                      & 0.273                       \\
            multilingual-e5 \cite{ref_e5}               & 0.058                       & 0.018                     & 0.088                      & 0.022                        & 0.073                      & 0.091                       \\
            \bottomrule
        \end{tabular}%
    }
\end{table}

\begin{table}
    \caption{Hiệu suất Mô hình Đã Tinh chỉnh}\label{tab:models}
    \centering
    \begin{tabular}{l l c c}
        \toprule
        \textbf{Nhiệm vụ Mô hình} & \textbf{Loại Lớp}   & \textbf{Độ chính xác} ($\uparrow$) & \textbf{Độ trễ} ($\downarrow$) \\
        \midrule
        Bộ phân loại Sentiment    & 3 lớp (Pos/Neg/Neu) & \textbf{93,5\%}                    & 12ms                           \\
        Bi-CrossEncoder Reranker  & Xếp hạng Relevance  & 91,0\%                             & 45ms                           \\
        Bộ phân loại Taxonomy     & 7 lớp (T1-T7)       & 89,2\%                             & 14ms                           \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Kết quả và Phân tích}

\subsection{Lựa chọn Thuật toán Phân cụm}
Chúng tôi so sánh phương pháp SAHC của chúng tôi (sử dụng lõi HDBSCAN) với các baselines. HDBSCAN được chọn vì khả năng xử lý Nhiễu và Cân bằng vượt trội.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/trend_tsne.png}
    \caption{Trực quan hóa t-SNE của các Cụm Sự kiện. Các màu đại diện cho các sự kiện riêng biệt được cô lập bởi thuật toán.} \label{fig:tsne}
\end{figure}

Phiếu chiếu t-SNE trong Hình \ref{fig:tsne} chứng minh hiệu quả của phân cụm của chúng tôi. Các sự kiện riêng biệt (các cụm màu) được phân tách tốt trong không gian ngữ nghĩa, trong khi các điểm nhiễu (xám) phân tán và được cô lập hiệu quả bởi thuật toán.

\begin{table}
    \caption{So sánh Phương pháp Phân cụm}\label{tab:clustering_select}
    \centering
    \begin{tabular}{l c c c c c}
        \toprule
        \textbf{Phương pháp}          & \textbf{Cụm (k)} & \textbf{Điểm Nhiễu} & \textbf{Silh. Score} ($\uparrow$) & \textbf{DB Index} ($\downarrow$) & \textbf{Thời gian} ($\downarrow$) \\
        \midrule
        K-Means                       & 253              & 0                   & 0,024                             & 3,177                            & 8,45s                             \\
        \textbf{HDBSCAN (Baseline)}   & \textbf{460}     & 1.984               & 0,109                             & \textbf{2,094}                   & 16,00s                            \\
        BERTopic                      & 223              & 1.801               & 0,112                             & 2,363                            & 55,72s                            \\
        \textbf{SAHC (Của chúng tôi)} & \textbf{315}     & \textbf{985}        & \textbf{0,135}                    & \textbf{1,951}                   & \textbf{18,4s}                    \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Phân tích:} Như hiển thị trong Bảng \ref{tab:clustering_select}, \textbf{K-Means} cho thấy không phù hợp với dữ liệu luồng xã hội vì nó ép tất cả nhiễu vào các cụm. \textbf{HDBSCAN Baseline}, trong khi xác định đúng nhiễu, chịu \textbf{phân mảnh quá mức} (460 cụm) và loại bỏ dữ liệu quá nhiều (1.984 điểm nhiễu), thường chia các sự kiện đơn lẻ thành nhiều micro-cụm.
Phương pháp \textbf{SAHC} của chúng tôi giải quyết điều này bằng cách sử dụng News Anchors để ``nối'' các micro-cụm này. Điều này thành công cô đặc không gian thành \textbf{315 sự kiện mạch lạc} và khôi phục $\approx 50\%$ dữ liệu hợp lệ trước đó bị loại bỏ là nhiễu (Nhiễu giảm xuống 985), tăng đáng kể Silhouette Score lên \textbf{0,135}.

\subsection{Đánh giá Định lượng (Mini-Ground Truth)}
Tất cả các metric phân cụm dựa trên nhãn (NMI, Purity, BCubed-F1, Entropy) được tính toán chỉ trên tập con Mini-Ground Truth (N=300), vì bộ dữ liệu đầy đủ không chứa nhãn sự kiện do con người chú thích.
Bảng \ref{tab:minigt} báo cáo hiệu suất của \textbf{cài đặt SAHC mặc định}, trong khi Bảng \ref{tab:ablation} cho thấy thêm rằng \textbf{pipeline hybrid đầy đủ} (thêm anchoring và tinh chỉnh LLM) cải thiện NMI từ 0,54 lên \textbf{0,5978}.

\begin{table}
    \caption{Hiệu suất trên Mini-Ground Truth (N=300) -- Cài đặt SAHC Cơ sở}\label{tab:minigt}
    \centering
    \begin{tabular}{l c l}
        \toprule
        \textbf{Metric}           & \textbf{Giá trị} & \textbf{Diễn giải}                         \\
        \midrule
        \textbf{NMI} ($\uparrow$) & \textbf{0,54}    & Thông tin tương hỗ tốt với nhãn người      \\
        Purity ($\uparrow$)       & 0,65             & Tính nhất quán lớp chiếm ưu thế cao        \\
        BCubed-F1 ($\uparrow$)    & 0,35             & Precision/recall cân bằng cho phân cụm     \\
        Entropy ($\downarrow$)    & 4,85             & Entropy thấp hơn chỉ ra cụm tinh khiết hơn \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Định nghĩa Metric:} Để đảm bảo đánh giá nghiêm ngặt, chúng tôi chọn các metric bao phủ các khía cạnh khác nhau của chất lượng phân cụm:
\begin{itemize}
    \item \textbf{Thông tin Tương hỗ Chuẩn hóa (NMI):} Đo sự phù hợp giữa nhãn cụm $Y$ và ground truth $C$:
          \begin{equation}
              NMI(Y, C) = \frac{2 \cdot I(Y; C)}{H(Y) + H(C)}
          \end{equation}
          Nơi $I$ là thông tin tương hỗ và $H$ là entropy.
    \item \textbf{Purity:} Lượng hóa sự chiếm ưu thế của lớp đa số trong mỗi cụm:
          \begin{equation}
              Purity(\Omega, C) = \frac{1}{N} \sum_k \max_j |\omega_k \cap c_j|
          \end{equation}
          Nơi $N$ là tổng số điểm dữ liệu, $\Omega = \{\omega_1, ..., \omega_k\}$ là tập các cụm, và $C = \{c_1, ..., c_j\}$ là tập các lớp ground truth.
    \item \textbf{Entropy:} Đo tính đồng nhất của một cụm (thấp hơn tốt hơn):
          \begin{equation}
              E(C) = - \sum_{k} \frac{n_k}{n} \log_2 \frac{n_k}{n}
          \end{equation}
          Nơi $n_k$ là số điểm trong cụm $k$. Giá trị gần 0 hm ý cụm chỉ chứa một lớp sự kiện duy nhất.
    \item \textbf{BCubed-F1:} Metric theo từng phần tử cân bằng tính đồng nhất và hoàn chỉnh của cụm. Nó là trung bình điều hòa của:
          \begin{itemize}
              \item \textit{BCubed Precision ($P_{b^3}$):} Precision trung bình theo từng mục. Với mỗi mục $x$, nó tính tỷ lệ các mục trong cụm của $x$ $C(x)$ có cùng nhãn lớp $L(x)$:
                    \begin{equation}
                        P_{b^3} = \frac{1}{N} \sum_{x \in X} \frac{|C(x) \cap L(x)|}{|C(x)|}
                    \end{equation}
              \item \textit{BCubed Recall ($R_{b^3}$):} Recall trung bình theo từng mục. Với mỗi mục $x$, nó tính tỷ lệ các mục trong lớp của $x$ $L(x)$ được nhóm vào cùng cụm $C(x)$:
                    \begin{equation}
                        R_{b^3} = \frac{1}{N} \sum_{x \in X} \frac{|C(x) \cap L(x)|}{|L(x)|}
                    \end{equation}
          \end{itemize}
          \begin{equation}
              F1_{b^3} = 2 \cdot \frac{P_{b^3} \cdot R_{b^3}}{P_{b^3} + R_{b^3}}
          \end{equation}
          Khác với purity, BCubed phạt cả gộp quá mức (Precision thấp) và phân mảnh quá mức (Recall thấp).
    \item \textbf{Silhouette Score:} Lượng hóa mức độ một đối tượng phù hợp với cụm của nó so với cụm lân cận gần nhất:
          \begin{equation}
              S(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}
          \end{equation}
          Nơi $a(i)$ là khoảng cách trung bình trong cụm và $b(i)$ là khoảng cách trung bình đến cụm gần nhất. $S \in [-1, 1]$, nơi các giá trị gần 1 chỉ ra các cụm dày đặc, phân tách tốt.
\end{itemize}

\subsection{Đánh giá Batch và Phân tích Thành phần}
Để lượng hóa đóng góp của mỗi module, chúng tôi tiến hành nghiên cứu ablation trên tập con \textbf{Mini-Ground Truth} (N=300), nơi các metric dựa trên nhãn (NMI, BCubed-F1, Entropy) được định nghĩa tốt.
Bảng \ref{tab:ablation} cho thấy việc thêm dần anchoring và tinh chỉnh LLM cải thiện sự phù hợp phân cụm với nhãn người.

\begin{table}
    \caption{Nghiên cứu Ablation trên Mini-Ground Truth (N=300)}\label{tab:ablation}
    \centering
    \begin{tabular}{l c c c}
        \toprule
        \textbf{Cấu hình}               & \textbf{NMI} ($\uparrow$) & \textbf{BCubed-F1} ($\uparrow$) & \textbf{Entropy} ($\downarrow$) \\
        \midrule
        Baseline (Chỉ HDBSCAN)          & 0,53                      & 0,31                            & 4,91                            \\
        \textbf{Pipeline Hybrid Đầy đủ} & \textbf{0,5978}           & \textbf{0,3821}                 & \textbf{4,4178}                 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Phân tích Định tính: LLM-làm-Giám-khảo}
Các metric truyền thống như Silhouette Score thường không nắm bắt được tính mạch lạc ngữ nghĩa. Để giải quyết điều này, chúng tôi triển khai giao thức \textbf{LLM-làm-Giám-khảo}, nơi một mô hình instruction-tuned mạnh (Gemini Pro) hoạt động như proxy cho đánh giá của con người.
\begin{itemize}
    \item \textbf{Giao thức:} LLM được cung cấp nội dung của một cụm và được hỏi hai câu hỏi: (1) ``Cụm này có mô tả một sự kiện duy nhất, riêng biệt không?'' (Tính Nhất quán Chủ đề, đánh giá 0-1), và (2) Cho hai cụm xung đột, ``Cụm nào mạch lạc hơn?'' (Tỷ lệ Thắng Theo cặp).
    \item \textbf{Prompting:} Chúng tôi sử dụng chiến lược ``Role-Play'' nơi LLM đóng vai như một biên tập viên tin tức chuyên gia. Đặc tả prompt đầy đủ được cung cấp trong Phụ lục Bảng \ref{tab:prompt_eval}.
\end{itemize}
Sử dụng framework này, chúng tôi đánh giá 100 cụm ngẫu nhiên:
\begin{itemize}
    \item \textbf{Sự kiện Rõ ràng (ví dụ: ``Xung đột Trung Đông''):} Đạt \textbf{Tính Nhất quán $\approx$ 1,0} và Tỷ lệ Thắng cao.
    \item \textbf{Cụm Hỗn hợp/Nhiễu:} Được LLM đánh dấu chính xác với điểm thấp hơn đáng kể (0,64), xác nhận tính hữu dụng của metric cho đảm bảo chất lượng tự động.
\end{itemize}

\subsection{Nghiên cứu Trường hợp Chi tiết}
Để hiểu rõ hơn hành vi của hệ thống, chúng tôi phân tích ba trường hợp cụ thể từ triển khai thử nghiệm:

\begin{enumerate}
    \item \textbf{Trường hợp Thành công: ``Vụ xả súng Bãi biển Bondi'' (T1).}
          \begin{itemize}
              \item \emph{Đầu vào:} ``{\fontencoding{T5}\selectfont Vụ xả súng tại bãi biển Bondi khiến 15 người thiệt mạng sau vụ tấn công bằng dao.}''
              \item \emph{Phân tích:} Mặc dù tin tức đồng thời về ``Du lịch Úc'', thuật toán SAHC đã cô lập đúng sự kiện này do trùng lặp từ vựng mạnh với các từ khóa ``Khủng hoảng''. LLM gán nó vào \textbf{T1 (Rủi ro Công cộng)} với độ tin cậy cao (>0,9).
          \end{itemize}

    \item \textbf{Trường hợp Thành công: ``Xung đột Trung Đông'' (T1).}
          \begin{itemize}
              \item \emph{Metrics:} Cụm này đạt điểm Tính Nhất quán Ngữ nghĩa \textbf{0,96} và Tỷ lệ Thắng \textbf{0,67} so với các cụm nhiễu.
              \item \emph{Insight:} Khối lượng lớn các neo tin tức đã xác minh cho phép hệ thống hấp thụ phản ứng mạng xã hội hiệu quả mà không ``trôi'' vào các chủ đề không liên quan.
          \end{itemize}

    \item \textbf{Phân tích Thất bại: ``Tin Quốc tế Hỗn hợp''.}
          \begin{itemize}
              \item \emph{Vấn đề:} Thuật toán gộp ``Vụ kiện gốc Việt'' với ``Xung đột lãnh thổ'' thành một cụm duy nhất.
              \item \emph{Nguyên nhân Gốc:} Điều này có thể xảy ra do thuật ngữ ngoại giao chung (ví dụ: ``chủ quyền'', ``luật quốc tế'') hoạt động như các neo vector giả, nối hai chủ đề khác nhau về ngữ nghĩa nhưng tương tự về từ vựng.
              \item \emph{Phát hiện:} Quan trọng là, giao thức \textbf{LLM-làm-Giám-khảo} của chúng tôi đã đánh dấu lỗi này, gán Điểm Tính Nhất quán thấp \textbf{0,64} và \textbf{Tỷ lệ Thắng 0,00}. Điều này chứng minh khả năng của hệ thống tự động phát hiện suy giảm chất lượng để xem xét của con người.
          \end{itemize}
\end{enumerate}

\section{Thảo luận}

\subsection{Thách thức và Bài học Rút ra}
Triển khai hệ thống đã bộc lộ ba trở ngại kỹ thuật chính:
\begin{enumerate}
    \item \textbf{Sự mơ hồ Ngữ nghĩa:} Các cộng đồng khác nhau sử dụng các thuật ngữ khác nhau cho cùng một sự kiện (ví dụ: ``Bão Yagi'' vs. ``Cơn bão số 3''). Chúng tôi giải quyết vấn đề này bằng \textbf{LLM-based Deduplication} (Phụ lục Bảng \ref{tab:prompt_dedup}) và điểm Hybrid Matching.
    \item \textbf{Khoảng trống Đánh giá:} Việc thiếu nhãn ground truth trong dữ liệu luồng gây khó khăn cho việc xác thực. Chúng tôi vượt qua bằng cách thiết lập giao thức \textbf{``Mini-Ground Truth''} (gán nhãn thủ công 300 mẫu) để chuẩn hóa các chỉ số NMI và F1.
    \item \textbf{Đánh đổi Độ trễ vs. Độ chính xác:} Gọi LLM cho mỗi cụm tạo ra độ trễ 2-3s. Chúng tôi giảm thiểu bằng cách tách pipeline thành \textbf{Fast Path} (Spark) cho phát hiện tức thì và \textbf{Slow Path} (Async Workers) cho phân tích sâu. Ngoài ra, chúng tôi triển khai \textbf{Quy tắc Cập nhật Tăng dần}: LLM chỉ được kích hoạt lại nếu cụm tăng $>20\%$ về khối lượng, giảm đáng kể chi phí suy luận dư thừa.
\end{enumerate}

\subsection{Nghiên cứu Khử (Ablation Study) và Phân tích Thành phần}
Để định lượng đóng góp của từng module, chúng tôi thực hiện nghiên cứu khử (ablation study) bằng cách vô hiệu hóa các thành phần có chọn lọc (Bảng \ref{tab:ablation}).
\begin{itemize}
    \item \textbf{Không có News Anchors:} Loại bỏ neo làm giảm đáng kể sự đồng thuận phân cụm với nhãn của người, xác nhận rằng dữ liệu xã hội đơn độc quá nhiễu cho phân cụm mạch lạc.
    \item \textbf{Không có LLM Refinement:} Sử dụng centroid thô thay vì tóm tắt LLM dẫn đến tiêu đề sự kiện mơ hồ, giảm điểm khả năng diễn giải (đánh giá người) từ 4.5 xuống 2.1.
\end{itemize}

\section{Kết luận và Công việc Tương lai}
\subsection{Kết luận}

Trong công trình này, chúng tôi trình bày một framework \textbf{Phát hiện Sự kiện Hai Đường (Dual-Path Event Detection)} mới giúp dung hòa hiệu quả sự đánh đổi giữa khả năng đáp ứng thời gian thực và chiều sâu ngữ nghĩa. Bằng cách tách rời phân cụm nhạy cảm với độ trễ (Fast Path) khỏi suy luận LLM tốn kém tính toán (Slow Path), hệ thống của chúng tôi đạt được phát hiện sự kiện gần như tức thời trên Fast Path (2.4s độ trễ xử lý trong thiết lập thử nghiệm của chúng tôi) trong khi duy trì chất lượng phân cụm mạnh trên tập con Mini-Ground Truth đã gán nhãn thông qua News Anchoring (SAHC cơ bản: NMI = 0.54, Purity = 0.65; pipeline lai đầy đủ: NMI tốt nhất = 0.5978).
Trên bộ dữ liệu đầy đủ, các chỉ số nội tại và đánh giá LLM-as-a-Judge xác nhận thêm tính nhất quán cụm được cải thiện so với các phương pháp cơ sở. Chúng tôi chứng minh rằng các phương pháp dựa trên mật độ truyền thống đơn độc không đủ cho dữ liệu xã hội nhiễu, đòi hỏi phương pháp lai nơi tin tức đã xác nhận đóng vai trò ổn định ngữ nghĩa. Công việc tương lai sẽ tập trung vào triển khai các mô hình embedding lượng tử hóa đến thiết bị biên và tích hợp tín hiệu đa phương thức (hình ảnh/video) để nâng cao nhận thức tình huống cho quản lý khủng hoảng.

\subsection{Công việc Tương lai}
Để mở rộng hệ thống hướng tới triển khai sản xuất ($>10.000$ sự kiện mỗi giây), chúng tôi lên kế hoạch các mở rộng sau:
\begin{itemize}
    \item \textbf{Tối ưu hóa Mô hình:} Áp dụng lượng tử hóa (ONNX/INT8) và suy luận nhận thức batch để giảm độ trễ và chi phí embedding trong khi bảo toàn chất lượng phân cụm.
    \item \textbf{Mô hình hóa Cảm xúc Nâng cao:} Mở rộng sentiment 3 lớp hiện tại thành cảm xúc chi tiết (ví dụ: Giận dữ, Sợ hãi, Hy vọng) để mô tả tốt hơn động lực khủng hoảng và phản ứng của công chúng.
    \item \textbf{Vòng Phản hồi Từ Người dùng:} Thêm công cụ chỉnh sửa mức dashboard (gộp/tách cụm, gán lại nhãn taxonomy) và đưa các chỉnh sửa đã xác thực vào Active Learning cho cải tiến liên tục.
    \item \textbf{Mở rộng Sản xuất:} Chuyển từ Spark Local sang triển khai phân tán trên \textbf{Kubernetes/YARN}, cho phép mở rộng theo chiều ngang độc lập của Kafka, Spark executors, và LLM workers.
\end{itemize}

\section{Cân nhắc Đạo đức và Hạn chế}
Với tư cách là một hệ thống tích hợp AI phân tích diễn ngôn công cộng, chúng tôi tuân theo các thực hành dữ liệu đạo đức và thừa nhận rõ ràng các hạn chế chính.
\begin{itemize}
    \item \textbf{Bảo vệ Quyền riêng tư:} Mặc dù hệ thống xử lý các bài đăng công khai, chúng tôi che giấu Thông tin Nhận dạng Cá nhân (PII) như số điện thoại và địa chỉ cụ thể trước khi lưu trữ. Chúng tôi trình bày kết quả ở cấp \emph{cụm/sự kiện} để tránh nhắm vào cá nhân.
    \item \textbf{Thiên lệch và Phạm vi Nguồn:} Các ``neo'' tin tức có thể phản ánh thiên lệch biên tập hoặc phạm vi không đầy đủ. Để giảm thiểu, pipeline hỗ trợ rõ ràng khám phá sự kiện \textbf{Chỉ-Xã-hội} (qua HDBSCAN) để các chủ đề ít được đưa tin vẫn có thể xuất hiện; tuy nhiên, lựa chọn neo có thể ảnh hưởng đến những gì trở thành ``đã xác minh'' trong các tóm tắt hạ nguồn.
    \item \textbf{An toàn và Độ tin cậy của LLM:} LLMs có thể tạo các tóm tắt có vẻ hợp lý nhưng không chính xác. Do đó, chúng tôi hạn chế việc sử dụng LLM vào \emph{làm giàu hậu kỳ} (không kích hoạt phát hiện) và áp dụng chính sách xác minh \textbf{Human-in-the-Loop} cho các cảnh báo rủi ro cao (T1--Khủng hoảng) trước khi leo thang bên ngoài.
\end{itemize}

\begin{credits}
    \subsubsection{\ackname} We would like to express our sincere gratitude to Dr. Do Trong Hop and Mr. Nguyen Ngoc Quy for their invaluable supervision, critical insights, and continuous support throughout the development of this research. Their expert guidance on system architecture and methodology was instrumental in the success of this project.
\end{credits}

\bibliographystyle{splncs04}
\bibliography{refs}



\appendix
\section{Phụ lục: Kỹ thuật Prompt}
Để đảm bảo tính tái lập, chúng tôi cung cấp các prompt cụ thể được sử dụng trong các module cốt lõi của hệ thống.

\begin{table}[h]
    \caption{Prompt Tinh chỉnh LLM (Giai đoạn 6)}\label{tab:prompt_refinement}
    \centering
    \begin{tabular}{|p{0.95\linewidth}|}
        \hline
        \textbf{Vai trò:} Biên tập viên Cấp cao (Tiếng Việt).                                                                                                      \\
        \textbf{Nhiệm vụ:} Xác định tiêu đề và trích xuất cấu trúc 5W1H.                                                                                           \\
        \textbf{Quy tắc:}                                                                                                                                          \\
        1. Tiêu đề: Tiêu đề tiếng Việt ngắn gọn, có sự thật ($\le$ 15 từ).                                                                                         \\
        2. Tóm tắt: Chi tiết 4-6 câu bao gồm số liệu, ngày tháng, địa điểm.                                                                                        \\
        3. 5W1H: Trích xuất Ai, Cái gì, Ở đâu, Khi nào, Tại sao.                                                                                                   \\
        4. Lời khuyên: Cung cấp lời khuyên chiến lược hành động cho Nhà nước và Doanh nghiệp.                                                                      \\
        5. Phân loại: Gán một trong 7 danh mục (T1-T7).                                                                                                            \\
        \textbf{Định dạng Đầu ra:} JSON dict chứa \texttt{refined\_title}, \texttt{summary}, \texttt{category}, \texttt{advice\_state}, \texttt{advice\_business}. \\
        \hline
    \end{tabular}
\end{table}

\begin{table}[h]
    \caption{Prompt LLM-làm-Giám-khảo (Đánh giá)}\label{tab:prompt_eval}
    \centering
    \begin{tabular}{|p{0.95\linewidth}|}
        \hline
        \textbf{Vai trò:} Chuyên gia Chất lượng Phân cụm Tin tức.                                                                                   \\
        \textbf{Ngữ cảnh:} Hai cụm không tên (A và B) bao gồm các bài đăng mẫu.                                                                     \\
        \textbf{Nhiệm vụ:} So sánh và quyết định:                                                                                                   \\
        1. Cụm nào MẠCH LẠC hơn (các bài đăng thảo luận cùng chủ đề)?                                                                               \\
        2. Cụm nào RÕ RÀNG hơn (ít trộn chủ đề hơn)?                                                                                                \\
        \textbf{Định dạng Đầu ra:} JSON dict với \texttt{better\_cluster} (``A'', ``B'', hoặc ``Tie''), \texttt{confidence}, và \texttt{reasoning}. \\
        \hline
    \end{tabular}
\end{table}

\begin{table}[h]
    \caption{Prompt Loại bỏ Trùng lặp Ngữ nghĩa (Giai đoạn 4)}\label{tab:prompt_dedup}
    \centering
    \begin{tabular}{|p{0.95\linewidth}|}
        \hline
        \textbf{Vai trò:} Biên tập viên Cấp cao.                                                          \\
        \textbf{Nhiệm vụ:} Xác định các tiêu đề tham chiếu đến CÙNG MỘT sự kiện thực trong thế giới thực. \\
        \textbf{Tiêu chí cho Khớp (Phải khớp tất cả 3):}                                                  \\
        1. CÙNG Địa điểm (ví dụ: ``Hà Nội'' vs ``Hà Nội'' $\checkmark$).                                  \\
        2. CÙNG Khung thời gian (ví dụ: ``Hôm nay'' vs ``Hôm nay'' $\checkmark$).                         \\
        3. CÙNG Thực thể Lõi (ví dụ: ``Bão Yagi'' vs ``Bão số 3'' $\checkmark$).                          \\
        \textbf{Định dạng Đầu ra:} JSON dict ánh xạ \texttt{``Tiêu đề Gốc'': ``Tiêu đề Chuẩn''}.          \\
        \hline
    \end{tabular}
\end{table}

\begin{table}[h]
    \caption{Prompt Lọc Google Trends (Giai đoạn 6)}\label{tab:prompt_trends}
    \centering
    \begin{tabular}{|p{0.95\linewidth}|}
        \hline
        \textbf{Vai trò:} Bộ Phân loại Google Trends (Việt Nam).                                                              \\
        \textbf{Nhiệm vụ:} Lọc NHIỄU và GỘP các trùng lặp.                                                                    \\
        \textbf{Quy tắc Lọc (Loại bỏ):}                                                                                       \\
        1. Thời tiết/Tiện ích (ví dụ: ``thời tiết hôm nay'', ``kết quả xổ số'').                                              \\
        2. Thuật ngữ Rộng Chung (ví dụ: ``tình yêu'', ``tin tức'', ``video'').                                                \\
        \textbf{Quy tắc Gộp (Nhóm):}                                                                                          \\
        - Kết hợp các biến thể của cùng sự kiện (ví dụ: ``lịch AFF Cup'' + ``bảng xếp hạng AFF Cup'' $\to$ ``AFF Cup 2024''). \\
        \textbf{Định dạng Đầu ra:} JSON dict với danh sách \texttt{filtered} và map \texttt{merged}.                          \\
        \hline
    \end{tabular}
\end{table}
\end{document}
