"""
Extract sentiment training data from existing pipeline results.
Uses the sentiment field already generated by the pipeline's sentiment analyzer.
Outputs: data/sentiment_train.jsonl with format {"text": "...", "label": "Positive/Negative/Neutral"}
"""
import json
import os
import glob
from collections import Counter
from rich.console import Console

console = Console()

# Configuration
INPUT_PATTERNS = [
    "crawlers/results/results_*.json",
    "crawlers/new_data/facebook/*.json"
]
OUTPUT_FILE = "data/sentiment_train.jsonl"

# Simple keyword-based sentiment for bootstrapping
POSITIVE_KEYWORDS = [
    "tuyệt vời", "xuất sắc", "hạnh phúc", "vui", "thành công", "chiến thắng",
    "ủng hộ", "yêu thích", "tốt", "hay", "giỏi", "đẹp", "thích", "cảm ơn",
    "chúc mừng", "hài lòng", "ấn tượng", "tích cực", "lạc quan"
]

NEGATIVE_KEYWORDS = [
    "tệ", "xấu", "thất bại", "buồn", "giận", "tức", "ghét", "chán",
    "thất vọng", "phẫn nộ", "bất bình", "lo ngại", "đau", "khổ", "chết",
    "tai nạn", "lừa đảo", "phản đối", "chỉ trích", "bê bối", "scandal"
]

def classify_sentiment(text):
    """Simple keyword-based sentiment as bootstrap."""
    text_lower = text.lower()
    
    pos_count = sum(1 for kw in POSITIVE_KEYWORDS if kw in text_lower)
    neg_count = sum(1 for kw in NEGATIVE_KEYWORDS if kw in text_lower)
    
    if pos_count > neg_count and pos_count >= 2:
        return "Positive"
    elif neg_count > pos_count and neg_count >= 2:
        return "Negative"
    return None  # Skip neutral/ambiguous for cleaner training data

def extract_data():
    samples = []
    
    for pattern in INPUT_PATTERNS:
        files = glob.glob(pattern)
        console.print(f"[dim]Checking {pattern}: {len(files)} files[/dim]")
        
        for f in files:
            try:
                with open(f, 'r', encoding='utf-8') as file:
                    data = json.load(file)
                    
                if not isinstance(data, list):
                    continue
                    
                for item in data:
                    content = item.get('post_content') or item.get('content') or item.get('text', '')
                    if not content or len(content) < 30:
                        continue
                    
                    # Try existing sentiment label first
                    label = item.get('sentiment')
                    
                    # Only keep clear Positive/Negative labels
                    if label not in ['Positive', 'Negative']:
                        label = classify_sentiment(content)
                    
                    if label in ['Positive', 'Negative']:
                        samples.append({
                            "text": content[:512],
                            "label": label
                        })
            except Exception as e:
                pass
                
    return samples

def main():
    console.print("[cyan]Extracting sentiment training data...[/cyan]")
    
    samples = extract_data()
    
    if not samples:
        console.print("[red]No samples found.[/red]")
        return
    
    # Deduplicate
    seen = set()
    unique = []
    for s in samples:
        key = s['text'][:100]
        if key not in seen:
            seen.add(key)
            unique.append(s)
    
    # Show distribution
    dist = Counter(s['label'] for s in unique)
    console.print(f"[green]Total: {len(unique)} samples[/green]")
    console.print(f"[dim]Distribution: {dict(dist)}[/dim]")
    
    # Save
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        for s in unique:
            f.write(json.dumps(s, ensure_ascii=False) + '\n')
    
    console.print(f"[bold green]Saved to {OUTPUT_FILE}[/bold green]")

if __name__ == "__main__":
    main()
