"""
Extract sentiment training data from existing pipeline results.
Uses the sentiment labels already generated by wonrax/phobert-base-vietnamese-sentiment.
Outputs: data/sentiment_train.jsonl with format {"text": "...", "label": "Positive/Negative/Neutral"}
"""
import json
import os
import glob
from collections import Counter
from rich.console import Console

console = Console()

# Configuration
INPUT_PATTERNS = [
    "crawlers/results/results_*.json",
]
OUTPUT_FILE = "data/sentiment_train.jsonl"

VALID_LABELS = {"Positive", "Negative", "Neutral"}

def extract_sentiment_data():
    samples = []
    
    for pattern in INPUT_PATTERNS:
        files = glob.glob(pattern)
        console.print(f"[dim]Checking {pattern}: {len(files)} files[/dim]")
        
        for f in files:
            try:
                with open(f, 'r', encoding='utf-8') as file:
                    data = json.load(file)
                    
                if not isinstance(data, list):
                    continue
                    
                for item in data:
                    content = item.get('post_content') or item.get('content') or item.get('text', '')
                    sentiment = item.get('sentiment')
                    
                    if not content or len(content) < 20:
                        continue
                    
                    if sentiment not in VALID_LABELS:
                        continue
                    
                    samples.append({
                        "text": content[:512],
                        "label": sentiment
                    })
            except Exception as e:
                console.print(f"[yellow]Skip {f}: {e}[/yellow]")
                
    return samples

def main():
    console.print("[cyan]Extracting sentiment training data...[/cyan]")
    
    samples = extract_sentiment_data()
    
    if not samples:
        console.print("[red]No samples found.[/red]")
        return
    
    # Deduplicate
    seen = set()
    unique_samples = []
    for s in samples:
        key = s['text'][:100]
        if key not in seen:
            seen.add(key)
            unique_samples.append(s)
    
    # Show distribution
    dist = Counter(s['label'] for s in unique_samples)
    console.print(f"[green]Total unique samples: {len(unique_samples)}[/green]")
    console.print(f"[bold]Distribution:[/bold]")
    for label, count in sorted(dist.items()):
        pct = count / len(unique_samples) * 100
        console.print(f"  {label}: {count} ({pct:.1f}%)")
    
    # Save
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        for s in unique_samples:
            f.write(json.dumps(s, ensure_ascii=False) + '\n')
    
    console.print(f"[bold green]Saved to {OUTPUT_FILE}[/bold green]")

if __name__ == "__main__":
    main()
