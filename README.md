# Multi-Source Trend Detection and ANalysis System

Real-time trend detection tá»« multiple sources: Google Trends, Facebook, News sites.

## ðŸ“ Project Structure

```
â”œâ”€â”€ src/                        # Core analysis modules
â”‚   â”œâ”€â”€ pipeline/               # Pipeline orchestration
â”‚   â”‚   â”œâ”€â”€ main_pipeline.py    # Main trend discovery pipeline
â”‚   â”‚   â”œâ”€â”€ pipeline_stages.py  # SAHC clustering & matching stages
â”‚   â”‚   â””â”€â”€ trend_scoring.py    # G/F/N score calculator
â”‚   â”œâ”€â”€ core/                   # NLP & Analysis engines
â”‚   â”‚   â”œâ”€â”€ analysis/           # Clustering & Summarization
â”‚   â”‚   â”œâ”€â”€ extraction/         # NER & Taxonomy classification
â”‚   â”‚   â””â”€â”€ llm/                # LLM Refinement logic
â”‚   â””â”€â”€ utils/                  # Shared utilities
â”‚
â”œâ”€â”€ crawlers/                   # Data collection crawlers
â”‚   â”œâ”€â”€ vnexpress_crawler.py    # VNExpress news crawler
â”‚   â”œâ”€â”€ thanhnien_crawler.py    # Thanh Nien news crawler
â”‚   â””â”€â”€ facebook/               # Facebook page crawler
â”‚
â”œâ”€â”€ results/                    # Output files (gitignored)
â”‚   â”œâ”€â”€ results.json            # Matched trends data
â”‚   â”œâ”€â”€ trend_analysis.png      # Top trends chart
â”‚   â””â”€â”€ trend_tsne.png          # t-SNE visualization
â”‚
â”œâ”€â”€ notebooks/                  # Jupyter notebooks
â”‚   â””â”€â”€ kaggle_trend_analysis.ipynb  # Kaggle-ready notebook
â”‚
â”œâ”€â”€ data/                       # Crawled data storage
â”œâ”€â”€ flow.mmd                    # Pipeline flow diagram
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ run_crawlers.py             # Crawler orchestration
```

## Results Output

## Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
playwright install firefox
```

### 2. Run Trend Analysis

```bash
# Basic usage (Search-Social-News integration)
python src/pipeline/main_pipeline.py --social crawlers/facebook/*.json --trends crawlers/trendings/*.csv --output results.json

# Advanced: Enable LLM refinement & Summarization
python src/pipeline/main_pipeline.py --social crawlers/facebook/*.json --trends crawlers/trendings/*.csv --llm --summarize-all --output results.json
```

### 3. Evaluate & Visualize

```bash
# Default: Direct trend assignment (recommended)
python crawlers/evaluate_trends.py --input results.json

# Experimental: HDBSCAN clustering
python crawlers/evaluate_trends.py --input results.json --use-hdbscan

# Filter routine trends (weather, prices, etc.)
python crawlers/evaluate_trends.py --input results.json --filter-routine
```

## Pipeline Flow

```mermaid
Google Trends CSV â†’ Build Aliases â†’ Normalize Texts
                                         â†“
News + FB Posts â†’ Normalize â†’ Embed â†’ Match â†’ Valid Trends â†’ Score â†’ Classify
```

## Options

### main_pipeline.py

| Option | Description |
| :--- | :--- |
| `--social` | Path to social/FB JSON files (supports globs) |
| `--trends` | Path to Google Trends CSV files |
| `--news` | Path to News CSV files |
| `--llm` | Enable LLM Refinement for naming and classification |
| `--refine-trends` | Use LLM to clean Google Trends noise before matching |
| `--save-all` | Include unmatched posts in the output JSON |
| `--output` | Save results to specified JSON file |

### evaluate_trends.py

### evaluate_trends.py

| Option | Default | Description |
|--------|---------|-------------|
| `--min-posts` | `3` | Minimum posts for valid trend |
| `--use-hdbscan` | `False` | Use HDBSCAN clustering (experimental) |
| `--filter-routine` | `False` | Filter weather/price trends |

## Output

Each trend is scored and classified:

```json
{
  "trend": "CÃ´ng PhÆ°á»£ng",
  "Class": "Social-Driven",
  "Composite": 67.5,
  "G": 45, "F": 82, "N": 30,
  "posts": 156
}
```

**Classifications:**
- `Strong Multi-source`: High G + F + N
- `Social & News`: High F + N
- `Social-Driven`: High Facebook engagement
- `News-Driven`: High news coverage
- `Emerging`: Low scores across all

## Technical Notes

### Why Alias Normalization > NER?
- NER (underthesea) khÃ´ng nháº­n cÃ¡c tÃªn quá»‘c táº¿ (e.g., "Yagi")
- Alias uses Google Trends keywords â†’ higher match accuracy
- Test showed +16% improvement with aliases

### Why Direct Assignment > HDBSCAN?
- Data has 652+ small topics with no density peaks
- HDBSCAN classifies 84% as noise
- Direct trend assignment already provides meaningful clusters
