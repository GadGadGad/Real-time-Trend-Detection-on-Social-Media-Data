\section{Related Work}
Event detection and classification from social media has evolved significantly over the past decade, progressing from statistical methods to neural approaches and, most recently, to Large Language Model (LLM)-augmented systems.
We organize the related literature into four categories: (1) traditional statistical methods, (2) neural and embedding-based approaches, (3) LLM and Retrieval-Augmented Generation (RAG) methods, and (4) real-time streaming architectures.
Table \ref{tab:related_work} provides a comparative summary of representative approaches.

\subsection{Traditional Statistical Methods}
Early event detection and classification systems relied heavily on term-frequency analysis and topic modeling.
\textbf{TF-IDF and BM25} \cite{ref_bm25} approaches identify bursty keywords that deviate from normal frequency patterns, enabling rapid detection of trending topics.
\textbf{Latent Dirichlet Allocation (LDA)} \cite{ref_lda} and its online variants model documents as mixtures of latent topics, allowing unsupervised discovery of event themes.
\textbf{EDCoW} \cite{ref_edcow} introduced wavelet-based signal processing to detect ``bursty'' keywords in Twitter streams, using co-occurrence graphs to cluster related terms into events.

\textbf{Strengths:} These methods are computationally efficient and interpretable, making them suitable for high-throughput scenarios.
\textbf{Limitations:} They struggle with short, noisy social texts where word co-occurrence is sparse, and they cannot capture semantic similarity between lexically different expressions (e.g., ``Typhoon Yagi'' vs. ``Storm No. 3'').

\subsection{Neural and Embedding-Based Approaches}
The advent of deep learning enabled semantic representation of text, significantly improving event detection and classification quality.
\textbf{BERT} \cite{ref_bert} and its variants provide contextualized word embeddings that capture nuanced meaning.
\textbf{Sentence-BERT (SBERT)} \cite{ref_sbert} adapts BERT for efficient sentence-level similarity computation using Siamese networks, enabling fast nearest-neighbor search in embedding space.
\textbf{BERTopic} \cite{ref_bertopic} combines transformer embeddings with clustering (HDBSCAN) and class-based TF-IDF to produce coherent topic representations.

Clustering algorithms form the backbone of many neural event detection and classification systems.
\textbf{K-Means} \cite{ref_kmeans} partitions data into $k$ spherical clusters but requires a predefined cluster count and cannot handle noise.
\textbf{DBSCAN} \cite{ref_dbscan} identifies arbitrary-shaped clusters based on density and explicitly labels noise points, but uses a global density threshold that fails when cluster densities vary.
\textbf{HDBSCAN} \cite{ref_hdbscan} extends DBSCAN with hierarchical density estimation, automatically identifying clusters of varying densities while isolating noise---making it particularly suitable for social media data where event clusters exhibit diverse engagement levels.

\textbf{Strengths:} Embedding-based methods capture semantic relationships and handle vocabulary variation effectively.
\textbf{Limitations:} High-dimensional embeddings incur significant computational overhead, and real-time inference remains challenging at scale. Additionally, these methods often require careful hyperparameter tuning (e.g., cluster thresholds) for each domain.

\subsection{LLM and Retrieval-Augmented Generation}
Large Language Models have recently transformed NLP tasks through emergent reasoning capabilities \cite{ref_llm_reasoning}.
\textbf{GPT-4, Gemini, and Claude} demonstrate strong zero-shot performance on classification, summarization, and information extraction tasks without task-specific fine-tuning.
However, LLMs suffer from ``hallucinations''---generating plausible but factually incorrect content---especially when operating without grounding context.

\textbf{Retrieval-Augmented Generation (RAG)} \cite{ref_rag} addresses this limitation by conditioning LLM responses on retrieved relevant documents.
In the context of event detection and classification, RAG enables grounding LLM summarization in verified social data points, improving factual accuracy.
Several recent works have explored LLM-based event extraction and classification, though most focus on offline batch processing rather than real-time streaming.

\textbf{Strengths:} LLMs provide superior semantic understanding and can generate interpretable event summaries with structured 5W1H (Who, What, When, Where, Why, How) extraction.
\textbf{Limitations:} LLM inference is computationally expensive (typically 100-1000$\times$ slower than embedding-based classification), making direct integration into low-latency streaming pipelines impractical.

\subsection{Real-Time Streaming Architectures}
Real-time event detection and classification requires specialized architectures that balance throughput, latency, and accuracy.
\textbf{Sakaki et al.} \cite{ref_twitter_earthquake} pioneered real-time earthquake detection from Twitter using Kalman filtering and particle filters to track event propagation.
\textbf{TwitterNews+} \cite{ref_streaming_survey} proposed a framework combining incremental clustering with named entity recognition for news event detection and classification from Twitter streams.

Modern streaming systems typically employ the \textbf{Lambda Architecture} (batch + stream layers) or the simpler \textbf{Kappa Architecture} (stream-only), using message brokers like Apache Kafka for fault-tolerant ingestion and stream processors like Spark Streaming or Flink for real-time computation.

\textbf{Strengths:} These architectures achieve sub-second latency and horizontal scalability.
\textbf{Limitations:} Most streaming approaches sacrifice semantic depth for speed, using lightweight features (keywords, hashtags) rather than full semantic understanding.

\subsection{Research Gap and Our Contribution}
As summarized in Table \ref{tab:related_work}, existing approaches typically optimize for \emph{either} low-latency streaming detection \emph{or} high semantic coherence with LLM reasoning---but practical deployments require \textbf{both} simultaneously.
Our \textbf{Dual-Path Architecture} addresses this gap by decoupling fast clustering (SAHC on the Fast Path) from deep semantic enrichment (LLM on the Slow Path), achieving real-time latency while maintaining event interpretability.
Furthermore, we introduce the \textbf{LLM-as-a-Judge} protocol to address the evaluation gap when ground-truth labels are scarce in streaming scenarios.

\begin{table}
    \caption{Comparison of Event Detection and Classification Approaches}\label{tab:related_work}
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{l c c c c c}
            \toprule
            \textbf{Approach}                        & \textbf{Semantic}      & \textbf{Real-time}  & \textbf{Noise}      & \textbf{Interpretable} & \textbf{Scalable}   \\
                                                     & \textbf{Understanding} & \textbf{Capable}    & \textbf{Handling}   & \textbf{Output}        &                     \\
            \midrule
            TF-IDF / BM25 \cite{ref_bm25}            & Low                    & \checkmark          & Limited             & --                     & \checkmark          \\
            LDA \cite{ref_lda}                       & Medium                 & --                  & Limited             & \checkmark             & \checkmark          \\
            EDCoW \cite{ref_edcow}                   & Low                    & \checkmark          & Limited             & --                     & \checkmark          \\
            BERT + Clustering \cite{ref_bert}        & High                   & --                  & Medium              & --                     & --                  \\
            BERTopic \cite{ref_bertopic}             & High                   & --                  & \checkmark          & \checkmark             & --                  \\
            TwitterNews+ \cite{ref_streaming_survey} & Medium                 & \checkmark          & Medium              & --                     & \checkmark          \\
            \midrule
            \textbf{Ours (SAHC + LLM)}               & \textbf{High}          & \textbf{\checkmark} & \textbf{\checkmark} & \textbf{\checkmark}    & \textbf{\checkmark} \\
            \bottomrule
        \end{tabular}%
    }
\end{table}
