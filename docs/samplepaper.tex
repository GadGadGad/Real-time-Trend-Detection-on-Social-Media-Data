% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[utf8]{inputenc}
\usepackage[T1,T5]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[a4paper,left=30mm,right=30mm]{geometry}
%
\begin{document}
%
\title{Real-time Event Detection System: Integrating Social Streams and Mainstream News with LLM Intelligence}
\titlerunning{Real-time Event Detection System}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Nhat Tang\inst{1} \and
    Minh Nhut Le\inst{1} \and
    Do Trong Hop\inst{2} \and
    Nguyen Ngoc Quy\inst{2}}
%

% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Faculty of Computer Science\\
    University of Information Technology, VNU-HCM\\
    \email{\{22521035, 22521060\}@gm.uit.edu.vn} \and
    Faculty of Information and Computer Engineering\\
    University of Information Technology, VNU-HCM\\
    \email{\{hopdt, quynn\}@uit.edu.vn}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
    In the era of information overload, rapidly detecting and understanding emerging events from social streams is crucial for public safety and market intelligence.
    However, practical systems face two core challenges: (i) \textit{semantic ambiguity} in short, noisy social posts, and (ii) an \textit{evaluation gap} due to the lack of labels in streaming data.
    We present a \textbf{real-time event detection system} built on a novel \textbf{Social-Aware Hierarchical Clustering (SAHC)} strategy and a \textbf{dual-path architecture} that combines Spark/Kafka for low-latency detection with asynchronous LLM workers for deep semantic refinement.
    On a \textbf{Mini-Ground Truth} subset (N=300 manually labeled social posts), our full hybrid pipeline achieves \textbf{NMI = 0.5978}.
    On the full dataset of 7{,}605 items, we report intrinsic clustering metrics (Silhouette, Davies--Bouldin, noise ratio) and an \textbf{LLM-as-a-Judge} protocol to assess semantic coherence at scale.

    \keywords{Event Detection \and Social Media Mining \and Real-time Streaming \and Large Language Models \and Hybrid Clustering.}
\end{abstract}

%
%
%
\section{Introduction}
Social media platforms have become the primary source of real-time information. However, processing this data presents unique challenges: (1) \textbf{High Noise Ratio:} Valid news is often buried under spam and daily chatter (e.g., "Lottery results", "Weather"); (2) \textbf{Semantic Ambiguity:} Different communities use vastly different vocabularies to describe the same event (e.g., "Typhoon Yagi" vs. "Storm No. 3"); and (3) \textbf{Lack of Verification:} Viral rumors spread faster than factual corrections.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/dashboard.png}
    \caption{System Dashboard: Real-time visualization of trending events and their sentiment.} \label{fig:dashboard}
\end{figure}

As shown in Fig. \ref{fig:dashboard}, the system dashboard integrates three key views: (1) A real-time ranked list of trending events; (2) A time-series chart tracking sentiment evolution; and (3) A geospatial map highlighting event hotspots. This interface allows stakeholders to quickly grasp the "What, Where, and How" of emerging issues.

\noindent\textbf{Research Gap.}
Existing event detection pipelines often optimize for either (i) low-latency streaming detection using lightweight statistical or embedding-based clustering, or (ii) high semantic coherence and interpretability using heavier neural/LLM reasoning.
However, practical deployments require \emph{all three} simultaneously: \textbf{(a) real-time latency}, \textbf{(b) coherent event grouping under noisy short texts}, and \textbf{(c) scalable evaluation} when ground-truth labels are scarce in streams.
This creates a gap between \emph{system-level real-time constraints} and \emph{semantic/evaluation rigor}.

\noindent\textbf{Research Questions.}
We study the following questions:
\begin{itemize}
    \item \textbf{RQ1:} How can we detect emerging events from noisy social streams under strict latency constraints ($<10$s) while minimizing fragmentation and noise?
    \item \textbf{RQ2:} How can we improve semantic coherence and event interpretability without blocking the streaming pipeline?
    \item \textbf{RQ3:} How can we evaluate clustering quality at scale when full human labels are unavailable for streaming data?
\end{itemize}

To answer these questions, we propose a \textbf{Dual-Path Architecture} that separates low-latency detection from semantic refinement.
In the Fast Path, we perform streaming attachment and discovery via a \textbf{Social-Aware Hierarchical Clustering (SAHC)} strategy to reduce noise and over-fragmentation.
In the Slow Path, asynchronous LLM workers generate structured 5W1H summaries and support semantic quality control.
To close the evaluation gap, we combine a \textbf{Mini-Ground Truth} protocol (manual labels on a small subset) with an \textbf{LLM-as-a-Judge} procedure to assess semantic coherence at scale.

Our specific contributions include:
\begin{enumerate}
    \item A \textbf{Dual-Path Architecture} balancing < 10s latency for detection and deep LLM reasoning for insight.
    \item A \textbf{Social-Aware Hierarchical Clustering (SAHC)} algorithm using news anchors to reduce noise by ~50\% compared to vanilla HDBSCAN under the same embedding \& preprocessing.
    \item A rigorous evaluation using both a \textbf{Mini-Ground Truth} dataset (N=300) and \textbf{LLM-as-a-Judge} metrics.
\end{enumerate}

\section{Related Work}
Event detection from social media has evolved significantly over the past decade, progressing from statistical methods to neural approaches and, most recently, to Large Language Model (LLM)-augmented systems.
We organize the related literature into four categories: (1) traditional statistical methods, (2) neural and embedding-based approaches, (3) LLM and Retrieval-Augmented Generation (RAG) methods, and (4) real-time streaming architectures.
Table \ref{tab:related_work} provides a comparative summary of representative approaches.

\subsection{Traditional Statistical Methods}
Early event detection systems relied heavily on term-frequency analysis and topic modeling.
\textbf{TF-IDF and BM25} \cite{ref_bm25} approaches identify bursty keywords that deviate from normal frequency patterns, enabling rapid detection of trending topics.
\textbf{Latent Dirichlet Allocation (LDA)} \cite{ref_lda} and its online variants model documents as mixtures of latent topics, allowing unsupervised discovery of event themes.
\textbf{EDCoW} \cite{ref_edcow} introduced wavelet-based signal processing to detect ``bursty'' keywords in Twitter streams, using co-occurrence graphs to cluster related terms into events.

\textbf{Strengths:} These methods are computationally efficient and interpretable, making them suitable for high-throughput scenarios.
\textbf{Limitations:} They struggle with short, noisy social texts where word co-occurrence is sparse, and they cannot capture semantic similarity between lexically different expressions (e.g., ``Typhoon Yagi'' vs. ``Storm No. 3'').

\subsection{Neural and Embedding-Based Approaches}
The advent of deep learning enabled semantic representation of text, significantly improving event detection quality.
\textbf{BERT} \cite{ref_bert} and its variants provide contextualized word embeddings that capture nuanced meaning.
\textbf{Sentence-BERT (SBERT)} \cite{ref_sbert} adapts BERT for efficient sentence-level similarity computation using Siamese networks, enabling fast nearest-neighbor search in embedding space.
\textbf{BERTopic} \cite{ref_bertopic} combines transformer embeddings with clustering (HDBSCAN) and class-based TF-IDF to produce coherent topic representations.

Clustering algorithms form the backbone of many neural event detection systems.
\textbf{K-Means} \cite{ref_kmeans} partitions data into $k$ spherical clusters but requires a predefined cluster count and cannot handle noise.
\textbf{DBSCAN} \cite{ref_dbscan} identifies arbitrary-shaped clusters based on density and explicitly labels noise points, but uses a global density threshold that fails when cluster densities vary.
\textbf{HDBSCAN} \cite{ref_hdbscan} extends DBSCAN with hierarchical density estimation, automatically identifying clusters of varying densities while isolating noise---making it particularly suitable for social media data where event clusters exhibit diverse engagement levels.

\textbf{Strengths:} Embedding-based methods capture semantic relationships and handle vocabulary variation effectively.
\textbf{Limitations:} High-dimensional embeddings incur significant computational overhead, and real-time inference remains challenging at scale. Additionally, these methods often require careful hyperparameter tuning (e.g., cluster thresholds) for each domain.

\subsection{LLM and Retrieval-Augmented Generation}
Large Language Models have recently transformed NLP tasks through emergent reasoning capabilities \cite{ref_llm_reasoning}.
\textbf{GPT-4, Gemini, and Claude} demonstrate strong zero-shot performance on classification, summarization, and information extraction tasks without task-specific fine-tuning.
However, LLMs suffer from ``hallucinations''---generating plausible but factually incorrect content---especially when operating without grounding context.

\textbf{Retrieval-Augmented Generation (RAG)} \cite{ref_rag} addresses this limitation by conditioning LLM responses on retrieved relevant documents.
In the context of event detection, RAG enables grounding LLM summarization in verified social data points, improving factual accuracy.
Several recent works have explored LLM-based event extraction and classification, though most focus on offline batch processing rather than real-time streaming.

\textbf{Strengths:} LLMs provide superior semantic understanding and can generate interpretable event summaries with structured 5W1H (Who, What, When, Where, Why, How) extraction.
\textbf{Limitations:} LLM inference is computationally expensive (typically 100-1000$\times$ slower than embedding-based classification), making direct integration into low-latency streaming pipelines impractical.

\subsection{Real-Time Streaming Architectures}
Real-time event detection requires specialized architectures that balance throughput, latency, and accuracy.
\textbf{Sakaki et al.} \cite{ref_twitter_earthquake} pioneered real-time earthquake detection from Twitter using Kalman filtering and particle filters to track event propagation.
\textbf{TwitterNews+} \cite{ref_streaming_survey} proposed a framework combining incremental clustering with named entity recognition for news event detection from Twitter streams.

Modern streaming systems typically employ the \textbf{Lambda Architecture} (batch + stream layers) or the simpler \textbf{Kappa Architecture} (stream-only), using message brokers like Apache Kafka for fault-tolerant ingestion and stream processors like Spark Streaming or Flink for real-time computation.

\textbf{Strengths:} These architectures achieve sub-second latency and horizontal scalability.
\textbf{Limitations:} Most streaming approaches sacrifice semantic depth for speed, using lightweight features (keywords, hashtags) rather than full semantic understanding.

\subsection{Research Gap and Our Contribution}
As summarized in Table \ref{tab:related_work}, existing approaches typically optimize for \emph{either} low-latency streaming detection \emph{or} high semantic coherence with LLM reasoning---but practical deployments require \textbf{both} simultaneously.
Our \textbf{Dual-Path Architecture} addresses this gap by decoupling fast clustering (SAHC on the Fast Path) from deep semantic enrichment (LLM on the Slow Path), achieving real-time latency while maintaining event interpretability.
Furthermore, we introduce the \textbf{LLM-as-a-Judge} protocol to address the evaluation gap when ground-truth labels are scarce in streaming scenarios.

\begin{table}
    \caption{Comparison of Event Detection Approaches}\label{tab:related_work}
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{l c c c c c}
            \toprule
            \textbf{Approach}                        & \textbf{Semantic}      & \textbf{Real-time}  & \textbf{Noise}      & \textbf{Interpretable} & \textbf{Scalable}   \\
                                                     & \textbf{Understanding} & \textbf{Capable}    & \textbf{Handling}   & \textbf{Output}        &                     \\
            \midrule
            TF-IDF / BM25 \cite{ref_bm25}            & Low                    & \checkmark          & Limited             & --                     & \checkmark          \\
            LDA \cite{ref_lda}                       & Medium                 & --                  & Limited             & \checkmark             & \checkmark          \\
            EDCoW \cite{ref_edcow}                   & Low                    & \checkmark          & Limited             & --                     & \checkmark          \\
            BERT + Clustering \cite{ref_bert}        & High                   & --                  & Medium              & --                     & --                  \\
            BERTopic \cite{ref_bertopic}             & High                   & --                  & \checkmark          & \checkmark             & --                  \\
            TwitterNews+ \cite{ref_streaming_survey} & Medium                 & \checkmark          & Medium              & --                     & \checkmark          \\
            \midrule
            \textbf{Ours (SAHC + LLM)}               & \textbf{High}          & \textbf{\checkmark} & \textbf{\checkmark} & \textbf{\checkmark}    & \textbf{\checkmark} \\
            \bottomrule
        \end{tabular}%
    }
\end{table}

\section{Preliminaries}

\subsection{Large Language Models and Retrieval Augmented Generation}
Large Language Models (LLMs) such as GPT-4 and Gemini have revolutionized Natural Language Processing (NLP) by demonstrating emergent reasoning capabilities. Built upon the Transformer architecture \cite{ref_bert}, these models utilize self-attention mechanisms to capture long-range dependencies in textual data.
\begin{equation}
    Attention(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}
Despite their prowess, LLMs suffer from "hallucinations"—generating plausible but factually incorrect assertions. To mitigate this in domain-specific applications, Retrieval-Augmented Generation (RAG) is employed. RAG enhances the generative process by conditioning the LLM on retrieved relevant documents $D = \{d_1, ..., d_k\}$ before generating a response $y$:
\begin{equation}
    P(y|x) = \sum_{z \in D} P(z|x)P(y|x,z)
\end{equation}
In our system, we adapt this paradigm by treating "Clusters" as the retrieved context $z$, grounding the LLM's summarization in verified social data points.

\subsection{Real-time Streaming Architectures}
Modern event detection requires handling data with high velocity and volume. The \textbf{Kappa Architecture} simplifies the traditional Lambda Architecture by treating everything as a stream, utilizing a robust log-based message broker like Apache Kafka.
Kafka partitions data across distributed brokers, ensuring fault tolerance and high throughput. Consumers read data from these partitions using offsets $O$, maintaining exactly-once processing guarantees:
\begin{equation}
    \text{Lag}(t) = \text{Offset}_{produce}(t) - \text{Offset}_{consume}(t)
\end{equation}
For processing, \textbf{Spark Structured Streaming} treats live data streams as an unbounded input table. It processes textual data in micro-batches (e.g., 200ms intervals), enabling the application of batch-like operations (such as aggregations and joins) on streaming data with minimal latency overhead.

\subsection{Density-Based Clustering (HDBSCAN)}
Unlike centroid-based methods (K-Means) which assume spherical clusters, Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) \cite{ref_hdbscan} identifies clusters of varying densities and shapes.
HDBSCAN relies on a transformed distance metric to separate sparse noise from dense clusters. The \textbf{Core Distance} of a point $p$, denoted as $d_{core}(p)$, is defined as the distance to its $k$-th nearest neighbor. To ensure potential clusters are robust, the \textbf{Mutual Reachability Distance} between points $p$ and $q$ is formalized as:
\begin{equation}
    d_{mreach}(p, q) = \max \{ d_{core}(p), d_{core}(q), d(p, q) \}
\end{equation}
This metric effectively "pushes" sparse points away from each other. A Minimum Spanning Tree (MST) is then constructed using $d_{mreach}$ as edge weights. By iteratively removing edges with the highest weight, HDBSCAN builds a hierarchy of connected components, extracting stable clusters that persist over a wide range of density thresholds. This property is crucial for social media data, where event clusters ("Viral Trends") typically exhibit much higher density than background noise ("Daily Chatter").

\subsection{Event Taxonomy and Categorization}
To ensure actionable insights, we move beyond binary sentiment to a usage-based taxonomy (T1-T7), categorizing events by their value to specific stakeholders:

\begin{itemize}
    \item \textbf{T1. Crisis \& Public Risk} (Target: Emergency Services)\\
          \emph{Core Question: Is immediate intervention required?} Covers accidents, disasters, and riots.

    \item \textbf{T2. Policy Signal} (Target: Government)\\
          \emph{Core Question: How is the public reacting to new policies?} Covers new laws and official statements.

    \item \textbf{T3. Reputation Risk} (Target: PR Agencies)\\
          \emph{Core Question: Is public trust being damaged?} Covers scandals, boycotts, and controversies.

    \item \textbf{T4. Market Opportunity} (Target: Marketing Teams)\\
          \emph{Core Question: Is there monetizable demand?} Covers viral products and emerging lifestyle trends.

    \item \textbf{T5. Cultural Trend} (Target: Content Creators)\\
          \emph{Core Question: Is this trend worth riding for attention?} Covers memes and entertainment events.

    \item \textbf{T6. Operational Pain Point} (Target: Service Operators)\\
          \emph{Core Question: What are people complaining about?} Covers traffic, outages, and service failures.

    \item \textbf{T7. Routine/Noise} (Target: System Filter)\\
          \emph{Core Question: Should this be filtered?} Covers daily weather, routine sports, and lottery results.
\end{itemize}

\section{Proposed System Architecture}
The system follows a \textbf{Kappa-style streaming architecture} separated into two processing paths to address the "Latency vs. Accuracy" trade-off.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/full_pipeline.png}
    \caption{Proposed System Architecture: Dual-Path Processing (Fast Path vs. Slow Path).} \label{fig:pipeline}
\end{figure}

Fig. \ref{fig:pipeline} illustrates the end-to-end data flow. Raw data is ingested via Kafka, processed in micro-batches by Spark (Fast Path), and asynchronously enriched by Gemini Pro (Slow Path) before being served to the UI. This separation ensures that the heavy inference time of LLMs does not block the high-throughput ingestion pipeline.

\subsection{System Workflow}
The lifecycle of a social post in our system follows five stages:
\begin{enumerate}
    \item \textbf{Ingestion Layer:} Python crawlers, orchestrated by \textbf{Airflow}, continuously collect raw text from 40+ high-traffic Fanpages. Data is normalized to JSON format and pushed to the \textbf{Apache Kafka} topic \texttt{posts\_stream\_v1}, with \textbf{Zookeeper} handling cluster coordination.
    \item \textbf{Filtering (Spark Streaming):} The \textbf{Spark} engine consumes the Kafka stream in micro-batches. A Heuristic Guard drops routine noise, while a \textbf{Smart Query Constructor} expands potential trend keywords (e.g., "Sea Games" $\to$ "Sea Games 33 Schedule") to increase recall.
<<<<<<< HEAD
    \item \textbf{Clustering (Fast Path):} Valid social posts are vectorized using ONNX models and \textbf{matched to Anchor vectors} (News/Trends) using a \textbf{latency-optimized hybrid score} (dense-sparse fusion of cosine similarity and normalized BM25). 
    We support a Cross-Encoder reranking module at the matching stage; however, to satisfy real-time constraints, the \textbf{online deployment uses a lightweight verification strategy} that only re-scores the winning candidate (Top-1) as a \textbf{verification gate}. 
    In offline analysis or configurable modes, the system can be extended to rerank Top-$K$ candidates for higher precision.
    Posts that are rejected by the verification gate or fail the threshold are buffered as \textit{residual} for the \textbf{Discovery} step (HDBSCAN).
=======
    \item \textbf{Clustering (Fast Path):} Valid social posts are vectorized using ONNX models and \textbf{matched to Anchor vectors} (News/Trends) using a \textbf{latency-optimized hybrid score} (cosine similarity with a heuristic keyword boost).
          We support a Cross-Encoder reranking module at the matching stage; however, to satisfy real-time constraints, the \textbf{online deployment uses a lightweight verification strategy} that only re-scores the winning candidate (Top-1) as a \textbf{verification gate}.
          In offline analysis or configurable modes, the system can be extended to rerank Top-$K$ candidates for higher precision.
          Posts that are rejected by the verification gate or fail the threshold are buffered as \textit{residual} for the \textbf{Discovery} step (HDBSCAN).
>>>>>>> refs/remotes/origin/main

    \item \textbf{Intelligence (Slow Path):} Asynchronous \textbf{Python Workers} poll confirmed clusters from the \textbf{PostgreSQL} unified store. A cluster is considered \emph{significant} and eligible for LLM enrichment only if it reaches the impact threshold in terms of \textbf{distinct users} ($U(C)\ge \delta_{significant}$). Workers then query \textbf{Gemini Pro} to extract structured 5W1H summaries and deduplicate overlapping topics (see Appendix Table \ref{tab:prompt_refinement}).
    \item \textbf{Visualization:} The Dashboard polls the PostgreSQL database to display verified, enriched events in real-time.
\end{enumerate}

\noindent
\begin{center}
    \setlength{\fboxsep}{10pt}
    \fcolorbox{black}{gray!10}{%
        \begin{minipage}{0.95\textwidth}
            \textbf{\large End-to-End Processing Trace: "Typhoon Yagi Alert"}
            \vspace{0.2cm}
            \par
            To illustrate the pipeline, consider a single social post lifecycle:
            \begin{enumerate}
                \item \textbf{Input:} Crawler fetches raw text "Bão số 3 giật cấp 12..." from fanpage \textit{ThongTinChinhPhu}.
                \item \textbf{Ingestion:} Kafka producer serializes it to JSON and pushes to topic \texttt{posts\_stream\_v1} (Partition 0).
                \item \textbf{Filtration:} Spark Streaming consumes the message. The Heuristic Guard permits it, and Smart Query expands context to "Typhoon Yagi tracking".
                \item \textbf{Clustering:} The Onnx Worker maps it to vector $\mathbf{v}$. SAHC identifies the best-matching News Anchor "Typhoon Yagi" (Hybrid Score $> \lambda_{match}$) and attaches the post.
                \item \textbf{Intelligence:} The cluster grows beyond the significance threshold. An async worker triggers Gemini Pro, which returns a structured summary: "Typhoon Yagi approaching Quang Ninh."
                \item \textbf{Output:} The Dashboard receives the specific update and displays a "Public Risk" alert on the live map.
            \end{enumerate}
        \end{minipage}%
    }
\end{center}

\subsection{Fast Path vs. Slow Path Implementation}
\begin{itemize}
    \item \textbf{Ingestion (Kafka \& Zookeeper):}
          \begin{itemize}
              \item \textbf{Producer:} Serializes raw posts into JSON and publishes to the \texttt{posts\_stream\_v1} topic, handling retries and exponential backoff to ensure at-least-once delivery.
              \item \textbf{Broker:} Kafka acts as the distributed, fault-tolerant log storage, while \textbf{ZooKeeper} manages broker metadata, leader election, and partition assignment.
          \end{itemize}

    \item \textbf{Processing (Spark Structured Streaming):}
          \begin{itemize}
              \item Consumes events from Kafka with \textbf{exactly-once semantics} using checkpointing.
              \item Parses raw JSON payloads into Spark DataFrames for efficient columnar processing.
          \end{itemize}

    \item \textbf{Inference (Pandas UDF \& ONNX):}
          \begin{itemize}
              \item Uses PySpark’s \textbf{Pandas UDF} interface to vectorize text in batches, avoiding row-by-row Python serialization overhead.
              \item Invokes the \textbf{ONNX Runtime} engine on Worker nodes to compute embeddings (SBERT) for each micro-batch; \textbf{HDBSCAN} is triggered only on the \textit{unmatched/residual} set to discover emerging \textbf{Social-only} events.
          \end{itemize}

    \item \textbf{Slow Path Intelligence (Async Workers):}
          \begin{itemize}
              \item Designed to decouple heavy operations from the ingestion stream. Workers poll confirmed clusters and:
                    \begin{enumerate}
                        \item Query \textbf{Google Trends API} to fetch search volume (G-score), using the filter prompt in Appendix Table \ref{tab:prompt_trends}. This signal is treated as \textbf{confirmatory/lagging}: it typically accumulates after an event is already visible on social streams, and is therefore used to validate and prioritize clusters rather than to trigger initial detection.
                        \item Employ a \textbf{Context-Aware Prompting} strategy with Gemini Pro to synthesize coherent summaries.
                    \end{enumerate}
          \end{itemize}
\end{itemize}

\subsection{Latency Definition}
We distinguish between two types of latency in our system:
\begin{itemize}
    \item \textbf{Fast-Path Processing Latency:} measured from Kafka ingestion to cluster assignment within a Spark micro-batch.
    \item \textbf{End-to-End Alert Latency:} measured from data ingestion to dashboard update.
\end{itemize}

In our experimental setup, the Fast Path achieves an average processing latency of \textbf{2.4 seconds}, while the end-to-end alert latency remains \textbf{below 10 seconds}, as the Slow Path LLM enrichment is executed asynchronously and does not block real-time event detection.

\section{Methodology}

\subsection{Formal Problem Definition}
Let $\mathcal{S} = \{p_1, p_2, ..., p_n\}$ be a continuous stream of social media posts, where each post $p_t$ at time $t$ is a tuple $(u_t, \tau_t, \textbf{v}_t)$, representing the user, timestamp, and semantic embedding vector respectively.
The goal of real-time event detection is to map this stream into a set of disjoint clusters $\mathcal{C} = \{C_1, C_2, ..., C_k\}$, where each cluster $C_i$ represents a real-world event.
A valid event cluster $C_i$ must satisfy two conditions:
\begin{enumerate}
    \item \textbf{Cohesion:} $\forall p_a, p_b \in C_i, \text{sim}(\textbf{v}_a, \textbf{v}_b) \geq \theta_{sim}$
    \item \textbf{Impact:} $U(C_i) \geq \delta_{min}$, where $U(C_i)$ is the number of \emph{distinct users} discussing the event:
          \begin{equation}
              U(C_i) = \left|\left\{u_t \;\middle|\; p_t \in C_i\right\}\right|.
          \end{equation}

\end{enumerate}

In practice, we use $\delta_{significant}$ as the operational impact threshold in the streaming system (typically $\delta_{significant}\ge \delta_{min}$) to decide when a cluster is promoted for enrichment and visualization.

\subsection{Data Preprocessing and Normalization}

To ensure high-quality input for the clustering engine, raw data from both paths undergoes specific preprocessing steps:

\begin{itemize}
    \item \textbf{Fast Path (Social Speed):} Given the high-throughput nature of social streams, we apply minimal, latency-optimized cleaning:
          \begin{itemize}
              \item \textbf{Heuristic Filtering:} We discard posts with length $<20$ characters (insufficient semantic context for embedding) or $>800$ characters (high likelihood of marketing spam or irrelevant boilerplate), and remove known "credit" patterns (e.g., "cre:", "via:") which do not contribute to event identity.
              \item \textbf{Unicode Normalization:} Converting all text to NFC format and lowercasing to handle Vietnamese diacritics consistently.
          \end{itemize}

    \item \textbf{Slow Path (News Precision):} News data, used as high-confidence anchors, undergoes deep structural parsing:
          \begin{itemize}
              \item \textbf{Structural Extraction:} We use Regex to separate metadata (Author, Date, Location) from the body content, ensuring only the core narrative affects the centroid.
              \item \textbf{Alias Injection (Thesaurus):} To bridge the vocabulary gap between formal news and informal social posts, we inject known aliases into news vectors (e.g., mapping "Typhoon No. 3" $\to$ "Typhoon Yagi").
          \end{itemize}
\end{itemize}

\subsection{Social-Aware Hierarchical Clustering (SAHC)}

Standard clustering (K-Means, DBSCAN) often fails on social data due to noise. SAHC operates in three phases:
\begin{enumerate}
    \item \textbf{Phase 1: News Anchoring.} We first cluster verified News articles to form "Anchor Centroids". These represent high-confidence events.
    \item \textbf{Phase 2: Social Attachment.} Incoming social posts are mapped to these anchors using Cosine Similarity.
    \item \textbf{Phase 3: Social Discovery.} Posts that do not match any anchor undergo a second pass of density-based clustering (HDBSCAN) to detect "Social-Only" trends (unreported by news).
\end{enumerate}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/clustering_pipeline.png}
    \caption{Social-Aware Hierarchical Clustering (SAHC) Workflow.} \label{fig:clustering}
\end{figure}

The SAHC process is visualized in Fig. \ref{fig:clustering}. News articles first establish "Anchor" centroids (Blue nodes). Incoming Social posts (Green nodes) are then attracted to these anchors based on semantic similarity. Residual noise is finally filtered or clustered into new events by HDBSCAN, effectively reducing spam.

\begin{algorithm}
    \caption{SAHC-A: Anchor-First Dual-Path Event Detection}
    \begin{algorithmic}[1]
        \Require Incoming micro-batch $B$, Active Anchors $\mathcal{A}$, Global Residual Buffer $\mathcal{G}$
        \Ensure Updated Events $\mathcal{E}$, Updated Buffer $\mathcal{G}$

        \State $\mathcal{R}_{batch} \leftarrow \emptyset$ \Comment{Unmatched posts in this batch}

        \State \textbf{// Path 1: Anchor Matching (Fast Path)}
        \For{each post $p \in B$}
            \State $\mathbf{v}_p \leftarrow \text{Encoder}(p_{text})$
            \State $a^* \leftarrow \arg\max_{a \in \mathcal{A}} \cos(\mathbf{v}_p,\mathbf{v}_a)$ \Comment{Fast retrieval}
            \State $dense\_sim \leftarrow \cos(\mathbf{v}_p,\mathbf{v}_{a^*})$
            \State $raw\_bm25 \leftarrow \text{BM25}(p,a^*)$
            \State $sparse\_score \leftarrow raw\_bm25 \; / \; \max_{a' \in \mathcal{A}} \text{BM25}(p,a')$ \Comment{Max-normalization}
            \State $score \leftarrow 0.7 \cdot dense\_sim + 0.3 \cdot sparse\_score$ \Comment{Linear Fusion}
            
            \If{$score \ge \lambda_{match}$}
                \State Add $p$ to Event $E_{a^*}$
                \State Update Centroid: $\mathbf{v}_{a^*} \leftarrow (1-\alpha)\mathbf{v}_{a^*} + \alpha\mathbf{v}_p$ \Comment{Moving Average}
            \Else
                \State $\mathcal{R}_{batch} \leftarrow \mathcal{R}_{batch} \cup \{p\}$
            \EndIf
        \EndFor

        \State \textbf{// Path 2: Social Discovery (Slow Path)}
        \State $\mathcal{G} \leftarrow \mathcal{G} \cup \mathcal{R}_{batch}$ \Comment{Merge with global buffer}
        \If{$|\mathcal{G}| \ge N_{min\_clustering}$}
        \State $\mathcal{C}_{new} \leftarrow \text{HDBSCAN}(\mathcal{G}, \min_{pts}=5, \epsilon=0.3)$
        \For{each cluster $c \in \mathcal{C}_{new}$}
        \State $U(c) \leftarrow \left|\{u(p)\;|\; p \in c\}\right|$ \Comment{Distinct user count}
        \If{$U(c) \ge \delta_{significant}$}
        \State Initialize new Event $E_{new}$ from $c$
        \State Remove points in $c$ from $\mathcal{G}$
        \EndIf
        \EndFor

        \State Prune old points from $\mathcal{G}$ \Comment{Time-decay cleanup}
        \EndIf

    \end{algorithmic}
\end{algorithm}

\textbf{Latency-Optimized Hybrid Matching:}
To balance semantic understanding and exact entity matching under real-time constraints, we employ a lightweight dense--sparse fusion score that combines cosine similarity and BM25.
Additionally, we integrate an optional Cross-Encoder module: in the current online configuration it acts as a \textbf{verification/recovery gate} by re-scoring only the top candidate when needed, rather than performing full Top-$K$ reranking. Concretely, it (i) rejects a top-1 attachment if the cross-encoder score falls below a threshold, and (ii) can ``save'' borderline matches after title refinement by re-checking the refined pair. This preserves low latency while reducing false attachments. 
\begin{equation}
    \label{eq:hybrid_real}
    \text{Score}(p,a) = w_d \cdot \cos(\mathbf{v}_p,\mathbf{v}_a) + w_s \cdot \frac{\text{BM25}(p,a)}{\max_{a' \in \mathcal{A}} \text{BM25}(p,a')},
\end{equation}
where $w_d=0.7, w_s=0.3$ are the default fusion weights in our implementation. The sparse BM25 score is \textbf{max-normalized} across the candidate set $\mathcal{A}$ to ensure scale consistency with the cosine similarity component (after cosine rescaling/clipping in implementation).

\subsection{Trend Scoring Logic}
To determine which events are "Trending", we calculate a Unified Trend Score ($T$) for each cluster based on three weighted signals:
\begin{equation}
    T = 0.4 \cdot G + 0.35 \cdot N + 0.25 \cdot F
\end{equation}
In our setting, social streams provide the earliest burst signals, while Google Trends typically peaks later. Therefore, $G$ is used primarily to confirm and prioritize candidate clusters already detected on the Fast Path.

Where the components represent:
\begin{itemize}
    \item \textbf{G (Google Search Volume):} The normalized search interest score from Google Trends, serving as a \textbf{confirmatory (lagging) signal} of public attention.
    \item \textbf{N (News Volume):} The count of distinct mainstream news clusters linked to this event.
    \item \textbf{F (Facebook Engagement):} The weighted sum of interactions ($Likes + 2 \cdot Comments + 3 \cdot Shares$).
\end{itemize}
To handle the power-law distribution of social data, each component is normalized using a \textbf{Log-Min-Max} scale before weighting:
\begin{equation}
    S_{component} = 100 \cdot \frac{\log_{10}(1 + v)}{\log_{10}(1 + v_{max})}
\end{equation}
Where $v$ is the raw value and $v_{max}$ is an empirical ceiling ($G_{max}=10^6$, $N_{max}=10$, $F_{max}=2\cdot10^4$). Specifically for Facebook ($F$), interactions are weighted: $v_F = Likes + 2 \cdot Comments + 3 \cdot Shares$. This ensures that high-engagement events are categorized correctly even with low volume. A cluster is promoted to the dashboard only if $T > Threshold$.

\section{Data and Experimental Setup}

\subsection{Data Sources}
Our system monitors two primary data streams, unified into a common schema for processing:
\begin{itemize}
    \item \textbf{Social Media (Facebook):} Collected from high-engagement Fanpages (e.g., Theanh28, ThongTinChinhPhu). Data fields include: \texttt{pageName}, \texttt{postId}, \texttt{time} (ISO-8601), \texttt{text} content, and engagement metrics (\texttt{likes}, \texttt{comments}, \texttt{shares}).
    \item \textbf{Mainstream News:} Articles crawled from verified outlets (VnExpress, Tuoi Tre). Key fields: \texttt{ArticleID}, \texttt{URL}, \texttt{Title}, \texttt{Content}, and \texttt{PublishTime}.
    \item \textbf{Google Trends:} Real-time search keywords acting as a \textbf{confirmatory/lagging signal} to validate and prioritize candidate clusters detected from social streams.
\end{itemize}

The experimental dataset consists of \textbf{7,605 unique items} collected from Dec 8 to Dec 22, 2025, including \textbf{4,644 crawled news articles} and \textbf{2,961 social media posts}.
After content extraction, \textbf{4,603 news articles} contain valid body text and are used for anchoring and embedding. Here, \textbf{2,961} denotes the \emph{unique social posts used in experiments} after deduplication and basic validity checks (raw crawled volume is reported as an approximation).

\subsection{Exploratory Data Analysis}
Table \ref{tab:data_dist} presents the distribution of our experimental dataset. We observed a significant "Availability Gap": while we crawled over 9,400 news URLs, only \textbf{4,603 articles (48.6\%)} contained extractable body text, whereas \textbf{social media posts (2,961 items)} achieved \emph{near-complete} availability.
Furthermore, there is a structural divergence: News articles average $>500$ words with formal grammar, while social posts average $<50$ words with frequent slang. This dichotomy necessitates our "Dual-Path" design, where the \textbf{Fast Path} handles the high-velocity, short-text social stream, and the \textbf{Slow Path} leverages LLMs to synthesize the deeper context found in news anchors.

\begin{table}
    \caption{Dataset Distribution and Availability}\label{tab:data_dist}
    \centering
    \begin{tabular}{l c c c}
        \toprule
        \textbf{Source Type} & \textbf{Crawled URLs} & \textbf{Valid Content}   & \textbf{Avg Length (words)} \\
        \midrule
        Mainstream News      & $\approx 9,400$       & 4,603 (48.6\%)           & $> 500$                     \\
        Social Media         & $\approx 3,000$       & 2,961 ($\approx 98.7\%$) & $< 50$                      \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Deployment and Operational Environment}
To ensure reproducibility and scalability, the system is containerized using \textbf{Docker Compose} with the following core services:
\begin{itemize}
    \item \textbf{Software Stack:} Apache Kafka 7.5.0, Apache Spark 3.5.0, PostgreSQL 15, MongoDB 6.0.
    \item \textbf{Hardware:} Optimized for single-node deployment (8GB RAM, 4 vCPUs) or Kubernetes clustering.
    \item \textbf{Inference:} ONNX Runtime acceleration on NVIDIA T4 GPUs.
\end{itemize}

\subsubsection{Deployment Benefits}
The containerized microservices architecture offers three key advantages:
\begin{enumerate}
    \item \textbf{Scalability:} The decoupling of Ingestion (Kafka) and Processing (Spark/LLM) allows independent scaling. During high-traffic events (e.g., storms), we can horizontally add Spark Workers without reconfiguring the ingestion layer.
    \item \textbf{Cost Efficiency:} By offloading heavy batch clustering to local commodity hardware (CPU) and only invoking the paid LLM API for verified clusters (0.01\% of volume), we reduce operational costs by approx. 400x compared to pure LLM-based approaches.
    \item \textbf{Reproducibility:} The entire stack is defined in \texttt{docker-compose.yml}, ensuring that the "Demo" environment is identical to "Production", eliminating "it works on my machine" issues.
\end{enumerate}

\subsection{Dataset Construction Strategy}
To train our specialized models without incurring high labeling costs, we employed a \textbf{Hybrid Labeling Strategy}:
\begin{enumerate}
    \item \textbf{Self-Supervised Learning:} For the Cross-Encoder Reranker, we mined "Hard Negatives" (similar keywords but different semantics) to create 877 high-quality pairs.
          This reranker is used as an auxiliary matching component (verification / recovery) and as an offline benchmark module.
    \item \textbf{Active Learning with LLM:} For Sentiment and Taxonomy classification, we used Gemini Pro to pseudo-label roughly 10k samples, then manually reviewed low-confidence scores. This yielded \textbf{4,630 samples for Sentiment} and \textbf{3,687 samples for Taxonomy} training.
\end{enumerate}

\begin{figure}[h]
    \centering
    \begin{verbatim}
{"text": "Vụ xả súng tại bãi biển Bondi khiến 15 người thiệt mạng.", 
 "label_sentiment": "Neutral", "label_taxonomy": "T1 - Crisis & Public Risk"}
{"text": ["Bão số 15 Koto đang ở đâu?", "Áp thấp nhiệt đới mạnh lên thành bão."], 
 "label_relevance": 1.0}
\end{verbatim}
    \caption{JSONL Training Data Examples for Classification and Reranking.} \label{fig:jsonl}
\end{figure}

\subsection{Model Selection and Fine-tuning}
We fine-tuned specific models for Vietnamese to serve as the system's backbone. To ensure robustness, we constructed a \textbf{Stress Test Set} containing 20 hard samples per category, specifically targeting edge cases such as "Slang" (social media language), "Storm Synonyms" (ambiguous event naming), and "Domain Overlap". Table \ref{tab:training_config} details the specific training hyperparameters used, and Table \ref{tab:models} shows the final performance metrics on this rigorous test set.

\begin{table}
    \caption{Detailed Training Configuration and Hyperparameters}\label{tab:training_config}
    \centering
    \begin{tabular}{l l l l}
        \toprule
        \textbf{Model Target} & \textbf{Base Architecture} & \textbf{Dataset Size} & \textbf{Training Config}        \\
        \midrule
        Sentiment             & \texttt{uitnlp/visobert}   & 4,630 (3 classes)     & Epochs: 20, Batch: 32, LR: 2e-5 \\
        Taxonomy              & \texttt{uitnlp/visobert}   & 3,687 (7 classes)     & Epochs: 20, Batch: 16, LR: 2e-5 \\
        Reranker              & \texttt{ms-marco-MiniLM}   & 877 pairs (Gold)      & Contractive Loss, Epochs: 20    \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/embedding_benchmark.png}
    \caption{Benchmark results comparing Embedding Models across different edge cases.} \label{fig:benchmark}
\end{figure}



Fig. \ref{fig:benchmark} compares the stability (gap between positive/negative pairs) of 5 state-of-the-art models. We define the Stability Gap mathematically as:
\begin{equation}
    Gap = Sim(q, pos) - Sim(q, neg)
\end{equation}
\texttt{vietnamese-document-embedding} (blue bar) consistently maintains the largest margin across edge cases like "Storm Synonyms" and "Slang", validating its selection as our backbone model.

Results showed that \texttt{dangvantuan/vietnamese-document-embedding} achieved the best stability (Avg Gap 0.265). Table \ref{tab:embedding_benchmark} details the performance across specific linguistic challenges, defined as follows:

\begin{itemize}
    \item \textbf{Avg Gap:} The mean stability margin across all test cases.
    \item \textbf{Storm:} Ability to group diverse storm names (e.g., "Typhoon Yagi", "Storm No.3").
    \item \textbf{Domain:} Cross-domain retrieval (e.g., matching "Healthcare" query to "Hospital" posts).
    \item \textbf{Category:} Distinguishing between event types (e.g., "Flood" vs. "Traffic Jam").
    \item \textbf{Slang:} Handling Vietnamese teen-code and internet slang (e.g., "bão" vs "b4o").
    \item \textbf{Abbrev:} Resolving common abbreviations (e.g., "HCM" $\rightarrow$ "Ho Chi Minh City").
\end{itemize}

\textbf{Analysis:} While \texttt{vn-sbert} excels at keyword-heavy tasks like "Storm" grouping (Gap: 0.355), it fails completely on "Slang" (-0.005), indicating it treats slang as noise. In contrast, \texttt{vn-doc-embedding} maintains robust performance on "Slang" (0.283) and "Abbrev" (0.419), making it the superior choice for processing raw social media text where informal language is prevalent.

\begin{table}
    \caption{Embedding Stability Gap Analysis (Metric: Cosine Distance Diff)}\label{tab:embedding_benchmark}
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{l c c c c c c}
            \toprule
            \textbf{Model Identifier}                   & \textbf{Avg Gap} $\uparrow$ & \textbf{Storm} $\uparrow$ & \textbf{Domain} $\uparrow$ & \textbf{Category} $\uparrow$ & \textbf{Slang}  $\uparrow$ & \textbf{Abbrev}  $\uparrow$ \\
            \midrule
            \textbf{vn-doc-embedding*} \cite{ref_vndoc} & \textbf{0.265}              & 0.239                     & 0.224                      & \textbf{0.162}               & \textbf{0.283}             & \textbf{0.419}              \\
            vn-bi-encoder                               & 0.193                       & 0.155                     & 0.213                      & 0.107                        & 0.108                      & 0.381                       \\
            vn-sbert \cite{ref_visobert}                & 0.188                       & \textbf{0.355}            & 0.172                      & 0.033                        & -0.005                     & 0.386                       \\
            bge-m3 \cite{ref_bge}                       & 0.160                       & -0.003                    & \textbf{0.269}             & 0.070                        & 0.192                      & 0.273                       \\
            multilingual-e5 \cite{ref_e5}               & 0.058                       & 0.018                     & 0.088                      & 0.022                        & 0.073                      & 0.091                       \\
            \bottomrule
        \end{tabular}%
    }
\end{table}

\begin{table}
    \caption{Fine-tuned Model Performance}\label{tab:models}
    \centering
    \begin{tabular}{l l c c}
        \toprule
        \textbf{Model Task}      & \textbf{Class Type}     & \textbf{Accuracy} ($\uparrow$) & \textbf{Latency} ($\downarrow$) \\
        \midrule
        Sentiment Classifier     & 3 classes (Pos/Neg/Neu) & \textbf{93.5\%}                & 12ms                            \\
        Bi-CrossEncoder Reranker & Relevance Scoring       & 91.0\%                         & 45ms                            \\
        Taxonomy Classifier      & 7 classes (T1-T7)       & 89.2\%                         & 14ms                            \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Results and Analysis}

\subsection{Clustering Algorithm Selection}
We compared our SAHC approach (using HDBSCAN core) against baselines. HDBSCAN was chosen for its superior Noise handling and Balance.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/trend_tsne.png}
    \caption{t-SNE Visualization of Event Clusters. Colors represent distinct events isolated by the algorithm.} \label{fig:tsne}
\end{figure}

The t-SNE projection in Fig. \ref{fig:tsne} demonstrates the effectiveness of our clustering. Distinct events (colored clusters) are well-separated in the semantic space, while noise points (gray) are scattered and effectively isolated by the algorithm.

\begin{table}
    \caption{Clustering Method Comparison}\label{tab:clustering_select}
    \centering
    \begin{tabular}{l c c c c c}
        \toprule
        \textbf{Method}             & \textbf{Clusters (k)} & \textbf{Noise Pts} & \textbf{Silh. Score} ($\uparrow$) & \textbf{DB Index} ($\downarrow$) & \textbf{Time} ($\downarrow$) \\
        \midrule
        K-Means                     & 253                   & 0                  & 0.024                             & 3.177                            & 8.45s                        \\
        \textbf{HDBSCAN (Baseline)} & \textbf{460}          & 1,984              & 0.109                             & \textbf{2.094}                   & 16.00s                       \\
        BERTopic                    & 223                   & 1,801              & 0.112                             & 2.363                            & 55.72s                       \\
        \textbf{SAHC (Ours)}        & \textbf{315}          & \textbf{985}       & \textbf{0.135}                    & \textbf{1.951}                   & \textbf{18.4s}               \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Analysis:} As shown in Table \ref{tab:clustering_select}, \textbf{K-Means} proved unsuitable for social streaming data as it forces all noise into clusters. \textbf{Baseline HDBSCAN}, while correctly identifying noise, suffered from \textbf{over-fragmentation} (460 clusters) and excessive data rejection (1,984 noise points), often breaking single events into multiple micro-clusters.
Our \textbf{SAHC} approach resolves this by using News Anchors to "bridge" these micro-clusters. This successfully condensed the space into \textbf{315 coherent events} and recovered $\approx 50\%$ of the valid data previously discarded as noise (Noise reduced to 985), significantly boosting the Silhouette Score to \textbf{0.135}.

\subsection{Quantitative Evaluation (Mini-Ground Truth)}
All label-based clustering metrics (NMI, Purity, BCubed-F1, Entropy) are computed exclusively on the Mini-Ground Truth subset (N=300), as the full dataset does not contain human-annotated event labels.
Table \ref{tab:minigt} reports the performance of our \textbf{default SAHC setting}, while Table \ref{tab:ablation} further shows that the \textbf{full hybrid pipeline} (adding anchoring and LLM refinement) improves NMI from 0.54 to \textbf{0.5978}.

\begin{table}
    \caption{Performance on Mini-Ground Truth (N=300) -- Base SAHC Setting}\label{tab:minigt}
    \centering
    \begin{tabular}{l c l}
        \toprule
        \textbf{Metric}           & \textbf{Value} & \textbf{Interpretation}                   \\
        \midrule
        \textbf{NMI} ($\uparrow$) & \textbf{0.54}  & Good mutual information with human labels \\
        Purity ($\uparrow$)       & 0.65           & High dominant class consistency           \\
        BCubed-F1 ($\uparrow$)    & 0.35           & Balanced precision/recall for clustering  \\
        Entropy ($\downarrow$)    & 4.85           & Lower entropy indicates purer clusters    \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Metric Definitions:} To ensure a rigorous evaluation, we selected metrics that cover different aspects of clustering quality:
\begin{itemize}
    \item \textbf{Normalized Mutual Information (NMI):} Measures the alignment between cluster labels $Y$ and ground truth $C$:
          \begin{equation}
              NMI(Y, C) = \frac{2 \cdot I(Y; C)}{H(Y) + H(C)}
          \end{equation}
          Where $I$ is mutual information and $H$ is entropy.
    \item \textbf{Purity:} Quantifies the dominance of the majority class in each cluster:
          \begin{equation}
              Purity(\Omega, C) = \frac{1}{N} \sum_k \max_j |\omega_k \cap c_j|
          \end{equation}
          Where $N$ is total data points, $\Omega = \{\omega_1, ..., \omega_k\}$ is the set of clusters, and $C = \{c_1, ..., c_j\}$ is the set of ground truth classes.
    \item \textbf{Entropy:} Measures the homogeneity of a cluster (lower is better):
          \begin{equation}
              E(C) = - \sum_{k} \frac{n_k}{n} \log_2 \frac{n_k}{n}
          \end{equation}
          Where $n_k$ is the number of points in cluster $k$. A value close to 0 implies the cluster contains only a single class of events.
    \item \textbf{BCubed-F1:} An element-wise metric that balances cluster homogeneity and completeness. It is the harmonic mean of:
          \begin{itemize}
              \item \textit{BCubed Precision ($P_{b^3}$):} Average per-item precision. For each item $x$, it calculates the fraction of items in $x$'s cluster $C(x)$ that share the same class label $L(x)$:
                    \begin{equation}
                        P_{b^3} = \frac{1}{N} \sum_{x \in X} \frac{|C(x) \cap L(x)|}{|C(x)|}
                    \end{equation}
              \item \textit{BCubed Recall ($R_{b^3}$):} Average per-item recall. For each item $x$, it calculates the fraction of items in $x$'s class $L(x)$ that are grouped into the same cluster $C(x)$:
                    \begin{equation}
                        R_{b^3} = \frac{1}{N} \sum_{x \in X} \frac{|C(x) \cap L(x)|}{|L(x)|}
                    \end{equation}
          \end{itemize}
          \begin{equation}
              F1_{b^3} = 2 \cdot \frac{P_{b^3} \cdot R_{b^3}}{P_{b^3} + R_{b^3}}
          \end{equation}
          Unlike purity, BCubed penalizes both over-merging (low Precision) and over-fragmentation (low Recall).
    \item \textbf{Silhouette Score:} Quantifies how well an object fits its own cluster versus the nearest neighbor cluster:
          \begin{equation}
              S(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}
          \end{equation}
          Where $a(i)$ is the mean intra-cluster distance and $b(i)$ is the mean nearest-cluster distance. $S \in [-1, 1]$, where values closer to 1 indicate dense, well-separated clusters.
\end{itemize}

\subsection{Batch Evaluation and Component Analysis}
To quantify the contribution of each module, we conduct an ablation study on the \textbf{Mini-Ground Truth} subset (N=300), where label-based metrics (NMI, BCubed-F1, Entropy) are well-defined.
Table \ref{tab:ablation} shows that progressively adding anchoring and LLM refinement improves clustering agreement with human labels.

\begin{table}
    \caption{Ablation Study on Mini-Ground Truth (N=300)}\label{tab:ablation}
    \centering
    \begin{tabular}{l c c c}
        \toprule
        \textbf{Configuration}        & \textbf{NMI} ($\uparrow$) & \textbf{BCubed-F1} ($\uparrow$) & \textbf{Entropy} ($\downarrow$) \\
        \midrule
        Baseline (HDBSCAN only)       & 0.53                      & 0.31                            & 4.91                            \\
        \textbf{Full Hybrid Pipeline} & \textbf{0.5978}           & \textbf{0.3821}                 & \textbf{4.4178}                 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Qualitative Analysis: LLM-as-a-Judge}
Traditional metrics like Silhouette Score often fail to capture semantic coherence. To address this, we implemented an \textbf{LLM-as-a-Judge} protocol, where a strong instruction-tuned model (Gemini Pro) acts as a proxy for human evaluation.
\begin{itemize}
    \item \textbf{Protocol:} The LLM is provided with a cluster's content and asked two questions: (1) "Does this cluster describe a single, distinct event?" (Topic Consistency, rated 0-1), and (2) Given two conflicting clusters, "Which one is more coherent?" (Pairwise Win-Rate).
    \item \textbf{Prompting:} We employ a "Role-Play" strategy where the LLM acts as an expert news editor. The full prompt specification is provided in Appendix Table \ref{tab:prompt_eval}.
\end{itemize}
Using this framework, we evaluated 100 random clusters:
\begin{itemize}
    \item \textbf{Clear Events (e.g., "Middle East Conflict"):} Achieved \textbf{Consistency $\approx$ 1.0} and high Win-Rates.
    \item \textbf{Mixed/Noisy Clusters:} Correctly flagged by the LLM with significantly lower scores (0.64), validating the metric's utility for automated quality assurance.
\end{itemize}

\subsection{Detailed Case Studies}
To better understand the system's behavior, we analyze three specific cases from the test deployment:

\begin{enumerate}
    \item \textbf{Success Case: "Bondi Beach Shooting" (T1).}
          \begin{itemize}
              \item \emph{Input:} "{\fontencoding{T5}\selectfont Vụ xả súng tại bãi biển Bondi khiến 15 người thiệt mạng sau vụ tấn công bằng dao.}" (Bondi beach shooting kills 15 people after knife attack.)
              \item \emph{Analysis:} Despite concurrent news about "Australia Travel", the SAHC algorithm correctly isolated this event due to strong lexical overlap with "Crisis" keywords. The LLM assigned it to \textbf{T1 (Public Risk)} with high confidence (>0.9).
          \end{itemize}

    \item \textbf{Success Case: "Middle East Conflict" (T1).}
          \begin{itemize}
              \item \emph{Metrics:} This cluster achieved a Semantic Consistency score of \textbf{0.96} and a Win-Rate of \textbf{0.67} against noise clusters.
              \item \emph{Insight:} The high volume of verified news anchors allowed the system to absorb social media reactions effectively without "drifting" into unrelated topics.
          \end{itemize}

    \item \textbf{Failure Analysis: "Mixed International News".}
          \begin{itemize}
              \item \emph{Issue:} The algorithm merged "Vietnamese-origin lawsuit" with "Territorial conflict" into a single cluster.
              \item \emph{Root Cause:} This likely occurred due to shared diplomatic terminology (e.g., "sovereignty", "international law") acting as spurious vector anchors, bridging two semantically distinct but lexically similar topics.
              \item \emph{Detection:} Crucially, our \textbf{LLM-as-a-Judge protocol} flagged this error, assigning a low Consistency Score of \textbf{0.64} and a \textbf{0.00 Win-Rate}. This demonstrates the system's ability to automatically detect quality degradation for human review.
          \end{itemize}
\end{enumerate}

\section{Discussion}

\subsection{Challenges and Lessons Learned}
Deploying the system revealed three major technical hurdles:
\begin{enumerate}
    \item \textbf{Semantic Ambiguity:} Different communities use disparate terms for the same event (e.g., "Bão Yagi" vs. "Cơn bão số 3"). We addressed this using \textbf{LLM-based Deduplication} (Appendix Table \ref{tab:prompt_dedup}) and the Hybrid Matching score.
    \item \textbf{Evaluation Gap:} The lack of labeled ground truth in streaming data made validation difficult. We overcame this by establishing a \textbf{"Mini-Ground Truth"} protocol (manually labeling 300 samples) to standardize our NMI and F1 metrics.
    \item \textbf{Latency vs. Accuracy Trade-off:} Calling LLMs for every cluster introduced a 2-3s latency. We mitigated this by separating the pipeline into a \textbf{Fast Path} (Spark) for instant detection and a \textbf{Slow Path} (Async Workers) for deep analysis. Furthermore, we implemented an \textbf{Incremental Update Rule}: the LLM is only re-triggered if a cluster grows by $>20\%$ in volume, significantly reducing redundant inference costs.
\end{enumerate}


\subsection{Ablation Study and Component Analysis}
To quantify the contribution of each module, we conducted an ablation study by selectively disabling components (Table \ref{tab:ablation}).
\begin{itemize}
    \item \textbf{w/o News Anchors:} Removing anchoring noticeably degrades clustering agreement with human labels, confirming that social data alone is too noisy for coherent clustering.
    \item \textbf{w/o Heuristic Guard:} Disabling the initial regex filter increased the cluster count by 300\%, but most were "spam" (lottery, weather), causing a 60\% drop in F1-score.
    \item \textbf{w/o LLM Refinement:} Using raw centroids instead of LLM summaries resulted in vague event titles, reducing the interpretability score (human-eval) from 4.5 to 2.1.
\end{itemize}

\section{Conclusion and Future Work}
\subsection{Conclusion}
In this work, we presented a novel \textbf{Dual-Path Event Detection} framework that effectively reconciles the trade-off between real-time responsiveness and semantic depth. By decoupling latency-sensitive clustering (Fast Path) from computationally intensive LLM reasoning (Slow Path), our system achieves near-instantaneous event detection on the Fast Path (2.4s processing latency under our test setup) while maintaining strong clustering quality on the labeled Mini-Ground Truth subset through News Anchoring (base SAHC: NMI = 0.54, Purity = 0.65; full hybrid pipeline: best NMI = 0.5978).
On the full dataset, intrinsic metrics and LLM-as-a-Judge evaluation further confirm improved cluster coherence over baseline methods. We demonstrated that traditional density-based methods alone are insufficient for noisy social data, requiring a hybrid approach where confirmed news serves as a semantic stabilizer. Future work will focus on deploying quantized embedding models to edge devices and integrating multi-modal signals (images/video) to further enhance situational awareness for crisis management.

\subsection{Future Work}
To scale the system toward production deployment ($>10{,}000$ events per second), we plan the following extensions:
\begin{itemize}
    \item \textbf{Model Optimization:} Apply quantization (ONNX/INT8) and batching-aware inference to reduce embedding latency and cost while preserving clustering quality.
    \item \textbf{Advanced Affect Modeling:} Extend the current 3-class sentiment to fine-grained emotions (e.g., Anger, Fear, Hope) to better characterize crisis dynamics and public reactions.
    \item \textbf{Human Feedback Loop:} Add dashboard-level correction tools (merge/split clusters, relabel taxonomy) and feed validated edits back into Active Learning for continual improvement.
    \item \textbf{Production Scaling:} Migrate from Spark Local to a distributed deployment on \textbf{Kubernetes/YARN}, enabling independent horizontal scaling of Kafka, Spark executors, and LLM workers.
\end{itemize}

\section{Ethical Considerations and Limitations}
As an AI-integrated system analyzing public discourse, we follow ethical data practices and explicitly acknowledge key limitations.
\begin{itemize}
    \item \textbf{Privacy Protection:} Although the system processes public posts, we mask Personally Identifiable Information (PII) such as phone numbers and specific addresses before storage. We present results at the \emph{cluster/event} level to avoid targeting individuals.
    \item \textbf{Source Bias and Coverage:} News ``anchors'' may reflect editorial bias or incomplete coverage. To mitigate this, the pipeline explicitly supports \textbf{Social-only} event discovery (via HDBSCAN) so that underreported topics can still surface; nevertheless, anchor choice can influence what becomes ``verified'' in downstream summaries.
    \item \textbf{LLM Safety and Reliability:} LLMs can produce plausible but incorrect summaries. We therefore restrict LLM usage to \emph{post-hoc enrichment} (not triggering detection) and apply a \textbf{Human-in-the-Loop} verification policy for high-risk alerts (T1--Crisis) before any external escalation.
\end{itemize}


\begin{credits}
    \subsubsection{\ackname} We would like to express our sincere gratitude to Dr. Do Trong Hop and Mr. Nguyen Ngoc Quy for their invaluable supervision, critical insights, and continuous support throughout the development of this research. Their expert guidance on system architecture and methodology was instrumental in the success of this project.
\end{credits}

\bibliographystyle{splncs04}
\bibliography{refs}



\appendix
\section{Appendix: Prompt Engineering}
To ensure reproducibility, we provide the specific prompts used in our system's core modules.

\begin{table}[h]
    \caption{LLM Refinement Prompt (Phase 6)}\label{tab:prompt_refinement}
    \centering
    \begin{tabular}{|p{0.95\linewidth}|}
        \hline
        \textbf{Role:} Senior News Editor (Vietnamese).                                                                                                               \\
        \textbf{Task:} Identify title and extract 5W1H structure.                                                                                                     \\
        \textbf{Rules:}                                                                                                                                               \\
        1. Title: Concise, factual Vietnamese title ($\le$ 15 words).                                                                                                 \\
        2. Summary: Detailed 4-6 sentences including numbers, dates, locations.                                                                                       \\
        3. 5W1H: Extract Who, What, Where, When, Why.                                                                                                                 \\
        4. Advice: Provide actionable strategic advice for State and Business.                                                                                        \\
        5. Classification: Assign one of 7 categories (T1-T7).                                                                                                        \\
        \textbf{Output Format:} JSON dict containing \texttt{refined\_title}, \texttt{summary}, \texttt{category}, \texttt{advice\_state}, \texttt{advice\_business}. \\
        \hline
    \end{tabular}
\end{table}

\begin{table}[h]
    \caption{LLM-as-a-Judge Prompt (Evaluation)}\label{tab:prompt_eval}
    \centering
    \begin{tabular}{|p{0.95\linewidth}|}
        \hline
        \textbf{Role:} News Clustering Quality Expert.                                                                                     \\
        \textbf{Context:} Two unnamed clusters (A and B) consisting of sample posts.                                                       \\
        \textbf{Task:} Compare and decide:                                                                                                 \\
        1. Which cluster is more COHERENT (posts discuss the same topic)?                                                                  \\
        2. Which cluster is CLEARER (less topic mixing)?                                                                                   \\
        \textbf{Output Format:} JSON dict with \texttt{better\_cluster} ("A", "B", or "Tie"), \texttt{confidence}, and \texttt{reasoning}. \\
        \hline
    \end{tabular}
\end{table}

\begin{table}[h]
    \caption{Semantic Deduplication Prompt (Phase 4)}\label{tab:prompt_dedup}
    \centering
    \begin{tabular}{|p{0.95\linewidth}|}
        \hline
        \textbf{Role:} Senior News Editor.                                                      \\
        \textbf{Task:} Identify titles referring to the EXACT SAME single real-world event.     \\
        \textbf{Criteria for Match (Must match all 3):}                                         \\
        1. SAME Location (e.g., "Hanoi" vs "Hanoi" $\checkmark$).                               \\
        2. SAME Timeframe (e.g., "Today" vs "Today" $\checkmark$).                              \\
        3. SAME Core Entity (e.g., "Typhoon Yagi" vs "Storm No. 3" $\checkmark$).               \\
        \textbf{Output Format:} JSON dict mapping \texttt{"Original Title": "Canonical Title"}. \\
        \hline
    \end{tabular}
\end{table}

\begin{table}[h]
    \caption{Google Trends Filter Prompt (Phase 6)}\label{tab:prompt_trends}
    \centering
    \begin{tabular}{|p{0.95\linewidth}|}
        \hline
        \textbf{Role:} Google Trends Classifier (Vietnam).                                                          \\
        \textbf{Task:} Filter NOISE and MERGE duplicates.                                                           \\
        \textbf{Filter Rules (Remove):}                                                                             \\
        1. Weather/Utilities (e.g., "weather today", "lottery results").                                            \\
        2. Generic Broad Terms (e.g., "love", "news", "video").                                                     \\
        \textbf{Merge Rules (Group):}                                                                               \\
        - Combine variants of the same event (e.g., "AFF Cup schedule" + "AFF Cup standings" $\to$ "AFF Cup 2024"). \\
        \textbf{Output Format:} JSON dict with \texttt{filtered} list and \texttt{merged} map.                      \\
        \hline
    \end{tabular}
\end{table}
\end{document}
