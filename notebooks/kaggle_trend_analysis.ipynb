{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üî• Multi-Source Trend Detection (Improved Pipeline)\n",
                "\n",
                "**Key improvements:**\n",
                "- ‚úÖ Alias-based text normalization (uses Google Trends keywords)\n",
                "- ‚úÖ Multilingual sentence embeddings (`paraphrase-multilingual-mpnet-base-v2`)\n",
                "- ‚úÖ Direct trend assignment (no HDBSCAN needed)\n",
                "- ‚úÖ Trend coverage analysis (filter to real trends only)\n",
                "\n",
                "> üí° **Enable GPU**: Settings ‚Üí Accelerator ‚Üí GPU T4 x2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q sentence-transformers rich"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import csv\n",
                "import os\n",
                "import glob\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from collections import Counter, defaultdict\n",
                "from typing import Dict, List\n",
                "\n",
                "from sentence_transformers import SentenceTransformer\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "from sklearn.manifold import TSNE\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "import torch\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"üñ•Ô∏è Device: {device}\")\n",
                "if device == 'cuda':\n",
                "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === PATHS (Update these for your Kaggle dataset) ===\n",
                "FB_DATA_PATH = \"/kaggle/input/your-dataset/fb_data.json\"\n",
                "NEWS_DATA_DIR = \"/kaggle/input/your-dataset/data\"\n",
                "TRENDS_CSV_FILES = [\n",
                "    \"/kaggle/input/your-dataset/trending_VN_7d.csv\"\n",
                "]\n",
                "\n",
                "# === MODEL CONFIG ===\n",
                "MODEL_NAME = \"paraphrase-multilingual-mpnet-base-v2\"  # Best for Vietnamese similarity\n",
                "SIMILARITY_THRESHOLD = 0.55\n",
                "MIN_POSTS_FOR_VALID_TREND = 3\n",
                "\n",
                "print(f\"ü§ñ Model: {MODEL_NAME}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Alias Normalizer (Replaces NER)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Global alias dictionary\n",
                "TREND_ALIASES: Dict[str, List[str]] = {}\n",
                "\n",
                "def build_alias_dictionary(trends: dict) -> Dict[str, List[str]]:\n",
                "    \"\"\"Build alias dictionary from Google Trends keywords.\"\"\"\n",
                "    global TREND_ALIASES\n",
                "    TREND_ALIASES = {}\n",
                "    \n",
                "    for main_trend, keywords in trends.items():\n",
                "        canonical = main_trend.lower().strip()\n",
                "        aliases = [k.lower().strip() for k in keywords if k.strip()]\n",
                "        if canonical not in aliases:\n",
                "            aliases.insert(0, canonical)\n",
                "        TREND_ALIASES[canonical] = aliases\n",
                "    \n",
                "    print(f\"üìö Built {len(TREND_ALIASES)} alias groups\")\n",
                "    return TREND_ALIASES\n",
                "\n",
                "\n",
                "def normalize_with_aliases(text: str, max_additions: int = 10) -> str:\n",
                "    \"\"\"Add known aliases to text for better matching.\"\"\"\n",
                "    if not TREND_ALIASES:\n",
                "        return text\n",
                "    \n",
                "    text_lower = text.lower()\n",
                "    additions = set()\n",
                "    \n",
                "    for canonical, aliases in TREND_ALIASES.items():\n",
                "        if canonical in text_lower:\n",
                "            additions.update(aliases[:5])\n",
                "            additions.add(canonical)\n",
                "        for alias in aliases[:5]:\n",
                "            if len(alias) > 3 and alias in text_lower:\n",
                "                additions.add(canonical)\n",
                "                additions.update(aliases[:3])\n",
                "                break\n",
                "    \n",
                "    if additions:\n",
                "        return \" \".join(list(additions)[:max_additions]) + \" \" + text\n",
                "    return text\n",
                "\n",
                "\n",
                "def batch_normalize(texts: List[str]) -> List[str]:\n",
                "    \"\"\"Normalize multiple texts.\"\"\"\n",
                "    results = []\n",
                "    for i, text in enumerate(texts):\n",
                "        if (i + 1) % 500 == 0:\n",
                "            print(f\"  Normalizing: {i+1}/{len(texts)}\")\n",
                "        results.append(normalize_with_aliases(text))\n",
                "    return results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_facebook_data(filepath):\n",
                "    \"\"\"Load Facebook posts from JSON.\"\"\"\n",
                "    if not os.path.exists(filepath):\n",
                "        print(f\"‚ö†Ô∏è FB file not found: {filepath}\")\n",
                "        return []\n",
                "    \n",
                "    with open(filepath, 'r', encoding='utf-8') as f:\n",
                "        data = json.load(f)\n",
                "    \n",
                "    unified = []\n",
                "    for item in data:\n",
                "        unified.append({\n",
                "            \"source\": f\"Face: {item.get('page_name', 'Unknown')}\",\n",
                "            \"content\": item.get('content', ''),\n",
                "            \"stats\": item.get('stats', {'likes': 0, 'comments': 0, 'shares': 0}),\n",
                "            \"time\": item.get('time_label', '')\n",
                "        })\n",
                "    return unified\n",
                "\n",
                "\n",
                "def load_news_articles(data_dir):\n",
                "    \"\"\"Load news from CSV files.\"\"\"\n",
                "    if not os.path.exists(data_dir):\n",
                "        print(f\"‚ö†Ô∏è News dir not found: {data_dir}\")\n",
                "        return []\n",
                "    \n",
                "    unified = []\n",
                "    pattern = os.path.join(data_dir, \"**\", \"articles.csv\")\n",
                "    csv_files = glob.glob(pattern, recursive=True)\n",
                "    \n",
                "    for filepath in csv_files:\n",
                "        source_name = os.path.basename(os.path.dirname(filepath)).upper()\n",
                "        with open(filepath, 'r', encoding='utf-8') as f:\n",
                "            reader = csv.DictReader(f)\n",
                "            for row in reader:\n",
                "                unified.append({\n",
                "                    \"source\": source_name,\n",
                "                    \"content\": f\"{row.get('title', '')}\\n{row.get('content', '')}\",\n",
                "                    \"stats\": {'likes': 0, 'comments': 0, 'shares': 0},\n",
                "                    \"time\": row.get('published_at', '')\n",
                "                })\n",
                "    return unified\n",
                "\n",
                "\n",
                "def load_trends(csv_files):\n",
                "    \"\"\"Load trends from CSV files.\"\"\"\n",
                "    trends = {}\n",
                "    for filepath in csv_files:\n",
                "        if not os.path.exists(filepath):\n",
                "            print(f\"‚ö†Ô∏è Trends file not found: {filepath}\")\n",
                "            continue\n",
                "        \n",
                "        with open(filepath, 'r', encoding='utf-8') as f:\n",
                "            reader = csv.reader(f)\n",
                "            next(reader)  # Skip header\n",
                "            for row in reader:\n",
                "                if len(row) < 5:\n",
                "                    continue\n",
                "                main_trend = row[0].strip()\n",
                "                keywords = [k.strip() for k in row[4].split(',') if k.strip()]\n",
                "                if main_trend not in keywords:\n",
                "                    keywords.insert(0, main_trend)\n",
                "                trends[main_trend] = keywords\n",
                "    return trends"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load all data\n",
                "print(\"üìÇ Loading data...\")\n",
                "fb_data = load_facebook_data(FB_DATA_PATH)\n",
                "news_data = load_news_articles(NEWS_DATA_DIR)\n",
                "all_data = fb_data + news_data\n",
                "\n",
                "trends = load_trends(TRENDS_CSV_FILES)\n",
                "\n",
                "print(f\"‚úÖ Loaded: {len(fb_data)} FB + {len(news_data)} News = {len(all_data)} total\")\n",
                "print(f\"‚úÖ Loaded: {len(trends)} Google Trends\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Semantic Matching"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def find_matches(posts, trends, model_name, threshold=0.55):\n",
                "    \"\"\"Find matches using semantic similarity with alias normalization.\"\"\"\n",
                "    \n",
                "    # Build aliases from trends\n",
                "    build_alias_dictionary(trends)\n",
                "    \n",
                "    # Prepare texts\n",
                "    trend_keys = list(trends.keys())\n",
                "    trend_texts = [f\"{t} \" + \" \".join(trends[t][:5]) for t in trend_keys]\n",
                "    post_contents = [p.get('content', '')[:500] for p in posts]\n",
                "    \n",
                "    # Normalize with aliases\n",
                "    print(\"üîÑ Normalizing texts with aliases...\")\n",
                "    trend_texts = batch_normalize(trend_texts)\n",
                "    post_contents = batch_normalize(post_contents)\n",
                "    print(\"‚úÖ Done!\")\n",
                "    \n",
                "    # Encode\n",
                "    print(f\"üß† Loading model: {model_name}...\")\n",
                "    model = SentenceTransformer(model_name, device=device)\n",
                "    \n",
                "    print(f\"üß† Encoding {len(trend_texts)} trends...\")\n",
                "    trend_embeddings = model.encode(trend_texts, show_progress_bar=True, batch_size=32)\n",
                "    \n",
                "    print(f\"üß† Encoding {len(posts)} posts...\")\n",
                "    post_embeddings = model.encode(post_contents, show_progress_bar=True, batch_size=64)\n",
                "    \n",
                "    print(\"üìê Calculating similarity...\")\n",
                "    similarity_matrix = cosine_similarity(post_embeddings, trend_embeddings)\n",
                "    \n",
                "    # Match\n",
                "    matches = []\n",
                "    for i, post in enumerate(posts):\n",
                "        sim_scores = similarity_matrix[i]\n",
                "        best_idx = np.argmax(sim_scores)\n",
                "        best_score = sim_scores[best_idx]\n",
                "        \n",
                "        if best_score > threshold:\n",
                "            matches.append({\n",
                "                \"post_content\": post.get('content', ''),\n",
                "                \"source\": post.get('source', 'Unknown'),\n",
                "                \"time\": post.get('time', ''),\n",
                "                \"stats\": post.get('stats', {}),\n",
                "                \"trend\": trend_keys[best_idx],\n",
                "                \"score\": float(best_score),\n",
                "                \"is_matched\": True\n",
                "            })\n",
                "    \n",
                "    print(f\"‚úÖ Found {len(matches)} matches\")\n",
                "    return matches, model  # Return model for reuse"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run matching\n",
                "matches, model = find_matches(all_data, trends, MODEL_NAME, SIMILARITY_THRESHOLD)\n",
                "print(f\"\\nüéâ Total matches: {len(matches)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Trend Coverage Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze which trends have real coverage\n",
                "trend_coverage = Counter([m['trend'] for m in matches])\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"üìä TREND COVERAGE ANALYSIS\")\n",
                "print(\"=\"*60)\n",
                "print(f\"   Total Google Trends: {len(trends)}\")\n",
                "print(f\"   Trends with matches: {len(trend_coverage)}\")\n",
                "print(f\"   Trends with NO data: {len(trends) - len(trend_coverage)}\")\n",
                "\n",
                "# Filter valid trends\n",
                "valid_trends = {t: c for t, c in trend_coverage.items() if c >= MIN_POSTS_FOR_VALID_TREND}\n",
                "print(f\"\\n‚úÖ Valid trends (>= {MIN_POSTS_FOR_VALID_TREND} posts): {len(valid_trends)}\")\n",
                "\n",
                "# Top trends\n",
                "print(\"\\nüî• TOP 20 REAL TRENDS:\")\n",
                "for i, (trend, count) in enumerate(sorted(valid_trends.items(), key=lambda x: -x[1])[:20]):\n",
                "    print(f\"   {i+1}. {trend}: {count} posts\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Bar chart of top trends\n",
                "top_20 = sorted(valid_trends.items(), key=lambda x: -x[1])[:20]\n",
                "labels = [t[:25] + \"...\" if len(t) > 25 else t for t, _ in top_20][::-1]\n",
                "counts = [c for _, c in top_20][::-1]\n",
                "\n",
                "plt.figure(figsize=(12, 10))\n",
                "bars = plt.barh(labels, counts, color='steelblue')\n",
                "plt.xlabel('Number of Posts')\n",
                "plt.title('Top 20 Real Trends (with data coverage)')\n",
                "\n",
                "for bar in bars:\n",
                "    plt.text(bar.get_width() + 0.3, bar.get_y() + bar.get_height()/2,\n",
                "             str(int(bar.get_width())), va='center')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('top_trends.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# t-SNE visualization by trend\n",
                "valid_trend_names = set(valid_trends.keys())\n",
                "filtered_matches = [m for m in matches if m['trend'] in valid_trend_names]\n",
                "\n",
                "print(f\"Visualizing {len(filtered_matches)} posts from {len(valid_trend_names)} trends\")\n",
                "\n",
                "texts = [m['post_content'][:500] for m in filtered_matches]\n",
                "trend_labels = [m['trend'] for m in filtered_matches]\n",
                "\n",
                "# Encode (reuse model)\n",
                "print(\"üß† Generating embeddings...\")\n",
                "embeddings = model.encode(texts, show_progress_bar=True, batch_size=64)\n",
                "\n",
                "# t-SNE\n",
                "print(\"üìâ Running t-SNE...\")\n",
                "perplexity = min(30, len(texts) - 1)\n",
                "tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
                "coords = tsne.fit_transform(embeddings)\n",
                "\n",
                "# Color by top 10 trends\n",
                "top_10 = [t for t, _ in sorted(valid_trends.items(), key=lambda x: -x[1])[:10]]\n",
                "colors = [top_10.index(t) if t in top_10 else -1 for t in trend_labels]\n",
                "\n",
                "plt.figure(figsize=(14, 10))\n",
                "scatter = plt.scatter(coords[:, 0], coords[:, 1], c=colors, cmap='tab10', alpha=0.7, s=40)\n",
                "plt.colorbar(scatter, label='Trend Index')\n",
                "plt.title(f'Real Trends Visualization ({len(valid_trend_names)} trends)')\n",
                "\n",
                "# Add labels for top 10\n",
                "for i, trend in enumerate(top_10):\n",
                "    mask = [c == i for c in colors]\n",
                "    if any(mask):\n",
                "        centroid = coords[[j for j, m in enumerate(mask) if m]].mean(axis=0)\n",
                "        plt.annotate(trend[:20], centroid, fontsize=9, weight='bold',\n",
                "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('trend_tsne.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Scoring & Classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "\n",
                "def compute_scores(trend_name, items, google_volume=0):\n",
                "    \"\"\"Compute G, F, N scores for a trend.\"\"\"\n",
                "    \n",
                "    # G Score (Google search volume)\n",
                "    MAX_VOL = 1000000\n",
                "    g_score = (math.log10(google_volume + 1) / math.log10(MAX_VOL + 1)) * 100 if google_volume > 0 else 0\n",
                "    \n",
                "    # F Score (Facebook engagement)\n",
                "    total_interactions = sum(\n",
                "        item['stats'].get('likes', 0) + \n",
                "        item['stats'].get('comments', 0) * 2 + \n",
                "        item['stats'].get('shares', 0) * 3\n",
                "        for item in items if 'Face' in item.get('source', '')\n",
                "    )\n",
                "    MAX_INTERACTIONS = 20000\n",
                "    f_score = (math.log10(total_interactions + 1) / math.log10(MAX_INTERACTIONS + 1)) * 100\n",
                "    \n",
                "    # N Score (News coverage)\n",
                "    news_count = len([item for item in items if 'Face' not in item.get('source', '')])\n",
                "    MAX_ARTICLES = 50\n",
                "    n_score = (news_count / MAX_ARTICLES) * 100\n",
                "    \n",
                "    # Composite\n",
                "    composite = 0.4 * min(g_score, 100) + 0.35 * min(f_score, 100) + 0.25 * min(n_score, 100)\n",
                "    \n",
                "    # Classification\n",
                "    HIGH = 40\n",
                "    if g_score > HIGH and f_score > HIGH and n_score > HIGH:\n",
                "        classification = \"Strong Multi-source\"\n",
                "    elif f_score > HIGH and n_score > HIGH:\n",
                "        classification = \"Social & News\"\n",
                "    elif f_score > HIGH:\n",
                "        classification = \"Social-Driven\"\n",
                "    elif n_score > HIGH:\n",
                "        classification = \"News-Driven\"\n",
                "    else:\n",
                "        classification = \"Emerging\"\n",
                "    \n",
                "    return {\n",
                "        \"G\": round(g_score, 1),\n",
                "        \"F\": round(f_score, 1),\n",
                "        \"N\": round(n_score, 1),\n",
                "        \"Composite\": round(composite, 1),\n",
                "        \"Class\": classification,\n",
                "        \"Interactions\": total_interactions,\n",
                "        \"NewsCount\": news_count\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build trend clusters from matches\n",
                "trend_clusters = defaultdict(list)\n",
                "for m in matches:\n",
                "    if m['trend'] in valid_trends:\n",
                "        trend_clusters[m['trend']].append(m)\n",
                "\n",
                "# Compute scores for each trend\n",
                "results = []\n",
                "for trend_name, items in trend_clusters.items():\n",
                "    scores = compute_scores(trend_name, items)\n",
                "    results.append({\n",
                "        \"trend\": trend_name,\n",
                "        \"posts\": len(items),\n",
                "        **scores\n",
                "    })\n",
                "\n",
                "# Sort by composite score\n",
                "results = sorted(results, key=lambda x: x['Composite'], reverse=True)\n",
                "\n",
                "# Display\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"üìä FINAL TREND RANKINGS\")\n",
                "print(\"=\"*80)\n",
                "print(f\"{'Rank':<5} {'Trend':<35} {'Class':<18} {'Score':>6} {'G/F/N':>12} {'Posts':>6}\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "for i, r in enumerate(results[:30]):\n",
                "    gfn = f\"{r['G']:.0f}/{r['F']:.0f}/{r['N']:.0f}\"\n",
                "    print(f\"{i+1:<5} {r['trend'][:35]:<35} {r['Class']:<18} {r['Composite']:>6.1f} {gfn:>12} {r['posts']:>6}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Batch Summarization (Optional)\n",
                "\n",
                "Pre-compute summaries for all posts/articles and cache for later use."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# BATCH SUMMARIZE FACEBOOK POSTS\n",
                "# ==========================================\n",
                "import sys\n",
                "sys.path.insert(0, '/kaggle/working/Real-time-Event-Detection-on-Social-Media-Data')\n",
                "\n",
                "from scripts.batch_summarize import batch_summarize, merge_summaries_into_posts, load_posts\n",
                "\n",
                "# --- CONFIG ---\n",
                "FB_SUMMARY_OUTPUT = '/kaggle/working/fb_summaries.json'\n",
                "SUMMARY_MODEL = 'vit5-base'  # 'vit5-large' for best quality\n",
                "\n",
                "# --- RUN (only once!) ---\n",
                "fb_summaries = batch_summarize(\n",
                "    input_path=FB_DATA_PATH,\n",
                "    output_path=FB_SUMMARY_OUTPUT,\n",
                "    model_name=SUMMARY_MODEL,\n",
                "    max_length=200,\n",
                "    resume=True\n",
                ")\n",
                "\n",
                "# --- MERGE INTO DATA ---\n",
                "fb_posts_with_summary = load_posts(FB_DATA_PATH)\n",
                "fb_posts_with_summary = merge_summaries_into_posts(fb_posts_with_summary, FB_SUMMARY_OUTPUT)\n",
                "\n",
                "# Preview\n",
                "sample = [p for p in fb_posts_with_summary if p.get('summary')][:1]\n",
                "if sample:\n",
                "    print(\"\\nüìã Sample FB post with summary:\")\n",
                "    print(f\"Original: {str(sample[0].get('content', ''))[:200]}...\")\n",
                "    print(f\"Summary:  {sample[0]['summary']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# BATCH SUMMARIZE NEWS ARTICLES (ALL SOURCES)\n",
                "# ==========================================\n",
                "import os\n",
                "import pandas as pd\n",
                "from scripts.batch_summarize import batch_summarize, load_summaries_for_use\n",
                "\n",
                "# --- CONFIG ---\n",
                "NEWS_SOURCES = ['vnexpress', 'tuoitre', 'thanhnien', 'vietnamnet', 'nld']\n",
                "NEWS_SUMMARY_DIR = '/kaggle/working/news_summaries'\n",
                "\n",
                "os.makedirs(NEWS_SUMMARY_DIR, exist_ok=True)\n",
                "\n",
                "# --- RUN FOR EACH SOURCE ---\n",
                "all_news_summaries = {}\n",
                "\n",
                "for source in NEWS_SOURCES:\n",
                "    input_path = f'{NEWS_DATA_DIR}/{source}/articles.csv'\n",
                "    output_path = f'{NEWS_SUMMARY_DIR}/{source}_summaries.json'\n",
                "    \n",
                "    if not os.path.exists(input_path):\n",
                "        print(f\"‚ö†Ô∏è Skipping {source}: {input_path} not found\")\n",
                "        continue\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*50)\n",
                "    print(f\"üì∞ Processing: {source.upper()}\")\n",
                "    print(\"=\"*50)\n",
                "    \n",
                "    summaries = batch_summarize(\n",
                "        input_path=input_path,\n",
                "        output_path=output_path,\n",
                "        model_name=SUMMARY_MODEL,\n",
                "        max_length=200,\n",
                "        resume=True\n",
                "    )\n",
                "    all_news_summaries[source] = summaries\n",
                "\n",
                "print(f\"\\n‚úÖ Total articles summarized: {sum(len(s) for s in all_news_summaries.values())}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# LOAD ALL SUMMARIES INTO DATAFRAMES\n",
                "# ==========================================\n",
                "all_news_dfs = {}\n",
                "\n",
                "for source in NEWS_SOURCES:\n",
                "    csv_path = f'{NEWS_DATA_DIR}/{source}/articles.csv'\n",
                "    summary_path = f'{NEWS_SUMMARY_DIR}/{source}_summaries.json'\n",
                "    \n",
                "    if not os.path.exists(csv_path) or not os.path.exists(summary_path):\n",
                "        continue\n",
                "    \n",
                "    df = pd.read_csv(csv_path)\n",
                "    summaries = load_summaries_for_use(summary_path)\n",
                "    \n",
                "    # Merge using URL as key\n",
                "    df['summary'] = df['url'].apply(lambda u: summaries.get(str(u), ''))\n",
                "    all_news_dfs[source] = df\n",
                "    print(f\"‚úÖ {source}: {len(df)} articles, {(df['summary'] != '').sum()} with summaries\")\n",
                "\n",
                "# Combine all\n",
                "if all_news_dfs:\n",
                "    combined_news_df = pd.concat(all_news_dfs.values(), ignore_index=True)\n",
                "    print(f\"\\nüìä Total: {len(combined_news_df)} articles\")\n",
                "    display(combined_news_df[[\"source\", \"title\", \"summary\"]].head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save matches\n",
                "with open('results.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(matches, f, ensure_ascii=False, indent=2)\n",
                "print(f\"üíæ Saved {len(matches)} matches to results.json\")\n",
                "\n",
                "# Save rankings\n",
                "with open('trend_rankings.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
                "print(f\"üíæ Saved {len(results)} trend rankings to trend_rankings.json\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}