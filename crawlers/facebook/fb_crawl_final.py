import time
import json
import os
import re
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.common.by import By

# ================= C·∫§U H√åNH D·ª∞ √ÅN =================
# L·∫•y ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a script
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
FILE_OUTPUT = os.path.join(SCRIPT_DIR, "fb_data.json")

TARGET_URLS = [
    "https://m.facebook.com/Theanh28",
    "https://www.facebook.com/kienkhongngu.vn", 
    "https://www.facebook.com/thongtinchinhphu",
]

PROFILE_PATH = "my_firefox_profile"

# C·∫•u h√¨nh s·ªë b√†i vi·∫øt t·ªëi thi·ªÉu c·∫ßn crawl
MIN_POSTS_PER_PAGE = 20  # S·ªë b√†i t·ªëi thi·ªÉu mu·ªën l·∫•y m·ªói page
MAX_SCROLL_ATTEMPTS = 15  # S·ªë l·∫ßn scroll t·ªëi ƒëa
SCROLL_PAUSE_TIME = 3  # Th·ªùi gian ch·ªù sau m·ªói l·∫ßn scroll (gi√¢y)
# ==================================================

def init_json():
    if not os.path.exists(FILE_OUTPUT):
        with open(FILE_OUTPUT, 'w', encoding='utf-8') as f:
            json.dump([], f, ensure_ascii=False, indent=4)

def clean_number(text):
    if not text: return 0
    match = re.search(r'(\d+[.,]?\d*[MK]?)', text)
    if not match: return 0
    
    num_str = match.group(1).replace(',', '.')
    multiplier = 1
    if 'K' in num_str:
        multiplier = 1000
        num_str = num_str.replace('K', '')
    elif 'M' in num_str:
        multiplier = 1000000
        num_str = num_str.replace('M', '')
        
    try:
        return int(float(num_str) * multiplier)
    except:
        return 0

def clean_time_string(text):
    """
    L√†m s·∫°ch chu·ªói th·ªùi gian ch·ª©a k√Ω t·ª± ·∫©n (\u200e) v√† icon l·∫°
    Input: "‚Äé4h‚ÄéÛ∞ûãÛ±ô∑" -> Output: "4h"
    """
    if not text: return ""
    # Gi·ªØ l·∫°i ch·ªØ c√°i (bao g·ªìm ti·∫øng Vi·ªát), s·ªë, kho·∫£ng tr·∫Øng, d·∫•u ch·∫•m
    clean = re.sub(r'[^\w\s,.]', '', text).strip()
    
    # N·∫øu d√≠nh icon ·ªü cu·ªëi (VD: "4hIcon"), ch·ªâ l·∫•y ph·∫ßn text ƒë·∫ßu ti√™n
    match = re.match(r'^([\w\s,.]+)', clean)
    if match:
        return match.group(1).strip()
    return clean

def is_timestamp(text):
    """
    Ki·ªÉm tra xem text c√≥ ph·∫£i l√† ƒë·ªãnh d·∫°ng th·ªùi gian kh√¥ng.
    """
    # L√†m s·∫°ch tr∆∞·ªõc khi check regex
    clean_text = clean_time_string(text).lower()
    
    if not clean_text: return False
    if len(clean_text) > 30: return False # Time label th∆∞·ªùng r·∫•t ng·∫Øn

    # Pattern: 1h, 12m, 3d, 4y, 2w (ch·∫•p nh·∫≠n c·∫£ kho·∫£ng tr·∫Øng "1 h")
    if re.match(r'^\d+\s*[wmhdys]$', clean_text): return True
    
    # Pattern with "hrs", "min", "mins", "day", "days"
    if re.match(r'^\d+\s*(hrs?|mins?|days?|weeks?|months?|years?)$', clean_text): return True
    
    # Pattern Ti·∫øng Vi·ªát: 2 gi·ªù, 5 ph√∫t, 1 ng√†y, 27 thg 11
    if re.match(r'^\d+\s*(gi·ªù|ph√∫t|ng√†y|nƒÉm|th√°ng|tu·∫ßn)', clean_text): return True
    if re.match(r'^\d{1,2}\s+thg\s+\d{1,2}.*', clean_text): return True
    
    # Pattern Ti·∫øng Anh: Nov 27, December 4, Jan 15
    if re.match(r'^[a-z]{3,9}\s\d{1,2}.*', clean_text): return True
    
    # Pattern: s·ªë gi·ªù tr∆∞·ªõc (VD: "4 gi·ªù tr∆∞·ªõc", "3 hours ago")
    if re.search(r'(gi·ªù|ph√∫t|ng√†y|tu·∫ßn|th√°ng|nƒÉm|hour|minute|day|week|month|year)s?\s*(tr∆∞·ªõc|ago)', clean_text): 
        return True
    
    # C√°c t·ª´ kh√≥a ƒë·∫∑c bi·ªát
    keywords = ["yesterday", "today", "h√¥m qua", "h√¥m nay", "v·ª´a xong", "just now", "mins", "hrs", "ƒë√£ ƒëƒÉng"]
    if any(k in clean_text for k in keywords): return True
    
    return False

def calculate_publish_time(raw_time_text):
    """
    Chuy·ªÉn ƒë·ªïi text Facebook (3h, 5m, Yesterday) th√†nh th·ªùi gian th·ª±c (ISO Format)
    """
    if not raw_time_text: return None
    
    now = datetime.now()
    text = raw_time_text.lower().strip()
    
    try:
        # 1. X·ª≠ l√Ω ph√∫t (m, min, ph√∫t)
        match = re.search(r'^(\d+)\s*[m|min|ph√∫t]', text)
        if match:
            minutes = int(match.group(1))
            return (now - timedelta(minutes=minutes)).isoformat()

        # 2. X·ª≠ l√Ω gi·ªù (h, hr, gi·ªù)
        match = re.search(r'^(\d+)\s*[h|hr|gi·ªù]', text)
        if match:
            hours = int(match.group(1))
            return (now - timedelta(hours=hours)).isoformat()

        # 3. X·ª≠ l√Ω ng√†y (d, day, ng√†y)
        match = re.search(r'^(\d+)\s*[d|day|ng√†y]', text)
        if match:
            days = int(match.group(1))
            return (now - timedelta(days=days)).isoformat()
            
        # 4. X·ª≠ l√Ω "H√¥m qua" / "Yesterday"
        if "yesterday" in text or "h√¥m qua" in text:
            return (now - timedelta(days=1)).isoformat()
            
        # 5. X·ª≠ l√Ω nƒÉm (1y)
        match = re.search(r'^(\d+)\s*y', text)
        if match:
            years = int(match.group(1))
            return (now - timedelta(days=years*365)).isoformat()

    except:
        pass
        
    return None

def expand_all(driver):
    try:
        btns = driver.find_elements(By.XPATH, "//span[contains(text(), 'See more') or contains(text(), 'Xem th√™m') or text()='‚Ä¶']")
        if btns:
            for btn in btns:
                try: driver.execute_script("arguments[0].click();", btn)
                except: pass
            time.sleep(2) 
    except: pass

def scroll_and_wait_for_content(driver, min_posts=MIN_POSTS_PER_PAGE, max_attempts=MAX_SCROLL_ATTEMPTS):
    """
    Cu·ªôn trang ƒë·ªÉ t·∫£i th√™m b√†i vi·∫øt (lazy loading).
    Pattern: repeat: down -> up (half-page) until no more new posts
    Tr·∫£ v·ªÅ s·ªë b√†i vi·∫øt ƒë√£ ph√°t hi·ªán.
    """
    print(f"  üìú ƒêang cu·ªôn trang ƒë·ªÉ t·∫£i b√†i vi·∫øt (m·ª•c ti√™u: {min_posts} b√†i)...")
    
    scroll_count = 0
    no_new_content_count = 0
    last_height = 0
    
    def count_posts():
        try:
            stream_container = driver.find_element(By.XPATH, "//div[@data-type='vscroller']")
            blocks = stream_container.find_elements(By.XPATH, "./div")
            return len([b for b in blocks if 'data-testid="post-profile-image' in b.get_attribute('outerHTML')])
        except:
            return 0
    
    def get_page_height():
        return driver.execute_script("return document.body.scrollHeight")
    
    def get_current_scroll():
        return driver.execute_script("return window.pageYOffset")
    
    def smooth_scroll(target_y, duration=1.0):
        """Cu·ªôn m∆∞·ª£t ƒë·∫øn v·ªã tr√≠ target_y trong kho·∫£ng duration gi√¢y"""
        start_y = get_current_scroll()
        distance = target_y - start_y
        steps = 20  # S·ªë b∆∞·ªõc cu·ªôn
        step_delay = duration / steps
        
        for i in range(1, steps + 1):
            # Easing function (ease-out)
            progress = i / steps
            eased_progress = 1 - (1 - progress) ** 2
            new_y = start_y + (distance * eased_progress)
            driver.execute_script(f"window.scrollTo(0, {int(new_y)});")
            time.sleep(step_delay)
    
    def scroll_to_bottom():
        target = get_page_height()
        smooth_scroll(target, duration=1.5)
    
    def scroll_to_half():
        target = get_page_height() // 2
        smooth_scroll(target, duration=1.0)
    
    while scroll_count < max_attempts:
        current_posts = count_posts()
        
        if current_posts >= min_posts:
            print(f"  ‚úÖ ƒê√£ t·∫£i ƒë·ªß {current_posts} b√†i vi·∫øt!")
            break
        
        scroll_count += 1
        
        # === B∆Ø·ªöC 1: Cu·ªôn xu·ªëng cu·ªëi trang ===
        scroll_to_bottom()
        time.sleep(SCROLL_PAUSE_TIME)
        
        # === B∆Ø·ªöC 2: Cu·ªôn l√™n n·ª≠a trang ===
        scroll_to_half()
        time.sleep(1)
        
        # M·ªü r·ªông c√°c b√†i vi·∫øt b·ªã thu g·ªçn
        expand_all(driver)
        
        # Ki·ªÉm tra chi·ªÅu cao m·ªõi
        new_height = get_page_height()
        current_posts = count_posts()
        
        print(f"    L·∫ßn cu·ªôn {scroll_count}: ƒë√£ t√¨m th·∫•y ~{current_posts} b√†i vi·∫øt")
        
        if new_height == last_height:
            no_new_content_count += 1
            if no_new_content_count >= 3:
                print(f"  ‚ö†Ô∏è Kh√¥ng c√≤n n·ªôi dung m·ªõi sau {scroll_count} l·∫ßn cu·ªôn.")
                break
        else:
            no_new_content_count = 0
            last_height = new_height
    
    return count_posts()


def parse_stream(driver, page_name_slug):
    posts_data = []
    
    try:
        stream_container = driver.find_element(By.XPATH, "//div[@data-type='vscroller']")
        blocks = stream_container.find_elements(By.XPATH, "./div")
    except:
        return []

    current_post = {
        "text": [], "images": [], "videos": [], 
        "likes": 0, "comments": 0, "shares": 0,
        "time_text": "",
        "has_avatar": False
    }

    for block in blocks:
        html_block = block.get_attribute('outerHTML')
        
        # 1. B·∫ÆT ƒê·∫¶U B√ÄI M·ªöI (D·ª±a v√†o Avatar)
        if 'data-testid="post-profile-image' in html_block:
            if current_post["has_avatar"] and (current_post["text"] or current_post["images"]):
                posts_data.append(current_post)
            
            current_post = {
                "text": [], "images": [], "videos": [],
                "likes": 0, "comments": 0, "shares": 0,
                "time_text": "",
                "has_avatar": True
            }
            
            # ===== T√åM TH·ªúI GIAN NGAY TRONG BLOCK AVATAR =====
            # Facebook th∆∞·ªùng ƒë·∫∑t timestamp g·∫ßn avatar (·ªü header c·ªßa post)
            try:
                # Ph∆∞∆°ng ph√°p 1: T√¨m trong abbr tag (c√≥ th·ªÉ c√≥ data-utime)
                abbr_elems = block.find_elements(By.TAG_NAME, "abbr")
                for abbr in abbr_elems:
                    time_attr = abbr.get_attribute("data-utime") or abbr.get_attribute("title")
                    if time_attr:
                        current_post["time_text"] = time_attr
                        break
                
                # Ph∆∞∆°ng ph√°p 2: T√¨m c√°c span/div ch·ª©a text th·ªùi gian
                if not current_post["time_text"]:
                    time_candidates = block.find_elements(By.XPATH, ".//span | .//a")
                    for tc in time_candidates:
                        raw_t = driver.execute_script("return arguments[0].textContent;", tc).strip()
                        if raw_t and is_timestamp(raw_t):
                            current_post["time_text"] = clean_time_string(raw_t)
                            break
            except:
                pass
            continue

        if not current_post["has_avatar"]: continue

        # 2. QU√âT TEXT AREA (X·ª≠ l√Ω c·∫£ Time label l·∫´n Content t·∫°i ƒë√¢y)
        try:
            # T√¨m t·∫•t c·∫£ TextArea (v√¨ tr√™n mobile, time label c≈©ng n·∫±m trong TextArea)
            text_elems = block.find_elements(By.XPATH, ".//div[contains(@data-mcomponent, 'TextArea')]")
            
            for elem in text_elems:
                # L·∫•y text th√¥ (raw)
                raw_txt = driver.execute_script("return arguments[0].textContent;", elem).strip()
                if not raw_txt: continue
                
                # --- CHECK XEM C√ì PH·∫¢I L√Ä TIME KH√îNG ---
                # Ch·ªâ check n·∫øu b√†i hi·ªán t·∫°i ch∆∞a c√≥ time
                if not current_post["time_text"]:
                    if is_timestamp(raw_txt):
                        # L√†m s·∫°ch (b·ªè icon tr√°i ƒë·∫•t, b·ªè k√Ω t·ª± ·∫©n)
                        clean_t = clean_time_string(raw_txt)
                        current_post["time_text"] = clean_t
                        # N·∫øu ƒë√£ l√† time th√¨ b·ªè qua, kh√¥ng add v√†o content
                        continue 
                # ---------------------------------------

                # N·∫æU KH√îNG PH·∫¢I TIME TH√å L√Ä CONTENT
                txt_lower = raw_txt.lower()
                
                # Logic l·ªçc r√°c c≈© c·ªßa b·∫°n
                if page_name_slug.lower() in txt_lower and len(raw_txt) < 50:
                    continue # B·ªè qua t√™n page l·∫∑p l·∫°i
                
                if "comment" not in txt_lower and "share" not in txt_lower:
                    clean_txt = raw_txt.replace("... See more", "").replace("... Xem th√™m", "")
                    current_post["text"].append(clean_txt)
        except: pass

        # 3. ·∫¢NH/VIDEO
        try:
            imgs = block.find_elements(By.TAG_NAME, "img")
            for img in imgs:
                alt = img.get_attribute("alt")
                if alt and len(alt) > 15 and "profile picture" not in alt.lower():
                    current_post["images"].append(alt)
            
            if 'data-type="video"' in html_block or 'aria-label="Video player"' in html_block:
                current_post["videos"].append("Video")
        except: pass

        # 4. STATS
        try:
            btns = block.find_elements(By.XPATH, ".//div[@role='button']")
            for btn in btns:
                label = btn.get_attribute("aria-label")
                if not label: continue
                
                if 'like' in label.lower() and 'comment' not in label.lower():
                    current_post["likes"] = clean_number(label)
                elif 'comment' in label.lower():
                    current_post["comments"] = clean_number(label)
                elif 'share' in label.lower():
                    current_post["shares"] = clean_number(label)
        except: pass

    if current_post["has_avatar"] and (current_post["text"] or current_post["images"]):
        posts_data.append(current_post)

    return posts_data

def save_to_json(new_posts, page_name):
    if not new_posts: return 0
    clean_data = []
    crawl_timestamp = datetime.now().isoformat()
    
    for p in new_posts:
        full_text = "\n".join(p["text"])
        
        # B·ªè qua b√†i qu√° ng·∫Øn v√† kh√¥ng c√≥ ·∫£nh/video
        if len(full_text) < 5 and not p["images"] and not p["videos"]:
            continue
            
        # --- T√çNH TO√ÅN GI·ªú ƒêƒÇNG ---
        calculated_time = calculate_publish_time(p["time_text"])
        if not calculated_time:
            calculated_time = crawl_timestamp # Fallback n·∫øu kh√¥ng t√≠nh ƒë∆∞·ª£c
        # --------------------------

        post_obj = {
            "page_name": page_name,
            "published_time": calculated_time,
            "crawl_time": crawl_timestamp,
            "time_label": p["time_text"],      # Label ƒë√£ ƒë∆∞·ª£c clean (VD: 4h)
            "content": full_text,
            "media": {
                "images": p["images"],
                "videos": p["videos"]
            },
            "stats": {
                "likes": p["likes"],
                "comments": p["comments"],
                "shares": p["shares"]
            }
        }
        clean_data.append(post_obj)
        
        print(f"    ‚úÖ [POST] Time: {p['time_text']} | Content: {full_text[:30]}...")

    try:
        with open(FILE_OUTPUT, 'r', encoding='utf-8') as f:
            try: current_data = json.load(f)
            except: current_data = []
        
        current_data.extend(clean_data)
        
        with open(FILE_OUTPUT, 'w', encoding='utf-8') as f:
            json.dump(current_data, f, ensure_ascii=False, indent=4)
            
    except Exception as e:
        print(f"‚ùå L·ªói ghi file: {e}")

    return len(clean_data)

def run_crawler():
    init_json() # kh·ªüi t·∫°o json
    print("üöó ƒêang kh·ªüi ƒë·ªông Firefox (Fixed Time Logic)...")
    
    options = Options()
    # User Agent gi·∫£ l·∫≠p Android ƒë·ªÉ √©p v·ªÅ giao di·ªán m.facebook nh·∫π nh·∫•t
    mobile_ua = "Mozilla/5.0 (Linux; Android 11; SAMSUNG SM-G973U) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/14.2 Chrome/87.0.4280.141 Mobile Safari/537.36"
    options.set_preference("general.useragent.override", mobile_ua)
    
    abs_profile_path = os.path.abspath(PROFILE_PATH)
    if not os.path.exists(abs_profile_path):
        print(f"üìÅ T·∫°o m·ªõi profile folder t·∫°i: {abs_profile_path}")
        os.makedirs(abs_profile_path)
        
    if os.path.exists(abs_profile_path):
        options.add_argument("-profile")
        options.add_argument(abs_profile_path)
    
    driver = webdriver.Firefox(options=options)
    
    try:
        print("üåê ƒêang v√†o Facebook...")
        driver.get("https://m.facebook.com")
        time.sleep(3)
        if "login" in driver.current_url:
            print("‚ö†Ô∏è CH∆ØA ƒêƒÇNG NH·∫¨P! H√£y ƒëƒÉng nh·∫≠p th·ªß c√¥ng r·ªìi quay l·∫°i ƒë√¢y b·∫•m Enter...")
            input()
        
        for url in TARGET_URLS:
            page_name_slug = url.split('/')[-1]
            print(f"\n--- ƒêang x·ª≠ l√Ω Page: {page_name_slug} ---")
            driver.get(url)
            time.sleep(5)
            
            # Cu·ªôn trang ƒë·ªÉ t·∫£i th√™m b√†i vi·∫øt (lazy loading)
            scroll_and_wait_for_content(driver, MIN_POSTS_PER_PAGE, MAX_SCROLL_ATTEMPTS)
            
            # M·ªü r·ªông t·∫•t c·∫£ b√†i vi·∫øt tr∆∞·ªõc khi parse
            expand_all(driver)
            
            print("  üîÑ ƒêang ph√¢n t√≠ch lu·ªìng b√†i vi·∫øt...")
            posts = parse_stream(driver, page_name_slug)
            
            saved = save_to_json(posts, page_name_slug)
            print(f"  üèÅ ƒê√£ l∆∞u {saved} b√†i v√†o {FILE_OUTPUT}.")

    except Exception as e:
        print(f"‚ùå L·ªói Critical: {e}")
    finally:
        print("üõë K·∫øt th√∫c session.")
        # driver.quit() # M·ªü d√≤ng n√†y n·∫øu mu·ªën t·ª± ƒë·ªông ƒë√≥ng tr√¨nh duy·ªát

if __name__ == "__main__":
    run_crawler()