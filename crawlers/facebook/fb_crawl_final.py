import time
import json
import os
import re
import argparse
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.firefox.options import Options as FirefoxOptions
from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.firefox.service import Service as FirefoxService
from selenium.webdriver.common.by import By
from rich.console import Console
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
from rich.table import Table
from rich import print as rprint

console = Console()


# ================= C·∫§U H√åNH D·ª∞ √ÅN =================
# C·∫•u h√¨nh s·∫Ω ƒë∆∞·ª£c parse t·ª´ argparse
# ==================================================

def init_json(file_output):
    if not os.path.exists(file_output):
        with open(file_output, 'w', encoding='utf-8') as f:
            json.dump([], f, ensure_ascii=False, indent=4)

def clean_number(text):
    if not text: return 0
    match = re.search(r'(\d+[.,]?\d*[MK]?)', text)
    if not match: return 0
    
    num_str = match.group(1).replace(',', '.')
    multiplier = 1
    if 'K' in num_str:
        multiplier = 1000
        num_str = num_str.replace('K', '')
    elif 'M' in num_str:
        multiplier = 1000000
        num_str = num_str.replace('M', '')
        
    try:
        return int(float(num_str) * multiplier)
    except:
        return 0

def clean_time_string(text):
    """
    L√†m s·∫°ch chu·ªói th·ªùi gian ch·ª©a k√Ω t·ª± ·∫©n (\u200e) v√† icon l·∫°
    Input: "‚Äé4h‚ÄéÛ∞ûãÛ±ô∑" -> Output: "4h"
    """
    if not text: return ""
    # Gi·ªØ l·∫°i ch·ªØ c√°i (bao g·ªìm ti·∫øng Vi·ªát), s·ªë, kho·∫£ng tr·∫Øng, d·∫•u ch·∫•m
    clean = re.sub(r'[^\w\s,.]', '', text).strip()
    
    # N·∫øu d√≠nh icon ·ªü cu·ªëi (VD: "4hIcon"), ch·ªâ l·∫•y ph·∫ßn text ƒë·∫ßu ti√™n
    match = re.match(r'^([\w\s,.]+)', clean)
    if match:
        return match.group(1).strip()
    return clean

def is_timestamp(text):
    """
    Ki·ªÉm tra xem text c√≥ ph·∫£i l√† ƒë·ªãnh d·∫°ng th·ªùi gian kh√¥ng.
    """
    # L√†m s·∫°ch tr∆∞·ªõc khi check regex
    clean_text = clean_time_string(text).lower()
    
    if not clean_text: return False
    if len(clean_text) > 30: return False # Time label th∆∞·ªùng r·∫•t ng·∫Øn

    # Pattern: 1h, 12m, 3d, 4y, 2w (ch·∫•p nh·∫≠n c·∫£ kho·∫£ng tr·∫Øng "1 h")
    if re.match(r'^\d+\s*[wmhdys]$', clean_text): return True
    
    # Pattern with "hrs", "min", "mins", "day", "days"
    if re.match(r'^\d+\s*(hrs?|mins?|days?|weeks?|months?|years?)$', clean_text): return True
    
    # Pattern Ti·∫øng Vi·ªát: 2 gi·ªù, 5 ph√∫t, 1 ng√†y, 27 thg 11
    if re.match(r'^\d+\s*(gi·ªù|ph√∫t|ng√†y|nƒÉm|th√°ng|tu·∫ßn)', clean_text): return True
    if re.match(r'^\d{1,2}\s+thg\s+\d{1,2}.*', clean_text): return True
    
    # Pattern Ti·∫øng Anh: Nov 27, December 4, Jan 15
    if re.match(r'^[a-z]{3,9}\s\d{1,2}.*', clean_text): return True
    
    # Pattern: s·ªë gi·ªù tr∆∞·ªõc (VD: "4 gi·ªù tr∆∞·ªõc", "3 hours ago")
    if re.search(r'(gi·ªù|ph√∫t|ng√†y|tu·∫ßn|th√°ng|nƒÉm|hour|minute|day|week|month|year)s?\s*(tr∆∞·ªõc|ago)', clean_text): 
        return True
    
    # C√°c t·ª´ kh√≥a ƒë·∫∑c bi·ªát
    keywords = ["yesterday", "today", "h√¥m qua", "h√¥m nay", "v·ª´a xong", "just now", "mins", "hrs", "ƒë√£ ƒëƒÉng"]
    if any(k in clean_text for k in keywords): return True
    
    return False

def calculate_publish_time(raw_time_text):
    """
    Chuy·ªÉn ƒë·ªïi text Facebook (3h, 5m, Yesterday) th√†nh th·ªùi gian th·ª±c (ISO Format)
    """
    if not raw_time_text: return None
    
    now = datetime.now()
    text = raw_time_text.lower().strip()
    
    try:
        # 1. X·ª≠ l√Ω ph√∫t (m, min, ph√∫t)
        match = re.search(r'^(\d+)\s*[m|min|ph√∫t]', text)
        if match:
            minutes = int(match.group(1))
            return (now - timedelta(minutes=minutes)).isoformat()

        # 2. X·ª≠ l√Ω gi·ªù (h, hr, gi·ªù)
        match = re.search(r'^(\d+)\s*[h|hr|gi·ªù]', text)
        if match:
            hours = int(match.group(1))
            return (now - timedelta(hours=hours)).isoformat()

        # 3. X·ª≠ l√Ω ng√†y (d, day, ng√†y)
        match = re.search(r'^(\d+)\s*[d|day|ng√†y]', text)
        if match:
            days = int(match.group(1))
            return (now - timedelta(days=days)).isoformat()
            
        # 4. X·ª≠ l√Ω "H√¥m qua" / "Yesterday"
        if "yesterday" in text or "h√¥m qua" in text:
            return (now - timedelta(days=1)).isoformat()
            
        # 5. X·ª≠ l√Ω nƒÉm (1y)
        match = re.search(r'^(\d+)\s*y', text)
        if match:
            years = int(match.group(1))
            return (now - timedelta(days=years*365)).isoformat()

    except:
        pass
        
    return None

def expand_all(driver):
    try:
        btns = driver.find_elements(By.XPATH, "//span[contains(text(), 'See more') or contains(text(), 'Xem th√™m') or text()='‚Ä¶']")
        if btns:
            for btn in btns:
                try: driver.execute_script("arguments[0].click();", btn)
                except: pass
            time.sleep(2) 
    except: pass

def scroll_and_wait_for_content(driver, min_posts=20, max_attempts=15, scroll_pause_time=3):
    """
    Cu·ªôn trang ƒë·ªÉ t·∫£i th√™m b√†i vi·∫øt (lazy loading).
    Pattern: repeat: down -> up (half-page) until no more new posts
    Tr·∫£ v·ªÅ s·ªë b√†i vi·∫øt ƒë√£ ph√°t hi·ªán.
    """
    console.print(f"  [cyan]üìú ƒêang cu·ªôn trang ƒë·ªÉ t·∫£i b√†i vi·∫øt (m·ª•c ti√™u: {min_posts} b√†i)...[/cyan]")
    
    scroll_count = 0
    no_new_content_count = 0
    last_height = 0
    
    def count_posts():
        try:
            stream_container = driver.find_element(By.XPATH, "//div[@data-type='vscroller']")
            blocks = stream_container.find_elements(By.XPATH, "./div")
            return len([b for b in blocks if 'data-testid="post-profile-image' in b.get_attribute('outerHTML')])
        except:
            return 0
    
    def get_page_height():
        return driver.execute_script("return document.body.scrollHeight")
    
    def get_current_scroll():
        return driver.execute_script("return window.pageYOffset")
    
    def smooth_scroll(target_y, duration=1.0):
        """Cu·ªôn m∆∞·ª£t ƒë·∫øn v·ªã tr√≠ target_y trong kho·∫£ng duration gi√¢y"""
        start_y = get_current_scroll()
        distance = target_y - start_y
        steps = 20  # S·ªë b∆∞·ªõc cu·ªôn
        step_delay = duration / steps
        
        for i in range(1, steps + 1):
            # Easing function (ease-out)
            progress = i / steps
            eased_progress = 1 - (1 - progress) ** 2
            new_y = start_y + (distance * eased_progress)
            driver.execute_script(f"window.scrollTo(0, {int(new_y)});")
            time.sleep(step_delay)
    
    def scroll_to_bottom():
        target = get_page_height()
        smooth_scroll(target, duration=1.5)
    
    def scroll_to_half():
        target = get_page_height() // 2
        smooth_scroll(target, duration=1.0)
    
    while scroll_count < max_attempts:
        current_posts = count_posts()
        
        if current_posts >= min_posts:
            console.print(f"  [bold green]‚úÖ ƒê√£ t·∫£i ƒë·ªß {current_posts} b√†i vi·∫øt![/bold green]")
            break
        
        scroll_count += 1
        
        # === B∆Ø·ªöC 1: Cu·ªôn xu·ªëng cu·ªëi trang ===
        scroll_to_bottom()
        time.sleep(scroll_pause_time)
        
        # === B∆Ø·ªöC 2: Cu·ªôn l√™n n·ª≠a trang ===
        scroll_to_half()
        time.sleep(1)
        
        # M·ªü r·ªông c√°c b√†i vi·∫øt b·ªã thu g·ªçn
        expand_all(driver)
        
        # Ki·ªÉm tra chi·ªÅu cao m·ªõi
        new_height = get_page_height()
        current_posts = count_posts()
        
        console.print(f"    [dim]L·∫ßn cu·ªôn {scroll_count}: ƒë√£ t√¨m th·∫•y ~{current_posts} b√†i vi·∫øt[/dim]")
        
        if new_height == last_height:
            no_new_content_count += 1
            if no_new_content_count >= 3:
                console.print(f"  [yellow]‚ö†Ô∏è Kh√¥ng c√≤n n·ªôi dung m·ªõi sau {scroll_count} l·∫ßn cu·ªôn.[/yellow]")
                break
        else:
            no_new_content_count = 0
            last_height = new_height
    
    return count_posts()


def parse_stream(driver, page_name_slug):
    posts_data = []
    
    try:
        stream_container = driver.find_element(By.XPATH, "//div[@data-type='vscroller']")
        blocks = stream_container.find_elements(By.XPATH, "./div")
    except:
        return []

    current_post = {
        "text": [], "images": [], "videos": [], 
        "likes": 0, "comments": 0, "shares": 0,
        "time_text": "",
        "has_avatar": False
    }

    for block in blocks:
        html_block = block.get_attribute('outerHTML')
        
        # 1. B·∫ÆT ƒê·∫¶U B√ÄI M·ªöI (D·ª±a v√†o Avatar)
        if 'data-testid="post-profile-image' in html_block:
            if current_post["has_avatar"] and (current_post["text"] or current_post["images"]):
                posts_data.append(current_post)
            
            current_post = {
                "text": [], "images": [], "videos": [],
                "likes": 0, "comments": 0, "shares": 0,
                "time_text": "",
                "has_avatar": True
            }
            
            # ===== T√åM TH·ªúI GIAN NGAY TRONG BLOCK AVATAR =====
            # Facebook th∆∞·ªùng ƒë·∫∑t timestamp g·∫ßn avatar (·ªü header c·ªßa post)
            try:
                # Ph∆∞∆°ng ph√°p 1: T√¨m trong abbr tag (c√≥ th·ªÉ c√≥ data-utime)
                abbr_elems = block.find_elements(By.TAG_NAME, "abbr")
                for abbr in abbr_elems:
                    time_attr = abbr.get_attribute("data-utime") or abbr.get_attribute("title")
                    if time_attr:
                        current_post["time_text"] = time_attr
                        break
                
                # Ph∆∞∆°ng ph√°p 2: T√¨m c√°c span/div ch·ª©a text th·ªùi gian
                if not current_post["time_text"]:
                    time_candidates = block.find_elements(By.XPATH, ".//span | .//a")
                    for tc in time_candidates:
                        raw_t = driver.execute_script("return arguments[0].textContent;", tc).strip()
                        if raw_t and is_timestamp(raw_t):
                            current_post["time_text"] = clean_time_string(raw_t)
                            break
            except:
                pass
            continue

        if not current_post["has_avatar"]: continue

        # 2. QU√âT TEXT AREA (X·ª≠ l√Ω c·∫£ Time label l·∫´n Content t·∫°i ƒë√¢y)
        try:
            # T√¨m t·∫•t c·∫£ TextArea (v√¨ tr√™n mobile, time label c≈©ng n·∫±m trong TextArea)
            text_elems = block.find_elements(By.XPATH, ".//div[contains(@data-mcomponent, 'TextArea')]")
            
            for elem in text_elems:
                # L·∫•y text th√¥ (raw)
                raw_txt = driver.execute_script("return arguments[0].textContent;", elem).strip()
                if not raw_txt: continue
                
                # --- CHECK XEM C√ì PH·∫¢I L√Ä TIME KH√îNG ---
                # Ch·ªâ check n·∫øu b√†i hi·ªán t·∫°i ch∆∞a c√≥ time
                if not current_post["time_text"]:
                    if is_timestamp(raw_txt):
                        # L√†m s·∫°ch (b·ªè icon tr√°i ƒë·∫•t, b·ªè k√Ω t·ª± ·∫©n)
                        clean_t = clean_time_string(raw_txt)
                        current_post["time_text"] = clean_t
                        # N·∫øu ƒë√£ l√† time th√¨ b·ªè qua, kh√¥ng add v√†o content
                        continue 
                # ---------------------------------------

                # N·∫æU KH√îNG PH·∫¢I TIME TH√å L√Ä CONTENT
                txt_lower = raw_txt.lower()
                
                # Logic l·ªçc r√°c c≈© c·ªßa b·∫°n
                if page_name_slug.lower() in txt_lower and len(raw_txt) < 50:
                    continue # B·ªè qua t√™n page l·∫∑p l·∫°i
                
                if "comment" not in txt_lower and "share" not in txt_lower:
                    clean_txt = raw_txt.replace("... See more", "").replace("... Xem th√™m", "")
                    current_post["text"].append(clean_txt)
        except: pass

        # 3. ·∫¢NH/VIDEO
        try:
            imgs = block.find_elements(By.TAG_NAME, "img")
            for img in imgs:
                alt = img.get_attribute("alt")
                if alt and len(alt) > 15 and "profile picture" not in alt.lower():
                    current_post["images"].append(alt)
            
            if 'data-type="video"' in html_block or 'aria-label="Video player"' in html_block:
                current_post["videos"].append("Video")
        except: pass

        # 4. STATS
        try:
            btns = block.find_elements(By.XPATH, ".//div[@role='button']")
            for btn in btns:
                label = btn.get_attribute("aria-label")
                if not label: continue
                
                if 'like' in label.lower() and 'comment' not in label.lower():
                    current_post["likes"] = clean_number(label)
                elif 'comment' in label.lower():
                    current_post["comments"] = clean_number(label)
                elif 'share' in label.lower():
                    current_post["shares"] = clean_number(label)
        except: pass

    if current_post["has_avatar"] and (current_post["text"] or current_post["images"]):
        posts_data.append(current_post)

    return posts_data

def save_to_json(new_posts, page_name, file_output):
    if not new_posts: return 0
    clean_data = []
    crawl_timestamp = datetime.now().isoformat()
    
    for p in new_posts:
        full_text = "\n".join(p["text"])
        
        # B·ªè qua b√†i qu√° ng·∫Øn v√† kh√¥ng c√≥ ·∫£nh/video
        if len(full_text) < 5 and not p["images"] and not p["videos"]:
            continue
            
        # --- T√çNH TO√ÅN GI·ªú ƒêƒÇNG ---
        calculated_time = calculate_publish_time(p["time_text"])
        if not calculated_time:
            calculated_time = crawl_timestamp # Fallback n·∫øu kh√¥ng t√≠nh ƒë∆∞·ª£c
        # --------------------------

        post_obj = {
            "page_name": page_name,
            "published_time": calculated_time,
            "crawl_time": crawl_timestamp,
            "time_label": p["time_text"],      # Label ƒë√£ ƒë∆∞·ª£c clean (VD: 4h)
            "content": full_text,
            "media": {
                "images": p["images"],
                "videos": p["videos"]
            },
            "stats": {
                "likes": p["likes"],
                "comments": p["comments"],
                "shares": p["shares"]
            }
        }
        clean_data.append(post_obj)
        
        console.print(f"    [green]‚úÖ[/green] [bold]POST[/bold] Time: [cyan]{p['time_text']}[/cyan] | {full_text[:40]}...")

    try:
        with open(file_output, 'r', encoding='utf-8') as f:
            try: current_data = json.load(f)
            except: current_data = []
        
        current_data.extend(clean_data)
        
        with open(file_output, 'w', encoding='utf-8') as f:
            json.dump(current_data, f, ensure_ascii=False, indent=4)
            
    except Exception as e:
        console.print(f"[bold red]‚ùå L·ªói ghi file: {e}[/bold red]")

    return len(clean_data)

def create_browser(browser_type, profile_path=None):
    """
    T·∫°o browser driver d·ª±a tr√™n lo·∫°i browser ƒë∆∞·ª£c ch·ªçn.
    H·ªó tr·ª£: firefox, chrome, chromium
    """
    # Use iOS user agent instead of Android to avoid intent:// redirects
    # iOS doesn't have intent protocol, so Facebook won't try to open mobile app
        # mobile_ua = "Mozilla/5.0 (Linux; Android 11; SAMSUNG SM-G973U) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/14.2 Chrome/87.0.4280.141 Mobile Safari/537.36"
    mobile_ua = "Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1"
    if browser_type == "firefox":
        console.print("[yellow]ü¶ä ƒêang kh·ªüi ƒë·ªông Firefox...[/yellow]")
        options = FirefoxOptions()
        options.set_preference("general.useragent.override", mobile_ua)
        
        if profile_path:
            abs_profile_path = os.path.abspath(profile_path)
            if not os.path.exists(abs_profile_path):
                console.print(f"[dim]üìÅ T·∫°o m·ªõi profile folder t·∫°i: {abs_profile_path}[/dim]")
                os.makedirs(abs_profile_path)
            options.add_argument("-profile")
            options.add_argument(abs_profile_path)
        
        return webdriver.Firefox(options=options)
    
    elif browser_type in ["chrome", "chromium"]:
        browser_name = "Chrome" if browser_type == "chrome" else "Chromium"
        console.print(f"[yellow]üåê ƒêang kh·ªüi ƒë·ªông {browser_name}...[/yellow]")
        
        options = ChromeOptions()
        options.add_argument(f"--user-agent={mobile_ua}")
        options.add_argument("--disable-blink-features=AutomationControlled")
        
        # Prevent Facebook from redirecting to intent:// (Android app) URLs
        options.add_argument("--disable-external-intents-redirect")
        options.add_argument("--disable-popup-blocking")
        options.add_argument("--disable-notifications")
        options.add_argument("--no-default-browser-check")
        options.add_argument("--ignore-certificate-errors")
        
        # Disable Chrome's built-in intent handling
        prefs = {
            "protocol_handler.excluded_schemes": {
                "intent": True
            },
            "profile.default_content_setting_values.notifications": 2
        }
        options.add_experimental_option("prefs", prefs)
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option("useAutomationExtension", False)
        
        # Cho Chromium, c·∫ßn ch·ªâ ƒë·ªãnh ƒë∆∞·ªùng d·∫´n binary
        if browser_type == "chromium":
            # Common Chromium paths on Linux
            chromium_paths = [
                "/usr/bin/chromium",
                "/usr/bin/chromium-browser",
                "/snap/bin/chromium",
                "/usr/lib/chromium/chromium",
            ]
            for path in chromium_paths:
                if os.path.exists(path):
                    options.binary_location = path
                    break
            else:
                console.print("[yellow]‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y Chromium, th·ª≠ d√πng Chrome...[/yellow]")
        
        if profile_path:
            abs_profile_path = os.path.abspath(profile_path)
            if not os.path.exists(abs_profile_path):
                console.print(f"[dim]üìÅ T·∫°o m·ªõi profile folder t·∫°i: {abs_profile_path}[/dim]")
                os.makedirs(abs_profile_path)
            options.add_argument(f"--user-data-dir={abs_profile_path}")
        
        return webdriver.Chrome(options=options)
    
    else:
        raise ValueError(f"Browser kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {browser_type}. Ch·ªçn: firefox, chrome, chromium")

def run_crawler(browser_type="firefox", target_urls=None, file_output="fb_data.json", min_posts=20, max_scrolls=15, scroll_pause=3, profile_path=None):
    if target_urls is None:
        target_urls = []

    init_json(file_output) # kh·ªüi t·∫°o json
    
    browser_display = {
        "firefox": "Firefox ü¶ä",
        "chrome": "Chrome üåê", 
        "chromium": "Chromium üåê"
    }.get(browser_type, browser_type)
    
    console.print(Panel.fit(
        f"[bold cyan]Facebook Crawler[/bold cyan]\n[dim]Using {browser_display} + Selenium[/dim]",
        border_style="blue"
    ))
    
    # T·∫°o profile path ri√™ng cho m·ªói browser n·∫øu ch∆∞a c√≥
    if not profile_path:
        profile_path = f"my_{browser_type}_profile"
        
    driver = create_browser(browser_type, profile_path)
    
    try:
        console.print("[cyan]üåê ƒêang v√†o Facebook...[/cyan]")
        driver.get("https://m.facebook.com")
        time.sleep(3)
        
        # Ki·ªÉm tra ƒëƒÉng nh·∫≠p - check nhi·ªÅu ƒëi·ªÅu ki·ªán ƒë·ªÉ ho·∫°t ƒë·ªông v·ªõi m·ªçi browser
        def is_logged_in():
            current_url = driver.current_url.lower()
            page_source = driver.page_source.lower()
            
            # Ch∆∞a ƒëƒÉng nh·∫≠p n·∫øu:
            # 1. URL ch·ª©a "login" ho·∫∑c "checkpoint"
            # 2. Trang c√≥ form ƒëƒÉng nh·∫≠p
            # 3. Trang c√≥ n√∫t "Log In" ho·∫∑c "ƒêƒÉng nh·∫≠p"
            login_url_keywords = ["login", "checkpoint", "recover", "identify"]
            if any(kw in current_url for kw in login_url_keywords):
                return False
            
            # Ki·ªÉm tra c√≥ form login kh√¥ng
            login_indicators = [
                'name="email"',
                'name="pass"', 
                'id="loginbutton"',
                'data-sigil="login_button"',
                'data-sigil="m_login_button"'
            ]
            if any(indicator in page_source for indicator in login_indicators):
                return False
                
            return True
        
        if not is_logged_in():
            console.print("[bold yellow]‚ö†Ô∏è CH∆ØA ƒêƒÇNG NH·∫¨P![/bold yellow]")
            console.print("[yellow]üëâ H√£y ƒëƒÉng nh·∫≠p Facebook th·ªß c√¥ng trong c·ª≠a s·ªï tr√¨nh duy·ªát.[/yellow]")
            console.print("[yellow]üëâ Sau khi ƒëƒÉng nh·∫≠p xong, quay l·∫°i ƒë√¢y v√† b·∫•m Enter ƒë·ªÉ ti·∫øp t·ª•c...[/yellow]")
            input()
            
            # ƒê·ª£i th√™m sau khi user b·∫•m Enter ƒë·ªÉ ƒë·∫£m b·∫£o trang ƒë√£ load xong
            time.sleep(2)
        
        for url in target_urls:
            page_name_slug = url.split('/')[-1]
            
            console.print()
            console.rule(f"[bold blue]{page_name_slug}[/bold blue]")
            
            driver.get(url)
            print(driver.current_url)

            time.sleep(5)
            
            # Cu·ªôn trang ƒë·ªÉ t·∫£i th√™m b√†i vi·∫øt (lazy loading)
            scroll_and_wait_for_content(driver, min_posts, max_scrolls, scroll_pause)
            
            # M·ªü r·ªông t·∫•t c·∫£ b√†i vi·∫øt tr∆∞·ªõc khi parse
            expand_all(driver)
            
            console.print("  [cyan]üîÑ ƒêang ph√¢n t√≠ch lu·ªìng b√†i vi·∫øt...[/cyan]")
            posts = parse_stream(driver, page_name_slug)
            
            saved = save_to_json(posts, page_name_slug, file_output)
            console.print(f"  [bold green]üèÅ ƒê√£ l∆∞u {saved} b√†i v√†o {file_output}.[/bold green]")

    except Exception as e:
        console.print(f"[bold red]‚ùå L·ªói Critical: {e}[/bold red]")
    finally:
        console.print()
        console.print(Panel.fit("[bold]üõë K·∫øt th√∫c session.[/bold]", border_style="red"))
        # driver.quit() # M·ªü d√≤ng n√†y n·∫øu mu·ªën t·ª± ƒë·ªông ƒë√≥ng tr√¨nh duy·ªát

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Facebook Crawler - Crawl posts from Facebook pages",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
            V√≠ d·ª• s·ª≠ d·ª•ng:
            python fb_crawl_final.py                    # M·∫∑c ƒë·ªãnh d√πng Firefox, crawl URLs m·∫∑c ƒë·ªãnh
            python fb_crawl_final.py --browser chrome   # D√πng Chrome
            python fb_crawl_final.py --urls https://www.facebook.com/kienthuc.net.vn
            python fb_crawl_final.py --min-posts 50 --max-scrolls 20
        """
    )
    
    default_urls = [
        "https://m.facebook.com/Theanh28",
        "https://m.facebook.com/kienkhongngu.vn", 
        "https://m.facebook.com/thongtinchinhphu",    # Vietnam News (TTXVN)
        "https://m.facebook.com/VnExpress",           # VnExpress
        "https://m.facebook.com/baotuoitre",          # Tu·ªïi Tr·∫ª
        "https://m.facebook.com/thanhnien",           # Thanh Ni√™n
        "https://m.facebook.com/vietnamnet.vn",       # Vietnamnet
        "https://m.facebook.com/baodantridientu",     # D√¢n Tr√≠
        "https://m.facebook.com/laodongonline",       # B√°o Lao ƒê·ªông
        "https://m.facebook.com/nhandanonline",       # B√°o Nh√¢n D√¢n
        "https://m.facebook.com/profile.php?id=100089883616175",
        # "https://m.facebook.com/hhsb.vn/",
        "https://m.facebook.com/tintucvietnammoinong/",
        "https://www.facebook.com/tintucvtv24",
        "https://www.facebook.com/doisongvnn",
        "https://www.facebook.com/VnProCon",
        "https://www.facebook.com/tapchitrithucznews.vn",
    ]

    parser.add_argument("-b", "--browser", type=str, choices=["firefox", "chrome", "chromium"], default="firefox", help="Ch·ªçn tr√¨nh duy·ªát (m·∫∑c ƒë·ªãnh: firefox)")
    parser.add_argument("-u", "--urls", type=str, nargs="+", default=default_urls, help="Danh s√°ch URL c·∫ßn crawl")
    parser.add_argument("-o", "--output", type=str, default="fb_data.json", help="ƒê∆∞·ªùng d·∫´n file output JSON")
    parser.add_argument("--min-posts", type=int, default=500, help="S·ªë b√†i vi·∫øt t·ªëi thi·ªÉu m·ªói page")
    parser.add_argument("--max-scrolls", type=int, default=200, help="S·ªë l·∫ßn scroll t·ªëi ƒëa")
    parser.add_argument("--scroll-pause", type=int, default=3, help="Th·ªùi gian ch·ªù sau m·ªói l·∫ßn scroll (gi√¢y)")
    parser.add_argument("--profile-path", type=str, default="my_firefox_profile", help="ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c profile (n·∫øu kh√¥ng set, t·ª± ƒë·ªông t·∫°o theo browser)")
    
    args = parser.parse_args()

    # X·ª≠ l√Ω ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi cho output n·∫øu c·∫ßn
    script_dir = os.path.dirname(os.path.abspath(__file__))
    file_output = args.output
    if not os.path.isabs(file_output):
        file_output = os.path.join(script_dir, file_output)

    run_crawler(
        browser_type=args.browser,
        target_urls=args.urls,
        file_output=file_output,
        min_posts=args.min_posts,
        max_scrolls=args.max_scrolls,
        scroll_pause=args.scroll_pause,
        profile_path=args.profile_path
    )