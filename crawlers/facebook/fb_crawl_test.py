import time
import json
import os
import re
import argparse
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.firefox.options import Options as FirefoxOptions
from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.firefox.service import Service as FirefoxService
from selenium.common.exceptions import StaleElementReferenceException
from selenium.webdriver.common.by import By
from rich.console import Console
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
from rich.table import Table
from rich import print as rprint

console = Console()

# ================= C·∫§U H√åNH V√Ä HELPER =================

def init_json(file_output):
    if not os.path.exists(file_output):
        with open(file_output, 'w', encoding='utf-8') as f:
            json.dump([], f, ensure_ascii=False, indent=4)

def clean_number(text):
    if not text: return 0
    match = re.search(r'(\d+[.,]?\d*[MK]?)', text)
    if not match: return 0
    
    num_str = match.group(1).replace(',', '.')
    multiplier = 1
    if 'K' in num_str:
        multiplier = 1000
        num_str = num_str.replace('K', '')
    elif 'M' in num_str:
        multiplier = 1000000
        num_str = num_str.replace('M', '')
        
    try:
        return int(float(num_str) * multiplier)
    except:
        return 0

def clean_time_string(text):
    if not text: return ""
    clean = re.sub(r'[^\w\s,.]', '', text).strip()
    match = re.match(r'^([\w\s,.]+)', clean)
    if match:
        return match.group(1).strip()
    return clean

def is_timestamp(text):
    clean_text = clean_time_string(text).lower()
    if not clean_text: return False
    if len(clean_text) > 30: return False 

    if re.match(r'^\d+\s*[wmhdys]$', clean_text): return True
    if re.match(r'^\d+\s*(hrs?|mins?|days?|weeks?|months?|years?)$', clean_text): return True
    if re.match(r'^\d+\s*(gi·ªù|ph√∫t|ng√†y|nƒÉm|th√°ng|tu·∫ßn)', clean_text): return True
    if re.match(r'^\d{1,2}\s+thg\s+\d{1,2}.*', clean_text): return True
    if re.match(r'^[a-z]{3,9}\s\d{1,2}.*', clean_text): return True
    if re.search(r'(gi·ªù|ph√∫t|ng√†y|tu·∫ßn|th√°ng|nƒÉm|hour|minute|day|week|month|year)s?\s*(tr∆∞·ªõc|ago)', clean_text): return True
    
    keywords = ["yesterday", "today", "h√¥m qua", "h√¥m nay", "v·ª´a xong", "just now", "mins", "hrs", "ƒë√£ ƒëƒÉng"]
    if any(k in clean_text for k in keywords): return True
    
    return False

def calculate_publish_time(raw_time_text):
    if not raw_time_text: return None
    now = datetime.now()
    text = raw_time_text.lower().strip()
    try:
        match = re.search(r'^(\d+)\s*[m|min|ph√∫t]', text)
        if match: return (now - timedelta(minutes=int(match.group(1)))).isoformat()

        match = re.search(r'^(\d+)\s*[h|hr|gi·ªù]', text)
        if match: return (now - timedelta(hours=int(match.group(1)))).isoformat()

        match = re.search(r'^(\d+)\s*[d|day|ng√†y]', text)
        if match: return (now - timedelta(days=int(match.group(1)))).isoformat()
            
        if "yesterday" in text or "h√¥m qua" in text:
            return (now - timedelta(days=1)).isoformat()
            
        match = re.search(r'^(\d+)\s*y', text)
        if match: return (now - timedelta(days=int(match.group(1))*365)).isoformat()
    except: pass
    return None

def expand_all(driver):
    try:
        btns = driver.find_elements(By.XPATH, "//span[contains(text(), 'See more') or contains(text(), 'Xem th√™m') or text()='‚Ä¶']")
        if btns:
            for btn in btns:
                try: driver.execute_script("arguments[0].click();", btn)
                except: pass
            time.sleep(2) 
    except: pass

def parse_stream(driver, page_name_slug):
    posts_data = []
    
    try:
        stream_container = driver.find_element(By.XPATH, "//div[@data-type='vscroller']")
        blocks = stream_container.find_elements(By.XPATH, "./div")
    except:
        return []

    current_post = {
        "text": [], "images": [], "videos": [], 
        "likes": 0, "comments": 0, "shares": 0,
        "time_text": "", "has_avatar": False
    }

    for block in blocks:
        try:
            # === B·∫ÆT ƒê·∫¶U V√ôNG AN TO√ÄN (B·ªè qua n·∫øu Element b·ªã Facebook x√≥a) ===
            html_block = block.get_attribute('outerHTML')
            
            # 1. B·∫ÆT ƒê·∫¶U B√ÄI M·ªöI (D·ª±a v√†o Avatar)
            if 'data-testid="post-profile-image' in html_block:
                if current_post["has_avatar"] and (current_post["text"] or current_post["images"]):
                    posts_data.append(current_post)
                
                current_post = {
                    "text": [], "images": [], "videos": [],
                    "likes": 0, "comments": 0, "shares": 0,
                    "time_text": "", "has_avatar": True
                }
                
                # T√¨m th·ªùi gian
                try:
                    abbr_elems = block.find_elements(By.TAG_NAME, "abbr")
                    for abbr in abbr_elems:
                        time_attr = abbr.get_attribute("data-utime") or abbr.get_attribute("title")
                        if time_attr:
                            current_post["time_text"] = time_attr
                            break
                    
                    if not current_post["time_text"]:
                        time_candidates = block.find_elements(By.XPATH, ".//span | .//a")
                        for tc in time_candidates:
                            raw_t = driver.execute_script("return arguments[0].textContent;", tc).strip()
                            if raw_t and is_timestamp(raw_t):
                                current_post["time_text"] = clean_time_string(raw_t)
                                break
                except: pass
                continue

            if not current_post["has_avatar"]: continue

            # 2. QU√âT TEXT
            try:
                text_elems = block.find_elements(By.XPATH, ".//div[contains(@data-mcomponent, 'TextArea')]")
                for elem in text_elems:
                    raw_txt = driver.execute_script("return arguments[0].textContent;", elem).strip()
                    if not raw_txt: continue
                    
                    if not current_post["time_text"]:
                        if is_timestamp(raw_txt):
                            current_post["time_text"] = clean_time_string(raw_txt)
                            continue 

                    txt_lower = raw_txt.lower()
                    if page_name_slug.lower() in txt_lower and len(raw_txt) < 50: continue
                    
                    if "comment" not in txt_lower and "share" not in txt_lower:
                        clean_txt = raw_txt.replace("... See more", "").replace("... Xem th√™m", "")
                        current_post["text"].append(clean_txt)
            except: pass

            # 3. QU√âT MEDIA
            try:
                imgs = block.find_elements(By.TAG_NAME, "img")
                for img in imgs:
                    alt = img.get_attribute("alt")
                    if alt and len(alt) > 15 and "profile picture" not in alt.lower():
                        current_post["images"].append(alt)
                
                if 'data-type="video"' in html_block or 'aria-label="Video player"' in html_block:
                    current_post["videos"].append("Video")
            except: pass

            # 4. QU√âT STATS
            try:
                btns = block.find_elements(By.XPATH, ".//div[@role='button']")
                for btn in btns:
                    label = btn.get_attribute("aria-label")
                    if not label: continue
                    
                    if 'like' in label.lower() and 'comment' not in label.lower():
                        current_post["likes"] = clean_number(label)
                    elif 'comment' in label.lower():
                        current_post["comments"] = clean_number(label)
                    elif 'share' in label.lower():
                        current_post["shares"] = clean_number(label)
            except: pass
            
            # === K·∫æT TH√öC V√ôNG AN TO√ÄN ===

        except StaleElementReferenceException:
            # N·∫øu element n√†y b·ªã l·ªói (do FB refresh DOM), b·ªè qua v√† ƒëi ti·∫øp
            continue

    if current_post["has_avatar"] and (current_post["text"] or current_post["images"]):
        posts_data.append(current_post)

    return posts_data

def save_to_json(new_posts, page_name, file_output):
    if not new_posts: return 0
    clean_data = []
    crawl_timestamp = datetime.now().isoformat()
    
    for p in new_posts:
        full_text = "\n".join(p["text"])
        
        if len(full_text) < 5 and not p["images"] and not p["videos"]:
            continue
            
        calculated_time = calculate_publish_time(p["time_text"])
        if not calculated_time:
            calculated_time = crawl_timestamp 

        post_obj = {
            "page_name": page_name,
            "published_time": calculated_time,
            "crawl_time": crawl_timestamp,
            "time_label": p["time_text"],
            "content": full_text,
            "media": {
                "images": p["images"],
                "videos": p["videos"]
            },
            "stats": {
                "likes": p["likes"],
                "comments": p["comments"],
                "shares": p["shares"]
            }
        }
        clean_data.append(post_obj)
        console.print(f"    [green]‚úÖ[/green] [bold]L∆ØU POST[/bold] | {full_text[:40]}...")

    try:
        with open(file_output, 'r', encoding='utf-8') as f:
            try: current_data = json.load(f)
            except: current_data = []
        
        current_data.extend(clean_data)
        
        with open(file_output, 'w', encoding='utf-8') as f:
            json.dump(current_data, f, ensure_ascii=False, indent=4)
            
    except Exception as e:
        console.print(f"[bold red]‚ùå L·ªói ghi file: {e}[/bold red]")

    return len(clean_data)

# ================= CORE LOGIC ƒê∆Ø·ª¢C S·ª¨A ƒê·ªîI =================

def process_page_scrolling(driver, page_name_slug, file_output, min_posts, max_attempts, scroll_pause_time):
    console.print(f"  [cyan]üìú ƒêang qu√©t b√†i vi·∫øt (M·ª•c ti√™u: {min_posts} b√†i)...[/cyan]")
    
    total_saved = 0
    processed_signatures = set()
    last_height = driver.execute_script("return document.body.scrollHeight")
    stuck_count = 0 # ƒê·∫øm s·ªë l·∫ßn b·ªã k·∫πt li√™n ti·∫øp

    # Helper: Cu·ªôn xu·ªëng t·ª´ t·ª´ thay v√¨ nh·∫£y c√≥c
    def human_scroll_down(step_delay=0.5):
        # L·∫•y chi·ªÅu cao m√†n h√¨nh hi·ªán t·∫°i
        viewport_height = driver.execute_script("return window.innerHeight")
        current_pos = driver.execute_script("return window.pageYOffset")
        doc_height = driver.execute_script("return document.body.scrollHeight")
        
        # Cu·ªôn t·ª´ng ƒëo·∫°n b·∫±ng 80% chi·ªÅu cao m√†n h√¨nh
        while current_pos < doc_height:
            current_pos += int(viewport_height * 0.8)
            driver.execute_script(f"window.scrollTo(0, {current_pos});")
            time.sleep(step_delay) # Ngh·ªâ ng·∫Øn gi·ªØa m·ªói c√∫ vu·ªët
            
            # C·∫≠p nh·∫≠t l·∫°i doc_height v√¨ c√≥ th·ªÉ FB ƒë√£ load th√™m ngay trong l√∫c ƒëang vu·ªët
            new_doc_height = driver.execute_script("return document.body.scrollHeight")
            if new_doc_height > doc_height:
                doc_height = new_doc_height
            
            # N·∫øu ƒë√£ ch·∫°m ƒë√°y th·ª±c t·∫ø
            if current_pos >= doc_height:
                break

    # Helper: Thao t√°c gi·∫£i c·ª©u khi b·ªã k·∫πt
    def unstuck_maneuver():
        console.print("    [yellow]‚ö†Ô∏è C√≥ v·∫ª b·ªã k·∫πt, ƒëang th·ª≠ cu·ªôn ng∆∞·ª£c ƒë·ªÉ k√≠ch ho·∫°t...[/yellow]")
        # Cu·ªôn l√™n 1 kho·∫£ng kh√° xa (kho·∫£ng 3 m√†n h√¨nh)
        driver.execute_script("window.scrollBy(0, -1500);")
        time.sleep(2)
        # Cu·ªôn t·ª´ t·ª´ xu·ªëng l·∫°i ƒë√°y
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(3)

    loop_count = 0
    while total_saved < min_posts and loop_count < max_attempts:
        loop_count += 1
        
        # 1. TH·ª∞C HI·ªÜN CU·ªòN
        # N·∫øu ƒëang b·ªã k·∫πt, d√πng bi·ªán ph√°p m·∫°nh, n·∫øu kh√¥ng th√¨ cu·ªôn b√¨nh th∆∞·ªùng
        if stuck_count >= 1:
            unstuck_maneuver()
        else:
            human_scroll_down(step_delay=0.2)
        
        time.sleep(scroll_pause_time)
        
        # 2. KI·ªÇM TRA CHI·ªÄU CAO ƒê·ªÇ BI·∫æT C√ì LOAD TH√äM KH√îNG
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            stuck_count += 1
        else:
            stuck_count = 0 # Reset n·∫øu ƒë√£ load ƒë∆∞·ª£c th√™m
            last_height = new_height
            
        # 3. EXPAND & PARSE (Gi·ªØ nguy√™n logic c≈©)
        expand_all(driver)
        raw_posts = parse_stream(driver, page_name_slug)
        
        new_batch = []
        for p in raw_posts:
            # Check tr√πng l·∫∑p
            content_sig = "\n".join(p["text"])[:50]
            unique_key = f"{p['time_text']}_{content_sig}"
            
            if unique_key not in processed_signatures:
                processed_signatures.add(unique_key)
                new_batch.append(p)
        
        if new_batch:
            saved_count = save_to_json(new_batch, page_name_slug, file_output)
            total_saved += saved_count
            # N·∫øu l∆∞u ƒë∆∞·ª£c b√†i m·ªõi, coi nh∆∞ kh√¥ng b·ªã k·∫πt, reset stuck_count
            stuck_count = 0
        
        console.print(f"    [dim]L·∫ßn {loop_count}: Th√™m {len(new_batch)} b√†i. T·ªïng: {total_saved}/{min_posts}. (Stuck: {stuck_count})[/dim]")
        
        if total_saved >= min_posts:
            console.print(f"  [bold green]‚úÖ ƒê√£ thu th·∫≠p ƒë·ªß {total_saved} b√†i![/bold green]")
            break
            
    if total_saved < min_posts:
        console.print(f"  [yellow]‚ö†Ô∏è D·ª´ng v√≤ng l·∫∑p. ƒê√£ l∆∞u: {total_saved}[/yellow]")

    return total_saved

def create_browser(browser_type, profile_path=None):
    mobile_ua = "Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1"
    if browser_type == "firefox":
        console.print("[yellow]ü¶ä ƒêang kh·ªüi ƒë·ªông Firefox...[/yellow]")
        options = FirefoxOptions()
        options.set_preference("general.useragent.override", mobile_ua)
        if profile_path:
            abs_profile_path = os.path.abspath(profile_path)
            if not os.path.exists(abs_profile_path): os.makedirs(abs_profile_path)
            options.add_argument("-profile")
            options.add_argument(abs_profile_path)
        return webdriver.Firefox(options=options)
    
    elif browser_type in ["chrome", "chromium"]:
        browser_name = "Chrome" if browser_type == "chrome" else "Chromium"
        console.print(f"[yellow]üåê ƒêang kh·ªüi ƒë·ªông {browser_name}...[/yellow]")
        options = ChromeOptions()
        options.add_argument(f"--user-agent={mobile_ua}")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument("--disable-external-intents-redirect")
        options.add_argument("--disable-popup-blocking")
        options.add_argument("--disable-notifications")
        options.add_argument("--no-default-browser-check")
        options.add_argument("--ignore-certificate-errors")
        prefs = {"protocol_handler.excluded_schemes": {"intent": True}, "profile.default_content_setting_values.notifications": 2}
        options.add_experimental_option("prefs", prefs)
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option("useAutomationExtension", False)
        
        if browser_type == "chromium":
            chromium_paths = ["/usr/bin/chromium", "/usr/bin/chromium-browser", "/snap/bin/chromium", "/usr/lib/chromium/chromium"]
            for path in chromium_paths:
                if os.path.exists(path):
                    options.binary_location = path
                    break
        
        if profile_path:
            abs_profile_path = os.path.abspath(profile_path)
            if not os.path.exists(abs_profile_path): os.makedirs(abs_profile_path)
            options.add_argument(f"--user-data-dir={abs_profile_path}")
        
        return webdriver.Chrome(options=options)
    else:
        raise ValueError(f"Browser kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {browser_type}")

def run_crawler(browser_type="firefox", target_urls=None, file_output="fb_data.json", min_posts=20, max_scrolls=15, scroll_pause=3, profile_path=None):
    if target_urls is None: target_urls = []
    init_json(file_output)
    
    driver = create_browser(browser_type, profile_path)
    
    try:
        console.print("[cyan]üåê ƒêang v√†o Facebook...[/cyan]")
        driver.get("https://m.facebook.com")
        time.sleep(3)
        
        def is_logged_in():
            current_url = driver.current_url.lower()
            page_source = driver.page_source.lower()
            login_url_keywords = ["login", "checkpoint", "recover", "identify"]
            if any(kw in current_url for kw in login_url_keywords): return False
            login_indicators = ['name="email"', 'name="pass"', 'id="loginbutton"', 'data-sigil="login_button"']
            if any(indicator in page_source for indicator in login_indicators): return False
            return True
        
        if not is_logged_in():
            console.print("[bold yellow]‚ö†Ô∏è CH∆ØA ƒêƒÇNG NH·∫¨P![/bold yellow]")
            console.print("[yellow]üëâ H√£y ƒëƒÉng nh·∫≠p Facebook th·ªß c√¥ng, sau ƒë√≥ quay l·∫°i ƒë√¢y v√† b·∫•m Enter...[/yellow]")
            input()
            time.sleep(2)
        
        for url in target_urls:
            page_name_slug = url.split('/')[-1]
            console.print()
            console.rule(f"[bold blue]{page_name_slug}[/bold blue]")
            
            driver.get(url)
            time.sleep(5)
            
            # G·ªçi h√†m process m·ªõi (V·ª´a scroll v·ª´a l∆∞u)
            process_page_scrolling(
                driver=driver,
                page_name_slug=page_name_slug,
                file_output=file_output,
                min_posts=min_posts,
                max_attempts=max_scrolls,
                scroll_pause_time=scroll_pause
            )

    except Exception as e:
        console.print(f"[bold red]‚ùå L·ªói Critical: {e}[/bold red]")
    finally:
        console.print()
        console.print(Panel.fit("[bold]üõë K·∫øt th√∫c session.[/bold]", border_style="red"))

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Facebook Crawler")
    
    default_urls = [
        "https://m.facebook.com/Theanh28",
        "https://m.facebook.com/kienkhongngu.vn", 
        "https://m.facebook.com/thongtinchinhphu",
        "https://m.facebook.com/VnExpress",
        "https://m.facebook.com/baotuoitre",
        "https://m.facebook.com/thanhnien",
        "https://m.facebook.com/vietnamnet.vn",
        "https://m.facebook.com/baodantridientu",
        "https://m.facebook.com/laodongonline",
        "https://m.facebook.com/nhandanonline",
        "https://m.facebook.com/tintucvietnammoinong/",
        "https://www.facebook.com/tintucvtv24",
        "https://www.facebook.com/doisongvnn",
        "https://www.facebook.com/VnProCon",
        "https://www.facebook.com/tapchitrithucznews.vn",
    ]

    parser.add_argument("-b", "--browser", type=str, choices=["firefox", "chrome", "chromium"], default="firefox")
    parser.add_argument("-u", "--urls", type=str, nargs="+", default=default_urls)
    parser.add_argument("-o", "--output", type=str, default="fb_data.json")
    parser.add_argument("--min-posts", type=int, default=500)
    parser.add_argument("--max-scrolls", type=int, default=200)
    parser.add_argument("--scroll-pause", type=int, default=3)
    parser.add_argument("--profile-path", type=str, default="my_firefox_profile")
    
    args = parser.parse_args()

    script_dir = os.path.dirname(os.path.abspath(__file__))
    file_output = args.output
    if not os.path.isabs(file_output):
        file_output = os.path.join(script_dir, file_output)

    run_crawler(
        browser_type=args.browser,
        target_urls=args.urls,
        file_output=file_output,
        min_posts=args.min_posts,
        max_scrolls=args.max_scrolls,
        scroll_pause=args.scroll_pause,
        profile_path=args.profile_path
    )