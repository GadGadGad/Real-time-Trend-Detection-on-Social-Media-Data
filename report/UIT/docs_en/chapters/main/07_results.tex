\chapter{Results and Analysis}

\section{Clustering Algorithm Selection}
We compared our SAHC approach (using HDBSCAN core) against baselines. HDBSCAN was chosen for its superior Noise handling and Balance.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/trend_tsne.png}
    \caption{t-SNE Visualization of Event Clusters. Colors represent distinct events isolated by the algorithm.} \label{fig:tsne}
\end{figure}

The t-SNE projection in Fig. \ref{fig:tsne} demonstrates the effectiveness of our clustering. Distinct events (colored clusters) are well-separated in the semantic space, while noise points (gray) are scattered and effectively isolated by the algorithm.

\begin{table}
    \caption{Clustering Method Comparison}\label{tab:clustering_select}
    \centering
    \begin{tabular}{l c c c c c}
        \toprule
        \textbf{Method}             & \textbf{Clusters (k)} & \textbf{Noise Pts} & \textbf{Silh. Score} ($\uparrow$) & \textbf{DB Index} ($\downarrow$) & \textbf{Time} ($\downarrow$) \\
        \midrule
        K-Means                     & 253                   & 0                  & 0.024                             & 3.177                            & 8.45s                        \\
        \textbf{HDBSCAN (Baseline)} & \textbf{460}          & 1,984              & 0.109                             & \textbf{2.094}                   & 16.00s                       \\
        BERTopic                    & 223                   & 1,801              & 0.112                             & 2.363                            & 55.72s                       \\
        \textbf{SAHC (Ours)}        & \textbf{315}          & \textbf{985}       & \textbf{0.135}                    & \textbf{1.951}                   & \textbf{18.4s}               \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Analysis:} As shown in Table \ref{tab:clustering_select}, \textbf{K-Means} proved unsuitable for social streaming data as it forces all noise into clusters. \textbf{Baseline HDBSCAN}, while correctly identifying noise, suffered from \textbf{over-fragmentation} (460 clusters) and excessive data rejection (1,984 noise points), often breaking single events into multiple micro-clusters.
Our \textbf{SAHC} approach resolves this by using News Anchors to "bridge" these micro-clusters. This successfully condensed the space into \textbf{315 coherent events} and recovered $\approx 50\%$ of the valid data previously discarded as noise (Noise reduced to 985), significantly boosting the Silhouette Score to \textbf{0.135}.

\section{Quantitative Evaluation (Mini-Ground Truth)}
All label-based clustering metrics (NMI, Purity, BCubed-F1, Entropy) are computed exclusively on the Mini-Ground Truth subset (N=300), as the full dataset does not contain human-annotated event labels.
Table \ref{tab:minigt} reports the performance of our \textbf{default SAHC setting}, while Table \ref{tab:ablation} further shows that the \textbf{full hybrid pipeline} (adding anchoring and LLM refinement) improves NMI from 0.54 to \textbf{0.5978}.

\begin{table}
    \caption{Performance on Mini-Ground Truth (N=300) -- Base SAHC Setting}\label{tab:minigt}
    \centering
    \begin{tabular}{l c l}
        \toprule
        \textbf{Metric}           & \textbf{Value} & \textbf{Interpretation}                   \\
        \midrule
        \textbf{NMI} ($\uparrow$) & \textbf{0.54}  & Good mutual information with human labels \\
        Purity ($\uparrow$)       & 0.65           & High dominant class consistency           \\
        BCubed-F1 ($\uparrow$)    & 0.35           & Balanced precision/recall for clustering  \\
        Entropy ($\downarrow$)    & 4.85           & Lower entropy indicates purer clusters    \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Metric Definitions:} To ensure a rigorous evaluation, we selected metrics that cover different aspects of clustering quality:
\begin{itemize}
    \item \textbf{Normalized Mutual Information (NMI):} Measures the alignment between cluster labels $Y$ and ground truth $C$:
          \begin{equation}
              NMI(Y, C) = \frac{2 \cdot I(Y; C)}{H(Y) + H(C)}
          \end{equation}
          Where $I$ is mutual information and $H$ is entropy.
    \item \textbf{Purity:} Quantifies the dominance of the majority class in each cluster:
          \begin{equation}
              Purity(\Omega, C) = \frac{1}{N} \sum_k \max_j |\omega_k \cap c_j|
          \end{equation}
          Where $N$ is total data points, $\Omega = \{\omega_1, ..., \omega_k\}$ is the set of clusters, and $C = \{c_1, ..., c_j\}$ is the set of ground truth classes.
    \item \textbf{Entropy:} Measures the homogeneity of a cluster (lower is better):
          \begin{equation}
              E(C) = - \sum_{k} \frac{n_k}{n} \log_2 \frac{n_k}{n}
          \end{equation}
          Where $n_k$ is the number of points in cluster $k$. A value close to 0 implies the cluster contains only a single class of events.
    \item \textbf{BCubed-F1:} An element-wise metric that balances cluster homogeneity and completeness. It is the harmonic mean of:
          \begin{itemize}
              \item \textit{BCubed Precision ($P_{b^3}$):} Average per-item precision. For each item $x$, it calculates the fraction of items in $x$'s cluster $C(x)$ that share the same class label $L(x)$:
                    \begin{equation}
                        P_{b^3} = \frac{1}{N} \sum_{x \in X} \frac{|C(x) \cap L(x)|}{|C(x)|}
                    \end{equation}
              \item \textit{BCubed Recall ($R_{b^3}$):} Average per-item recall. For each item $x$, it calculates the fraction of items in $x$'s class $L(x)$ that are grouped into the same cluster $C(x)$:
                    \begin{equation}
                        R_{b^3} = \frac{1}{N} \sum_{x \in X} \frac{|C(x) \cap L(x)|}{|L(x)|}
                    \end{equation}
          \end{itemize}
          \begin{equation}
              F1_{b^3} = 2 \cdot \frac{P_{b^3} \cdot R_{b^3}}{P_{b^3} + R_{b^3}}
          \end{equation}
          Unlike purity, BCubed penalizes both over-merging (low Precision) and over-fragmentation (low Recall).
    \item \textbf{Silhouette Score:} Quantifies how well an object fits its own cluster versus the nearest neighbor cluster:
          \begin{equation}
              S(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}
          \end{equation}
          Where $a(i)$ is the mean intra-cluster distance and $b(i)$ is the mean nearest-cluster distance. $S \in [-1, 1]$, where values closer to 1 indicate dense, well-separated clusters.
\end{itemize}

\section{Batch Evaluation and Component Analysis}
To quantify the contribution of each module, we conduct an ablation study on the \textbf{Mini-Ground Truth} subset (N=300), where label-based metrics (NMI, BCubed-F1, Entropy) are well-defined.
Table \ref{tab:ablation} shows that progressively adding anchoring and LLM refinement improves clustering agreement with human labels.

\begin{table}
    \caption{Ablation Study on Mini-Ground Truth (N=300)}\label{tab:ablation}
    \centering
    \begin{tabular}{l c c c}
        \toprule
        \textbf{Configuration}    & \textbf{NMI} ($\uparrow$) & \textbf{BCubed-F1} ($\uparrow$) & \textbf{Entropy} ($\downarrow$) \\
        \midrule
        Baseline (HDBSCAN only)   & 0.53                      & 0.31                            & 4.91                            \\
        \textbf{Full Pipeline}    & \textbf{0.5978}           & \textbf{0.3821}                 & \textbf{4.4178}                 \\
        \quad w/o News Anchors    & 0.48                      & 0.27                            & 5.12                            \\
        \quad w/o Heuristic Guard & 0.41                      & 0.15                            & 5.68                            \\
        \quad w/o LLM Refinement  & 0.54                      & 0.33                            & 4.85                            \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Qualitative Analysis: LLM-as-a-Judge}
Traditional metrics like Silhouette Score often fail to capture semantic coherence. To address this, we implemented an \textbf{LLM-as-a-Judge} protocol, where a strong instruction-tuned model (Gemini Pro) acts as a proxy for human evaluation.
\begin{itemize}
    \item \textbf{Protocol:} The LLM is provided with a cluster's content and asked two questions: (1) "Does this cluster describe a single, distinct event?" (Topic Consistency, rated 0-1), and (2) Given two conflicting clusters, "Which one is more coherent?" (Pairwise Win-Rate).
    \item \textbf{Prompting:} We employ a "Role-Play" strategy where the LLM acts as an expert news editor. The full prompt specification is provided in Appendix Table \ref{tab:prompt_eval}.
\end{itemize}
Using this framework, we evaluated 100 random clusters:
\begin{itemize}
    \item \textbf{Clear Events (e.g., "Middle East Conflict"):} Achieved \textbf{Consistency $\approx$ 1.0} and high Win-Rates.
    \item \textbf{Mixed/Noisy Clusters:} Correctly flagged by the LLM with significantly lower scores (0.64), validating the metric's utility for automated quality assurance.
\end{itemize}

\section{Detailed Case Studies}
To better understand the system's behavior, we analyze three specific cases from the test deployment:

\begin{enumerate}
    \item \textbf{Success Case: "Bondi Beach Shooting" (T1).}
          \begin{itemize}
              \item \emph{Input:} "{\fontencoding{T5}\selectfont Vụ xả súng tại bãi biển Bondi khiến 15 người thiệt mạng sau vụ tấn công bằng dao.}" (Bondi beach shooting kills 15 people after knife attack.)
              \item \emph{Analysis:} Despite concurrent news about "Australia Travel", the SAHC algorithm correctly isolated this event due to strong lexical overlap with "Crisis" keywords. The LLM assigned it to \textbf{T1 (Public Risk)} with high confidence (>0.9).
          \end{itemize}

    \item \textbf{Success Case: "Middle East Conflict" (T1).}
          \begin{itemize}
              \item \emph{Metrics:} This cluster achieved a Semantic Consistency score of \textbf{0.96} and a Win-Rate of \textbf{0.67} against noise clusters.
              \item \emph{Insight:} The high volume of verified news anchors allowed the system to absorb social media reactions effectively without "drifting" into unrelated topics.
          \end{itemize}

    \item \textbf{Failure Analysis: "Mixed International News".}
          \begin{itemize}
              \item \emph{Issue:} The algorithm merged "Vietnamese-origin lawsuit" with "Territorial conflict" into a single cluster.
              \item \emph{Root Cause:} This likely occurred due to shared diplomatic terminology (e.g., "sovereignty", "international law") acting as spurious vector anchors, bridging two semantically distinct but lexically similar topics.
              \item \emph{Detection:} Crucially, our \textbf{LLM-as-a-Judge protocol} flagged this error, assigning a low Consistency Score of \textbf{0.64} and a \textbf{0.00 Win-Rate}. This demonstrates the system's ability to automatically detect quality degradation for human review.
          \end{itemize}
\end{enumerate}
